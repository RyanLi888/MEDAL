"""
MEDAL-Lite Testing Script
Evaluate trained model on test dataset
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import torch
from torch.utils.data import TensorDataset, DataLoader
import numpy as np
import argparse
from datetime import datetime

from sklearn.metrics import precision_score, recall_score, f1_score

from MoudleCode.utils.config import config
from MoudleCode.utils.helpers import (
    set_seed, setup_logger, calculate_metrics, print_metrics, find_optimal_threshold
)
from MoudleCode.utils.visualization import (
    plot_feature_space, plot_confusion_matrix, plot_roc_curve, plot_probability_distribution
)
from MoudleCode.preprocessing.pcap_parser import load_dataset
from MoudleCode.feature_extraction.backbone import MicroBiMambaBackbone
from MoudleCode.classification.dual_stream import MEDAL_Classifier

# å¯¼å…¥é¢„å¤„ç†æ¨¡å—
try:
    from preprocess import check_preprocessed_exists, load_preprocessed, preprocess_test
    PREPROCESS_AVAILABLE = True
except ImportError:
    PREPROCESS_AVAILABLE = False

import logging


def test_model(classifier, X_test, y_test, config, logger, save_prefix="test"):
    """
    Test the model on test dataset
    
    Args:
        classifier: Trained MEDAL classifier
        X_test: (N, L, 5) test sequences
        y_test: (N,) test labels
        config: configuration object
        logger: logger
        save_prefix: prefix for saved files
        
    Returns:
        metrics: dictionary of evaluation metrics
    """
    logger.info("="*70)
    logger.info("ğŸ” æ¨¡å‹æµ‹è¯• Model Testing")
    logger.info("="*70)
    logger.info(f"æµ‹è¯•æ ·æœ¬æ•°: {len(X_test)}")
    logger.info(f"æ‰¹æ¬¡å¤§å°: 64")
    
    # è®°å½•é…ç½®ä¸­çš„å‚è€ƒé˜ˆå€¼ï¼ˆç”¨äºå¯¹æ¯”ä¸å¯è§†åŒ–ï¼‰
    config_threshold = getattr(config, 'MALICIOUS_THRESHOLD', 0.5)
    logger.info(f"âœ“ é…ç½®å‚è€ƒé˜ˆå€¼: {config_threshold:.2f} (ä»…ç”¨äºå‚è€ƒä¸å¯è§†åŒ–)")
    logger.info("")
    
    classifier.eval()
    classifier.to(config.DEVICE)
    
    # Create DataLoader
    dataset = TensorDataset(torch.FloatTensor(X_test), torch.LongTensor(y_test))
    test_loader = DataLoader(dataset, batch_size=64, shuffle=False)
    total_batches = len(test_loader)
    
    # Collect probabilities / labels / featuresï¼ˆå…ˆä¸ä½¿ç”¨é˜ˆå€¼ï¼‰
    all_probs = []
    all_labels = []
    all_features = []
    
    logger.info("å¼€å§‹æ¨ç†...")
    logger.info("-"*70)
    
    with torch.no_grad():
        for batch_idx, (X_batch, y_batch) in enumerate(test_loader):
            X_batch = X_batch.to(config.DEVICE)
            y_batch = y_batch.to(config.DEVICE)
            
            # ç›´æ¥å‰å‘è·å¾— logits å’Œç‰¹å¾ï¼Œä¸åœ¨è¿™é‡Œåšé˜ˆå€¼åˆ¤å†³
            logits, z = classifier(X_batch, return_features=True, return_separate=False)
            probs = torch.softmax(logits, dim=1)
            
            all_probs.append(probs.cpu().numpy())
            all_labels.append(y_batch.cpu().numpy())
            all_features.append(z.cpu().numpy())
            
            # æ¯10ä¸ªæ‰¹æ¬¡æˆ–æœ€åä¸€ä¸ªæ‰¹æ¬¡è¾“å‡ºè¿›åº¦
            if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == total_batches:
                progress = (batch_idx + 1) / total_batches * 100
                processed = (batch_idx + 1) * 64
                logger.info(f"  æ¨ç†è¿›åº¦: {batch_idx+1}/{total_batches} batches ({progress:.1f}%) | å·²å¤„ç† {processed}/{len(X_test)} ä¸ªæ ·æœ¬")
    
    # Concatenate results
    y_prob = np.concatenate(all_probs)
    y_true = np.concatenate(all_labels)
    features = np.concatenate(all_features)
    
    logger.info("-"*70)
    logger.info("âœ“ æ¨ç†å®Œæˆ")
    logger.info("")
    
    # ğŸš€ åœ¨æµ‹è¯•é›†ä¸Šæœç´¢æœ€ä¼˜é˜ˆå€¼ï¼ˆåå¤„ç†æ­¥éª¤ï¼Œæ¨¡å‹å·²å›ºå®šï¼Œä¸æ˜¯æ•°æ®æ³„éœ²ï¼‰
    logger.info("ğŸ“Š åŸºäºæµ‹è¯•é›† Binary F1-Score(pos=1) æœç´¢æœ€ä¼˜é˜ˆå€¼...")
    logger.info("   è¯´æ˜: è¿™æ˜¯åå¤„ç†æ­¥éª¤ï¼Œæ¨¡å‹å‚æ•°å·²å›ºå®šï¼Œç”¨äºä¼˜åŒ–å†³ç­–é˜ˆå€¼")
    optimal_threshold, optimal_metric, _ = find_optimal_threshold(
        y_true, y_prob, metric='f1_binary', positive_class=1
    )
    logger.info(f"âœ… æµ‹è¯•é›†æœ€ä¼˜é˜ˆå€¼: {optimal_threshold:.4f} (Binary F1 pos=1 = {optimal_metric:.4f})")
    logger.info(f"   é…ç½®é»˜è®¤é˜ˆå€¼: {config_threshold:.4f}")
    
    # é˜ˆå€¼å¯¹æ¯”ï¼ˆè§£é‡Šä¸ºä»€ä¹ˆå›¾é‡Œå¯èƒ½æ˜¯ 0.8+ è€Œä¸æ˜¯ 0.6ï¼‰
    candidate_thresholds = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, float(optimal_threshold), float(config_threshold)]
    candidate_thresholds = sorted(set([round(t, 4) for t in candidate_thresholds]))
    logger.info("ğŸ“ é˜ˆå€¼å¯¹æ¯” (Malicious=Positive):")
    logger.info(f"  {'threshold':>10s} | {'precision':>9s} | {'recall':>7s} | {'f1':>7s}")
    logger.info("  " + "-"*44)
    for th in candidate_thresholds:
        y_pred_th = (y_prob[:, 1] >= th).astype(int)
        p = precision_score(y_true, y_pred_th, pos_label=1, zero_division=0)
        r = recall_score(y_true, y_pred_th, pos_label=1, zero_division=0)
        f1 = f1_score(y_true, y_pred_th, pos_label=1, zero_division=0)
        marker = " â† æœ€ä¼˜" if abs(th - optimal_threshold) < 0.0001 else ""
        logger.info(f"  {th:10.4f} | {p:9.4f} | {r:7.4f} | {f1:7.4f}{marker}")
    logger.info("")
    
    # ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼ç”Ÿæˆé¢„æµ‹æ ‡ç­¾ï¼ˆè¿™æ˜¯æœ€ç»ˆä½¿ç”¨çš„é˜ˆå€¼ï¼‰
    y_pred = (y_prob[:, 1] >= optimal_threshold).astype(int)
    logger.info(f"âœ… æœ€ç»ˆè¯„ä¼°ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼: {optimal_threshold:.4f}")
    logger.info(f"   (ä¸‹æ–¹æ€§èƒ½æŒ‡æ ‡å‡åŸºäºæ­¤é˜ˆå€¼è®¡ç®—)")
    logger.info("")
    
    # Calculate metrics at optimal threshold
    logger.info("ğŸ“Š è®¡ç®—æ€§èƒ½æŒ‡æ ‡ (åŸºäºè‡ªåŠ¨é˜ˆå€¼)...")
    metrics = calculate_metrics(y_true, y_pred, y_prob)
    
    # Print metricsï¼ˆåŒ…å«è®ºæ–‡å£å¾„ï¼šæ¶æ„ç±»ä¸ºæ­£ç±»çš„å•ç±»F1ï¼‰
    print_metrics(metrics, logger)
    
    # Per-class metrics
    logger.info("\nPer-Class Analysis:")
    for class_idx, class_name in enumerate(['Benign', 'Malicious']):
        class_mask = y_true == class_idx
        class_acc = (y_pred[class_mask] == y_true[class_mask]).mean()
        logger.info(f"  {class_name:10s}: {class_mask.sum():5d} samples, Accuracy: {class_acc*100:.2f}%")
    
    # Visualizations
    logger.info("\nGenerating visualizations...")
    
    # Feature space
    feature_space_path = os.path.join(config.RESULT_DIR, "figures", f"{save_prefix}_feature_space.png")
    plot_feature_space(features, y_true, feature_space_path, 
                      title=f"Test Set Feature Space", method='tsne')
    
    # Confusion matrix
    confusion_matrix_path = os.path.join(config.RESULT_DIR, "figures", f"{save_prefix}_confusion_matrix.png")
    plot_confusion_matrix(metrics['confusion_matrix'], ['Benign', 'Malicious'], 
                         confusion_matrix_path, title=f"Test Set Confusion Matrix")
    
    # ROC curve
    roc_curve_path = os.path.join(config.RESULT_DIR, "figures", f"{save_prefix}_roc_curve.png")
    plot_roc_curve(y_true, y_prob, roc_curve_path, title=f"Test Set ROC Curve")
    
    # Probability distribution (ä½¿ç”¨è‡ªåŠ¨æœç´¢å¾—åˆ°çš„æœ€ä¼˜é˜ˆå€¼)
    prob_dist_path = os.path.join(config.RESULT_DIR, "figures", f"{save_prefix}_probability_distribution.png")
    plot_probability_distribution(y_true, y_prob, prob_dist_path, 
                                  title=f"Test Set Probability Distribution", threshold=optimal_threshold)
    
    logger.info("")
    logger.info("ğŸ“ è¾“å‡ºæ–‡ä»¶è·¯å¾„:")
    logger.info(f"  âœ“ ç‰¹å¾ç©ºé—´å¯è§†åŒ–: {feature_space_path}")
    logger.info(f"  âœ“ æ··æ·†çŸ©é˜µ: {confusion_matrix_path}")
    logger.info(f"  âœ“ ROCæ›²çº¿: {roc_curve_path}")
    logger.info(f"  âœ“ æ¦‚ç‡åˆ†å¸ƒå›¾: {prob_dist_path}")
    
    # Save predictions to result directory
    results_file = os.path.join(config.RESULT_DIR, "models", f"{save_prefix}_predictions.npz")
    np.savez(results_file,
             y_true=y_true,
             y_pred=y_pred,
             y_prob=y_prob,
             features=features)
    # Save metrics to text file
    metrics_file = os.path.join(config.RESULT_DIR, "models", f"{save_prefix}_metrics.txt")
    with open(metrics_file, 'w') as f:
        f.write("="*70 + "\n")
        f.write(f"MEDAL-Lite Test Results - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("="*70 + "\n\n")
        f.write(f"Test Samples: {len(y_true)}\n")
        f.write(f"  Benign:    {(y_true==0).sum()}\n")
        f.write(f"  Malicious: {(y_true==1).sum()}\n\n")
        f.write("Performance Metrics (Malicious=Positive):\n")
        f.write(f"  Accuracy:          {metrics['accuracy']:.4f}\n")
        f.write(f"  Precision (pos=1): {metrics['precision_pos']:.4f}\n")
        f.write(f"  Recall    (pos=1): {metrics['recall_pos']:.4f}\n")
        f.write(f"  F1 (pos=1):        {metrics['f1_pos']:.4f}\n")
        f.write("  --- reference ---\n")
        f.write(f"  F1-Macro:          {metrics['f1_macro']:.4f}\n")
        f.write(f"  F1-Weighted:       {metrics['f1_weighted']:.4f}\n")
        if 'auc' in metrics:
            f.write(f"  AUC:               {metrics['auc']:.4f}\n")
        f.write("\nConfusion Matrix:\n")
        f.write(str(metrics['confusion_matrix']) + "\n")
    
    logger.info(f"  âœ“ é¢„æµ‹ç»“æœ: {results_file}")
    logger.info(f"  âœ“ æ€§èƒ½æŒ‡æ ‡: {metrics_file}")
    
    return metrics


def main(args):
    """Main testing function"""
    
    # Setup
    set_seed(config.SEED)
    config.create_dirs()
    logger = setup_logger(os.path.join(config.OUTPUT_ROOT, "logs"), name='test')
    
    logger.info("="*70)
    logger.info("ğŸ§ª MEDAL-Lite Testing Pipeline")
    logger.info("="*70)
    
    # GPUä¿¡æ¯
    if torch.cuda.is_available():
        logger.info(f"âœ“ GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
        logger.info(f"  ä½¿ç”¨è®¾å¤‡: {config.DEVICE}")
    else:
        logger.info(f"âš  ä½¿ç”¨CPUè¿›è¡Œæ¨ç†")
        logger.info(f"  ä½¿ç”¨è®¾å¤‡: {config.DEVICE}")
    
    logger.info(f"æ—¶é—´æˆ³: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("")
    
    # ========================
    # Load Test Dataset
    # ========================
    logger.info("="*70)
    logger.info("ğŸ“¦ åŠ è½½æµ‹è¯•æ•°æ®é›† Loading Test Dataset")
    logger.info("="*70)
    logger.info(f"æµ‹è¯•é›†é…ç½®:")
    logger.info(f"  æ­£å¸¸æµé‡è·¯å¾„: {config.BENIGN_TEST}")
    logger.info(f"  æ¶æ„æµé‡è·¯å¾„: {config.MALICIOUS_TEST}")
    logger.info(f"  è¯´æ˜: å°†è¯»å–ä¸Šè¿°è·¯å¾„ä¸‹æ‰€æœ‰pcapæ–‡ä»¶ï¼Œæµæ•°åœ¨å¤„ç†æ—¶ç»Ÿè®¡")
    logger.info("")
    
    # ä¼˜å…ˆä½¿ç”¨é¢„å¤„ç†å¥½çš„æ•°æ®
    if PREPROCESS_AVAILABLE and check_preprocessed_exists('test'):
        logger.info("âœ“ å‘ç°é¢„å¤„ç†æ–‡ä»¶ï¼Œç›´æ¥åŠ è½½...")
        X_test, y_test, test_files = load_preprocessed('test')
        logger.info(f"  ä»é¢„å¤„ç†æ–‡ä»¶åŠ è½½: {X_test.shape[0]} ä¸ªæ ·æœ¬")
    else:
        # ä»PCAPæ–‡ä»¶åŠ è½½
        logger.info("å¼€å§‹åŠ è½½æµ‹è¯•æ•°æ®é›†ï¼ˆä»PCAPæ–‡ä»¶ï¼‰...")
        logger.info("ğŸ’¡ æç¤º: è¿è¡Œ 'python preprocess.py --test_only' å¯é¢„å¤„ç†æµ‹è¯•é›†ï¼ŒåŠ é€Ÿåç»­æµ‹è¯•")
        X_test, y_test, test_files = load_dataset(
            benign_dir=config.BENIGN_TEST,
            malicious_dir=config.MALICIOUS_TEST,
            sequence_length=config.SEQUENCE_LENGTH
        )
    
    if X_test is None:
        logger.error("âŒ æµ‹è¯•æ•°æ®é›†åŠ è½½å¤±è´¥!")
        return
    
    logger.info("âœ“ æµ‹è¯•æ•°æ®é›†åŠ è½½å®Œæˆ")
    logger.info(f"  æ•°æ®å½¢çŠ¶: {X_test.shape} (æ ·æœ¬æ•°Ã—åºåˆ—é•¿åº¦Ã—ç‰¹å¾ç»´åº¦)")
    logger.info(f"  æ­£å¸¸æ ·æœ¬: {(y_test==0).sum()} ä¸ª")
    logger.info(f"  æ¶æ„æ ·æœ¬: {(y_test==1).sum()} ä¸ª")
    logger.info("")
    
    # ========================
    # Load Model
    # ========================
    logger.info("="*70)
    logger.info("ğŸ“¥ åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ Loading Trained Model")
    logger.info("="*70)
    logger.info("ğŸ“¥ è¾“å…¥æ•°æ®è·¯å¾„:")
    logger.info(f"  âœ“ æµ‹è¯•æ•°æ®: {config.BENIGN_TEST} (æ­£å¸¸), {config.MALICIOUS_TEST} (æ¶æ„)")
    logger.info("")
    
    # Try to load model metadata to get the backbone path used during training
    metadata_path = os.path.join(config.CLASSIFICATION_DIR, "models", "model_metadata.json")
    backbone_path_from_metadata = None
    
    if os.path.exists(metadata_path):
        try:
            import json
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)
            backbone_path_from_metadata = metadata.get('backbone_path')
            if backbone_path_from_metadata:
                logger.info(f"âœ“ ä»æ¨¡å‹å…ƒæ•°æ®ä¸­è¯»å–åˆ°è®­ç»ƒæ—¶ä½¿ç”¨çš„éª¨å¹²ç½‘ç»œ:")
                logger.info(f"  {backbone_path_from_metadata}")
                logger.info("")
        except Exception as e:
            logger.warning(f"âš  æ— æ³•è¯»å–æ¨¡å‹å…ƒæ•°æ®: {e}")
    
    # Load backbone from feature_extraction directory
    backbone = MicroBiMambaBackbone(config)
    
    # Determine backbone path: use metadata if available, otherwise use default
    if backbone_path_from_metadata and os.path.exists(backbone_path_from_metadata):
        backbone_path = backbone_path_from_metadata
        logger.info("ä½¿ç”¨è®­ç»ƒæ—¶çš„éª¨å¹²ç½‘ç»œï¼ˆä»å…ƒæ•°æ®ï¼‰")
    else:
        backbone_path = os.path.join(config.FEATURE_EXTRACTION_DIR, "models", "backbone_pretrained.pth")
        if backbone_path_from_metadata:
            logger.warning(f"âš  å…ƒæ•°æ®ä¸­çš„éª¨å¹²ç½‘ç»œä¸å­˜åœ¨: {backbone_path_from_metadata}")
            logger.warning(f"  å›é€€åˆ°é»˜è®¤è·¯å¾„: {backbone_path}")
    
    if not os.path.exists(backbone_path):
        logger.error(f"âŒ éª¨å¹²ç½‘ç»œæ£€æŸ¥ç‚¹æœªæ‰¾åˆ°: {backbone_path}")
        logger.error("è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬!")
        return
    
    logger.info("æ­£åœ¨åŠ è½½éª¨å¹²ç½‘ç»œ...")
    logger.info(f"  ğŸ“¥ è¾“å…¥æ¨¡å‹: {backbone_path}")
    backbone.load_state_dict(torch.load(backbone_path, map_location=config.DEVICE))
    backbone.freeze()
    logger.info(f"âœ“ éª¨å¹²ç½‘ç»œåŠ è½½å®Œæˆ")
    
    # Load classifier from classification directory
    classifier = MEDAL_Classifier(backbone, config)
    classifier_path = os.path.join(config.CLASSIFICATION_DIR, "models", "classifier_final.pth")
    
    if not os.path.exists(classifier_path):
        logger.error(f"âŒ åˆ†ç±»å™¨æ£€æŸ¥ç‚¹æœªæ‰¾åˆ°: {classifier_path}")
        logger.error("è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬!")
        return
    
    logger.info("æ­£åœ¨åŠ è½½åˆ†ç±»å™¨...")
    logger.info(f"  ğŸ“¥ è¾“å…¥æ¨¡å‹: {classifier_path}")
    classifier.load_state_dict(torch.load(classifier_path, map_location=config.DEVICE))
    logger.info(f"âœ“ åˆ†ç±»å™¨åŠ è½½å®Œæˆ")
    
    # Count parameters
    n_params = sum(p.numel() for p in classifier.parameters())
    n_trainable = sum(p.numel() for p in classifier.parameters() if p.requires_grad)
    logger.info("")
    logger.info("ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
    logger.info(f"  æ€»å‚æ•°é‡: {n_params:,}")
    logger.info(f"  å¯è®­ç»ƒå‚æ•°: {n_trainable:,} (éª¨å¹²ç½‘ç»œå·²å†»ç»“)")
    logger.info("")
    
    # ========================
    # Test Model
    # ========================
    metrics = test_model(classifier, X_test, y_test, config, logger)
    
    logger.info("")
    logger.info("="*70)
    logger.info("ğŸ‰ æµ‹è¯•å®Œæˆ! Testing Complete!")
    logger.info("="*70)
    logger.info("")
    logger.info("ğŸ“ è¾“å‡ºæ–‡ä»¶è·¯å¾„:")
    logger.info(f"  âœ“ é¢„æµ‹ç»“æœ: {os.path.join(config.RESULT_DIR, 'models', 'test_predictions.npz')}")
    logger.info(f"  âœ“ æ€§èƒ½æŒ‡æ ‡: {os.path.join(config.RESULT_DIR, 'models', 'test_metrics.txt')}")
    logger.info(f"  âœ“ å¯è§†åŒ–å›¾è¡¨: {os.path.join(config.RESULT_DIR, 'figures')}")
    logger.info("")
    logger.info("="*70)
    
    return metrics


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Test MEDAL-Lite model")
    
    args = parser.parse_args()
    
    main(args)

