"""
MEDAL-Lite ä¸»è®­ç»ƒè„šæœ¬ (é‡æ„ç‰ˆ)
==============================
å®ç°å®Œæ•´çš„3é˜¶æ®µè®­ç»ƒæµç¨‹ï¼Œä½¿ç”¨config.pyä¸­çš„æœ€ä¼˜é…ç½®
"""
import sys
import os
from pathlib import Path

# ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨sys.pathä¸­
project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

import torch
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import numpy as np
import random
import hashlib
import argparse
from datetime import datetime

from MoudleCode.utils.config import config
from MoudleCode.utils.helpers import (
    set_seed, setup_logger, inject_label_noise,
    calculate_metrics, print_metrics, save_checkpoint, find_optimal_threshold
)
from MoudleCode.utils.logging_utils import (
    log_section_header, log_subsection_header, log_key_value,
    log_stage_start, log_stage_end, log_input_paths, log_output_paths,
    log_training_config, log_data_stats, log_model_info, log_progress,
    log_epoch_metrics, log_early_stopping, log_final_summary
)
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, f1_score, silhouette_score
from MoudleCode.utils.visualization import (
    plot_feature_space, plot_noise_correction_comparison, plot_training_history
)
from MoudleCode.preprocessing.pcap_parser import load_dataset
from MoudleCode.feature_extraction.backbone import (
    MicroBiMambaBackbone, SimMTMLoss, build_backbone
)
from MoudleCode.feature_extraction.traffic_augmentation import DualViewAugmentation
from MoudleCode.feature_extraction.instance_contrastive import (
    InstanceContrastiveLearning, HybridPretrainingLoss
)
from MoudleCode.utils.checkpoint import load_state_dict_shape_safe

# å¯¼å…¥é¢„å¤„ç†æ¨¡å—
try:
    from scripts.utils.preprocess import check_preprocessed_exists, load_preprocessed, preprocess_train
    from scripts.utils.preprocess import normalize_burstsize_inplace
    PREPROCESS_AVAILABLE = True
except ImportError:
    PREPROCESS_AVAILABLE = False

from MoudleCode.label_correction.hybrid_court import HybridCourt
from MoudleCode.data_augmentation.tabddpm import TabDDPM
from MoudleCode.classification.dual_stream import MEDAL_Classifier, DualStreamLoss


def _rng_fingerprint_short() -> str:
    h = hashlib.sha256()
    try:
        h.update(repr(random.getstate()).encode('utf-8'))
    except Exception:
        h.update(b'py_random_error')
    try:
        ns = np.random.get_state()
        h.update(str(ns[0]).encode('utf-8'))
        h.update(np.asarray(ns[1], dtype=np.uint32).tobytes())
        h.update(str(ns[2]).encode('utf-8'))
        h.update(str(ns[3]).encode('utf-8'))
        h.update(str(ns[4]).encode('utf-8'))
    except Exception:
        h.update(b'numpy_random_error')
    try:
        h.update(torch.get_rng_state().detach().cpu().numpy().tobytes())
    except Exception:
        h.update(b'torch_cpu_rng_error')
    try:
        if torch.cuda.is_available():
            for s in torch.cuda.get_rng_state_all():
                h.update(s.detach().cpu().numpy().tobytes())
        else:
            h.update(b'no_cuda')
    except Exception:
        h.update(b'torch_cuda_rng_error')
    return h.hexdigest()[:16]


def _seed_snapshot() -> str:
    torch_seed = None
    try:
        torch_seed = int(torch.initial_seed())
    except Exception:
        torch_seed = None
    return (
        f"config.SEED={int(getattr(config, 'SEED', -1))} | "
        f"torch.initial_seed={torch_seed}"
    )


def stage1_pretrain_backbone(backbone, train_loader, config, logger):
    """
    Stage 1: è‡ªç›‘ç£é¢„è®­ç»ƒéª¨å¹²ç½‘ç»œ (SimMTM + InfoNCE)
    
    Args:
        backbone: MicroBiMambaBackbone æ¨¡å‹
        train_loader: æ•°æ®åŠ è½½å™¨ï¼ˆä»…éœ€Xï¼Œæ— éœ€æ ‡ç­¾ï¼‰
        config: é…ç½®å¯¹è±¡
        logger: æ—¥å¿—è®°å½•å™¨
        
    Returns:
        backbone: é¢„è®­ç»ƒåçš„éª¨å¹²ç½‘ç»œ
        history: è®­ç»ƒå†å²
    """
    # æ¸…ç†GPUç¼“å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        logger.info("âœ“ GPU ç¼“å­˜å·²æ¸…ç†")
    
    # è¾“å‡ºé˜¶æ®µé…ç½®
    log_stage_start(logger, "STAGE 1: è‡ªç›‘ç£é¢„è®­ç»ƒéª¨å¹²ç½‘ç»œ", "è®­ç»ƒMicro-Bi-Mambaéª¨å¹²ç½‘ç»œï¼Œå­¦ä¹ æµé‡ç‰¹å¾è¡¨ç¤º")
    config.log_stage_config(logger, "Stage 1")
    
    # è¾“å‡ºè¾“å…¥è·¯å¾„
    log_input_paths(logger, {
        "è®­ç»ƒæ•°æ®(æ­£å¸¸)": config.BENIGN_TRAIN,
        "è®­ç»ƒæ•°æ®(æ¶æ„)": config.MALICIOUS_TRAIN
    })
    
    backbone.train()
    backbone.to(config.DEVICE)
    
    # æ£€æŸ¥å¯¹æ¯”å­¦ä¹ é…ç½®
    use_instance_contrastive = getattr(config, 'USE_INSTANCE_CONTRASTIVE', False)
    contrastive_method = getattr(config, 'CONTRASTIVE_METHOD', 'infonce')
    actual_batch_size = train_loader.batch_size
    
    # åˆ›å»ºå¢å¼ºå™¨å’ŒæŸå¤±å‡½æ•°
    if use_instance_contrastive:
        augmentation = DualViewAugmentation(config)
        instance_contrastive = InstanceContrastiveLearning(backbone, config).to(config.DEVICE)
        simmtm_loss_fn = SimMTMLoss(
            config,
            mask_rate=config.SIMMTM_MASK_RATE,
            noise_std=getattr(config, 'PRETRAIN_NOISE_STD', 0.05)
        )
        hybrid_loss_fn = HybridPretrainingLoss(
            simmtm_loss=simmtm_loss_fn,
            instance_contrastive=instance_contrastive,
            lambda_infonce=config.INFONCE_LAMBDA
        )
        logger.info(f"âœ“ æ··åˆæŸå¤±å‡½æ•°åˆå§‹åŒ–å®Œæˆ (SimMTM + {contrastive_method.upper()})")
        
        optimizer = optim.AdamW(
            list(backbone.parameters()) + list(instance_contrastive.projection_head.parameters()),
            lr=config.PRETRAIN_LR,
            weight_decay=config.PRETRAIN_WEIGHT_DECAY
        )
    else:
        simmtm_loss_fn = SimMTMLoss(
            config,
            mask_rate=config.SIMMTM_MASK_RATE,
            noise_std=getattr(config, 'PRETRAIN_NOISE_STD', 0.05)
        )
        logger.info(f"âœ“ æŸå¤±å‡½æ•°åˆå§‹åŒ–å®Œæˆ (SimMTMæ©ç ç‡: {config.SIMMTM_MASK_RATE})")
        
        optimizer = optim.AdamW(
            backbone.parameters(),
            lr=config.PRETRAIN_LR,
            weight_decay=config.PRETRAIN_WEIGHT_DECAY
        )
    
    pretrain_min_lr = float(getattr(config, 'PRETRAIN_MIN_LR', 1e-5))
    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=config.PRETRAIN_EPOCHS, eta_min=pretrain_min_lr
    )
    logger.info(f"âœ“ ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨åˆå§‹åŒ–å®Œæˆ")
    
    # è®­ç»ƒå†å²
    if use_instance_contrastive:
        history = {'loss': [], 'simmtm': [], 'infonce': []}
    else:
        history = {'loss': [], 'simmtm': []}
    
    # æ—©åœé…ç½®
    use_early_stopping = bool(getattr(config, 'PRETRAIN_EARLY_STOPPING', True))
    es_warmup_epochs = int(getattr(config, 'PRETRAIN_ES_WARMUP_EPOCHS', 50))
    es_patience = int(getattr(config, 'PRETRAIN_ES_PATIENCE', 20))
    es_min_delta = float(getattr(config, 'PRETRAIN_ES_MIN_DELTA', 0.01))

    best_loss = float('inf')
    best_epoch = -1
    best_state = None
    no_improve = 0
    
    # æ¢¯åº¦ç´¯ç§¯é…ç½®
    use_gradient_accumulation = (
        use_instance_contrastive and
        str(contrastive_method).lower() == 'nnclr' and
        int(getattr(config, 'PRETRAIN_GRADIENT_ACCUMULATION_STEPS', 1)) > 1
    )
    gradient_accumulation_steps = int(getattr(config, 'PRETRAIN_GRADIENT_ACCUMULATION_STEPS', 2)) if use_gradient_accumulation else 1
    
    log_subsection_header(logger, "å¼€å§‹è®­ç»ƒ")

    # è®­ç»ƒå¾ªç¯
    for epoch in range(config.PRETRAIN_EPOCHS):
        epoch_loss = 0.0
        epoch_simmtm = 0.0
        epoch_infonce = 0.0
        
        for batch_idx, batch_data in enumerate(train_loader):
            if isinstance(batch_data, (list, tuple)):
                X_batch = batch_data[0]
            else:
                X_batch = batch_data
            X_batch = X_batch.to(config.DEVICE)
            
            if batch_idx % gradient_accumulation_steps == 0:
                optimizer.zero_grad()
            
            if use_instance_contrastive:
                x_view1, x_view2 = augmentation(X_batch)
                loss, loss_dict = hybrid_loss_fn(
                    backbone=backbone,
                    x_original=X_batch,
                    x_view1=x_view1,
                    x_view2=x_view2,
                    epoch=epoch
                )
                epoch_simmtm += loss_dict['simmtm']
                epoch_infonce += loss_dict['infonce']
            else:
                loss = simmtm_loss_fn(backbone, X_batch)
                epoch_simmtm += loss.item()
            
            loss = loss / gradient_accumulation_steps
            loss.backward()
            
            if (batch_idx + 1) % gradient_accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):
                optimizer.step()
            
            epoch_loss += loss.item() * gradient_accumulation_steps
        
        scheduler.step()
        
        # è®¡ç®—å¹³å‡æŸå¤±
        n_batches = len(train_loader)
        epoch_loss /= n_batches
        epoch_simmtm /= n_batches
        
        history['loss'].append(epoch_loss)
        history['simmtm'].append(epoch_simmtm)
        
        if use_instance_contrastive:
            epoch_infonce /= n_batches
            history['infonce'].append(epoch_infonce)
        
        # è¾“å‡ºæ—¥å¿—
        progress = (epoch + 1) / config.PRETRAIN_EPOCHS * 100
        if use_instance_contrastive:
            method_name = str(contrastive_method).upper()
            logger.info(f"[Stage 1] Epoch [{epoch+1}/{config.PRETRAIN_EPOCHS}] ({progress:.1f}%) | "
                       f"Loss: {epoch_loss:.4f} | SimMTM: {epoch_simmtm:.4f} | "
                       f"{method_name}: {epoch_infonce:.4f} | LR: {scheduler.get_last_lr()[0]:.6f}")
        else:
            logger.info(f"[Stage 1] Epoch [{epoch+1}/{config.PRETRAIN_EPOCHS}] ({progress:.1f}%) | "
                       f"Loss: {epoch_loss:.4f} | SimMTM: {epoch_simmtm:.4f} | LR: {scheduler.get_last_lr()[0]:.6f}")

        # æ—©åœæ£€æŸ¥
        improved = (best_loss - epoch_loss) > es_min_delta
        if improved:
            best_loss = float(epoch_loss)
            best_epoch = int(epoch + 1)
            best_state = {k: v.detach().cpu().clone() for k, v in backbone.state_dict().items()}
            no_improve = 0
        else:
            no_improve += 1

        if use_early_stopping and (epoch + 1) >= es_warmup_epochs and no_improve >= es_patience:
            log_early_stopping(logger, epoch+1, best_epoch, best_loss, epoch_loss, no_improve, es_patience)
            break

    # æ¢å¤æœ€ä½³çŠ¶æ€
    if best_state is not None:
        load_state_dict_shape_safe(backbone, best_state, logger, prefix="backbone(best)")
        backbone.to(config.DEVICE)
    
    # è¾“å‡ºé˜¶æ®µæ€»ç»“
    log_stage_end(logger, "Stage 1", {
        "æœ€ç»ˆæŸå¤±": f"{history['loss'][-1]:.4f}",
        "æœ€ä½³æŸå¤±": f"{best_loss:.4f} (epoch {best_epoch})" if best_epoch > 0 else "N/A",
        "è®­ç»ƒè½®æ•°": len(history['loss'])
    })
    
    # ä¿å­˜æ¨¡å‹
    n_samples = len(train_loader.dataset)
    model_dim = config.MODEL_DIM
    if use_instance_contrastive:
        method_name = f"SimMTM_{str(contrastive_method).upper()}"
    else:
        method_name = "SimMTM"
    
    backbone_filename = f"backbone_{method_name}_{model_dim}d_{n_samples}.pth"
    backbone_path = os.path.join(config.FEATURE_EXTRACTION_DIR, "models", backbone_filename)
    torch.save(backbone.state_dict(), backbone_path)
    
    default_backbone_path = os.path.join(config.FEATURE_EXTRACTION_DIR, "models", "backbone_pretrained.pth")
    torch.save(backbone.state_dict(), default_backbone_path)
    
    log_output_paths(logger, {
        "éª¨å¹²ç½‘ç»œæ¨¡å‹": backbone_path,
        "é»˜è®¤å‰¯æœ¬": default_backbone_path
    })
    
    return backbone, history


def stage2_label_correction_and_augmentation(backbone, X_train, y_train_noisy, y_train_clean, config, logger, stage2_mode='standard'):
    """
    Stage 2: æ ‡ç­¾çŸ«æ­£ + æ•°æ®å¢å¼º
    
    Args:
        backbone: é¢„è®­ç»ƒçš„éª¨å¹²ç½‘ç»œï¼ˆå†»ç»“ï¼‰
        X_train: (N, L, D) è®­ç»ƒåºåˆ—
        y_train_noisy: (N,) å™ªå£°æ ‡ç­¾
        y_train_clean: (N,) å¹²å‡€æ ‡ç­¾ï¼ˆä»…ç”¨äºè¯„ä¼°ï¼‰
        config: é…ç½®å¯¹è±¡
        logger: æ—¥å¿—è®°å½•å™¨
        stage2_mode: 'standard' æˆ– 'clean_augment_only'
        
    Returns:
        Z_augmented: å¢å¼ºåçš„ç‰¹å¾
        y_augmented: å¢å¼ºåçš„æ ‡ç­¾
        sample_weights: æ ·æœ¬æƒé‡
        correction_stats: çŸ«æ­£ç»Ÿè®¡
        tabddpm: TabDDPMæ¨¡å‹
        n_original: åŸå§‹æ ·æœ¬æ•°
    """
    log_stage_start(logger, "STAGE 2: æ ‡ç­¾çŸ«æ­£ + æ•°æ®å¢å¼º", "çŸ«æ­£æ ‡ç­¾å™ªå£°å¹¶ç”Ÿæˆå¢å¼ºæ ·æœ¬")
    config.log_stage_config(logger, "Stage 2")
    
    log_input_paths(logger, {
        "è®­ç»ƒæ•°æ®(æ­£å¸¸)": config.BENIGN_TRAIN,
        "è®­ç»ƒæ•°æ®(æ¶æ„)": config.MALICIOUS_TRAIN,
        "éª¨å¹²ç½‘ç»œæ¨¡å‹": os.path.join(config.FEATURE_EXTRACTION_DIR, "models", "backbone_pretrained.pth")
    })
    
    # å†»ç»“éª¨å¹²ç½‘ç»œå¹¶æå–ç‰¹å¾
    backbone.to(config.DEVICE)
    backbone.freeze()
    backbone.eval()
    logger.info("âœ“ éª¨å¹²ç½‘ç»œå·²å†»ç»“ï¼Œå¼€å§‹ç‰¹å¾æå–...")
    
    with torch.no_grad():
        X_tensor = torch.FloatTensor(X_train).to(config.DEVICE)
        features_list = []
        batch_size = 64
        total_batches = (len(X_tensor) + batch_size - 1) // batch_size
        
        for i in range(0, len(X_tensor), batch_size):
            batch_idx = i // batch_size + 1
            X_batch = X_tensor[i:i+batch_size]
            z_batch = backbone(X_batch, return_sequence=False)
            features_list.append(z_batch.cpu().numpy())
            
            if batch_idx % 10 == 0 or batch_idx == total_batches:
                log_progress(logger, batch_idx, total_batches, "ç‰¹å¾æå–")
        
        features = np.concatenate(features_list, axis=0)
    
    logger.info(f"âœ“ ç‰¹å¾æå–å®Œæˆ: {features.shape}")
    
    # ä¿å­˜ç‰¹å¾
    features_path = os.path.join(config.FEATURE_EXTRACTION_DIR, "models", "train_features.npy")
    np.save(features_path, features)
    
    # ç‰¹å¾å¯è§†åŒ–
    feature_dist_path = os.path.join(config.LABEL_CORRECTION_DIR, "figures", "feature_distribution_stage2.png")
    plot_feature_space(features, y_train_clean, feature_dist_path,
                      title="Stage 2: Feature Distribution", method='tsne')
    
    # æ ‡ç­¾çŸ«æ­£
    log_subsection_header(logger, "æ­¥éª¤ 2.1: Hybrid Court æ ‡ç­¾çŸ«æ­£")
    logger.info(f"  è¾“å…¥: {len(y_train_noisy)} ä¸ªæ ·æœ¬ï¼Œå™ªå£°ç‡: {config.LABEL_NOISE_RATE*100:.0f}%")
    logger.info(f"  æ–¹æ³•: CL (ç½®ä¿¡å­¦ä¹ ) + MADE (å¯†åº¦ä¼°è®¡) + KNN (è¯­ä¹‰æŠ•ç¥¨)")
    
    hybrid_court = HybridCourt(config)

    if stage2_mode == 'clean_augment_only':
        # è·³è¿‡æ ‡ç­¾çŸ«æ­£ï¼Œç›´æ¥ä½¿ç”¨å¹²å‡€æ ‡ç­¾
        suspected_noise, pred_labels, pred_probs = hybrid_court.cl.fit_predict(features, y_train_clean)
        hybrid_court.made.fit(features, device=config.DEVICE)
        is_dense, density_scores = hybrid_court.made.predict_density(features, device=config.DEVICE)
        hybrid_court.knn.fit(features)
        neighbor_labels, neighbor_consistency = hybrid_court.knn.predict_semantic_label(features, y_train_clean)
        y_corrected = y_train_clean.copy()
        action_mask = np.zeros(len(y_train_clean), dtype=int)
        confidence = pred_probs.max(axis=1)
        correction_weight = np.ones(len(y_train_clean), dtype=np.float32)
        cl_confidence = pred_probs.max(axis=1)
    else:
        y_corrected, action_mask, confidence, correction_weight, density_scores, neighbor_consistency, pred_probs = hybrid_court.correct_labels(
            features, y_train_noisy, device=config.DEVICE
        )
        cl_confidence = pred_probs.max(axis=1)
    
    logger.info("âœ“ æ ‡ç­¾çŸ«æ­£å®Œæˆ")

    # drop ä¸åˆ é™¤æ•°æ®ï¼šå°† drop æ ·æœ¬å½’ç±»ä¸º reweightï¼ˆä½æƒé‡ï¼‰
    try:
        drop_mask = (action_mask == 2)
        if hasattr(correction_weight, '__len__') and int(drop_mask.sum()) > 0:
            correction_weight = correction_weight.astype(np.float32, copy=True)
            drop_reweight = float(getattr(config, 'STAGE2_DROP_AS_REWEIGHT_WEIGHT', 0.1))
            correction_weight[drop_mask] = np.minimum(correction_weight[drop_mask], drop_reweight)
            try:
                action_mask = np.asarray(action_mask).copy()
                action_mask[drop_mask] = 3
            except Exception:
                pass
            logger.info(f"ğŸ§¹ dropæ ·æœ¬ä¸åˆ é™¤ï¼šå·²å°†å…¶å½’ç±»ä¸ºreweightå¹¶è®¾ä¸ºä½æƒé‡ (count={int(drop_mask.sum())}, w={drop_reweight})")
    except Exception:
        pass
    
    # ä¿å­˜çŸ«æ­£ç»“æœ
    correction_results_path = os.path.join(config.LABEL_CORRECTION_DIR, "models", "correction_results.npz")
    np.savez(correction_results_path,
             y_noisy=y_train_noisy if stage2_mode != 'clean_augment_only' else y_train_clean,
             y_corrected=y_corrected,
             action_mask=action_mask,
             confidence=confidence,
             correction_weight=correction_weight,
             density_scores=density_scores,
             neighbor_consistency=neighbor_consistency,
             pred_probs=pred_probs)
    
    # è®¡ç®—çŸ«æ­£ç»Ÿè®¡
    keep_mask = action_mask != 2
    correction_accuracy = (y_corrected[keep_mask] == y_train_clean[keep_mask]).mean()
    
    correction_stats = {
        'accuracy': correction_accuracy,
        'n_keep': (action_mask == 0).sum(),
        'n_flip': (action_mask == 1).sum(),
        'n_drop': (action_mask == 2).sum(),
        'n_reweight': (action_mask == 3).sum()
    }
    
    log_data_stats(logger, {
        "çŸ«æ­£å‡†ç¡®ç‡": f"{correction_accuracy*100:.2f}%",
        "ä¿æŒæ ·æœ¬": correction_stats['n_keep'],
        "ç¿»è½¬æ ·æœ¬": correction_stats['n_flip'],
        "ä¸¢å¼ƒæ ·æœ¬": correction_stats['n_drop'],
        "é‡åŠ æƒæ ·æœ¬": correction_stats['n_reweight']
    }, "æ ‡ç­¾çŸ«æ­£ç»Ÿè®¡")

    # Stage2 è¾“å‡ºï¼šä¿ç•™å…¨éƒ¨åŸå§‹æ ·æœ¬ï¼ˆåŒ…æ‹¬ dropï¼Œä½†å…¶æƒé‡=0ï¼‰
    X_all = X_train
    y_all = y_corrected
    weights_all = correction_weight
    Z_all = features

    # ä¿å­˜åŸå§‹åºåˆ—ç”¨äºStage 3æ··åˆè®­ç»ƒï¼ˆä¿ç•™å…¨éƒ¨åŸå§‹æ ·æœ¬ï¼Œdrop çš„æƒé‡ä¸º0ï¼‰
    try:
        real_kept_path = os.path.join(config.DATA_AUGMENTATION_DIR, "models", "real_kept_data.npz")
        np.savez(real_kept_path, X_real=X_all, y_real=y_all, sample_weights_real=weights_all)
        logger.info(f"  âœ“ å·²ä¿å­˜åŸå§‹åºåˆ—: {real_kept_path}")
    except Exception as e:
        logger.warning(f"âš  æ— æ³•ä¿å­˜åŸå§‹åºåˆ—: {e}")
    
    stage2_use_tabddpm = bool(getattr(config, 'STAGE2_USE_TABDDPM', True))

    if not stage2_use_tabddpm:
        logger.info("")

        logger.info("æ­¥éª¤ 2.2: TabDDPM æ•°æ®å¢å¼ºï¼ˆå·²è·³è¿‡ï¼‰")
        Z_augmented = Z_all
        y_augmented = y_all
        sample_weights = weights_all
        n_train_original = int(Z_all.shape[0])
        return Z_augmented, y_augmented, sample_weights, correction_stats, None, n_train_original
    
    # TabDDPM æ•°æ®å¢å¼º
    log_subsection_header(logger, "æ­¥éª¤ 2.2: TabDDPM æ•°æ®å¢å¼º (Feature Space)")
    logger.info(f"  ç›®æ ‡: åœ¨éª¨å¹²ç½‘ç»œç‰¹å¾ç©ºé—´ä¸­è®­ç»ƒ/ç”Ÿæˆ")

    # æ•°æ®å¢å¼ºä»…ä½¿ç”¨é«˜æƒé‡æ•°æ®ï¼ˆæ¥è‡ªæ ‡ç­¾çŸ«æ­£æƒé‡ï¼‰
    try:
        aug_min_w = float(getattr(config, 'STAGE2_AUGMENT_MIN_WEIGHT', getattr(config, 'STAGE2_FEATURE_TIER2_MIN_WEIGHT', 0.7)))
    except Exception:
        aug_min_w = 0.7
    aug_mask = (np.asarray(weights_all) >= float(aug_min_w))
    Z_clean = Z_all[aug_mask]
    y_clean = np.asarray(y_all)[aug_mask]
    weights_clean = np.asarray(weights_all)[aug_mask]
    logger.info(f"ğŸ§ª TabDDPMè®­ç»ƒ/å¢å¼ºä»…ä½¿ç”¨é«˜æƒé‡æ ·æœ¬: {int(aug_mask.sum())}/{len(Z_all)} (threshold={aug_min_w})")
    try:
        if len(weights_clean) > 0:
            logger.info(
                f"ğŸ§ª TabDDPMè®­ç»ƒé›†æƒé‡ç»Ÿè®¡: min={float(np.min(weights_clean)):.4f}, mean={float(np.mean(weights_clean)):.4f}, max={float(np.max(weights_clean)):.4f}"
            )
    except Exception:
        pass
    
    logger.info(f"ğŸ”§ RNGæŒ‡çº¹(Stage2-TabDDPMè®­ç»ƒå‰): {_rng_fingerprint_short()} ({_seed_snapshot()})")
    
    tabddpm = TabDDPM(
        config,
        input_dim=Z_clean.shape[1],
        cond_indices=[],
        dep_indices=list(range(Z_clean.shape[1])),
        enable_protocol_constraints=False
    ).to(config.DEVICE)

    tabddpm.fit_scaler(Z_clean)

    ddpm_lr = float(getattr(config, 'DDPM_LR', 5e-4))
    optimizer_ddpm = optim.AdamW(tabddpm.parameters(), lr=ddpm_lr)
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆv2.3æ–°å¢ï¼‰
    ddpm_lr_scheduler_type = getattr(config, 'DDPM_LR_SCHEDULER', None)
    ddpm_scheduler = None
    if ddpm_lr_scheduler_type == 'cosine':
        ddpm_min_lr = float(getattr(config, 'DDPM_MIN_LR', 1e-5))
        n_epochs_ddpm = int(getattr(config, 'DDPM_EPOCHS', 100))
        ddpm_scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer_ddpm, T_max=n_epochs_ddpm, eta_min=ddpm_min_lr
        )
        logger.info(f"âœ“ TabDDPMå­¦ä¹ ç‡è°ƒåº¦å™¨: Cosine Annealing (lr={ddpm_lr} â†’ {ddpm_min_lr})")
    else:
        n_epochs_ddpm = int(getattr(config, 'DDPM_EPOCHS', 100))
    
    ddpm_use_early_stopping = bool(getattr(config, 'DDPM_EARLY_STOPPING', True))
    ddpm_es_warmup_epochs = int(getattr(config, 'DDPM_ES_WARMUP_EPOCHS', 20))
    ddpm_es_patience = int(getattr(config, 'DDPM_ES_PATIENCE', 30))
    ddpm_es_min_delta = float(getattr(config, 'DDPM_ES_MIN_DELTA', 0.001))
    ddpm_es_smooth_window = int(getattr(config, 'DDPM_ES_SMOOTH_WINDOW', 5))

    dataset_ddpm = TensorDataset(torch.FloatTensor(Z_clean), torch.LongTensor(y_clean))
    logger.info(f"ğŸ”§ RNGæŒ‡çº¹(Stage2-TabDDPM DataLoaderåˆ›å»ºå‰): {_rng_fingerprint_short()} ({_seed_snapshot()})")
    loader_ddpm = DataLoader(dataset_ddpm, batch_size=2048, shuffle=True)
    logger.info(f"ğŸ”§ RNGæŒ‡çº¹(Stage2-TabDDPM DataLoaderåˆ›å»ºå): {_rng_fingerprint_short()} ({_seed_snapshot()})")

    ddpm_best_loss = float('inf')
    ddpm_best_epoch = -1
    ddpm_best_state = None
    ddpm_no_improve = 0
    ddpm_loss_history = []  # ç”¨äºå¹³æ»‘çª—å£çš„æŸå¤±å†å²

    tabddpm.train()
    for epoch in range(n_epochs_ddpm):
        epoch_loss = 0.0
        for Z_batch, y_batch in loader_ddpm:
            z_0 = Z_batch.to(config.DEVICE)
            y_batch = y_batch.to(config.DEVICE)
            optimizer_ddpm.zero_grad()
            loss = tabddpm.compute_loss(
                z_0, y_batch,
                mask_prob=float(getattr(config, 'MASK_PROBABILITY', 0.5)),
                mask_lambda=float(getattr(config, 'MASK_LAMBDA', 0.1)),
                p_uncond=0.2
            )
            loss.backward()
            optimizer_ddpm.step()
            epoch_loss += float(loss.item())

        avg_loss = epoch_loss / max(len(loader_ddpm), 1)
        ddpm_loss_history.append(avg_loss)
        
        # è®¡ç®—å¹³æ»‘æŸå¤±ï¼ˆç§»åŠ¨å¹³å‡ï¼‰
        if len(ddpm_loss_history) >= ddpm_es_smooth_window:
            smoothed_loss = float(np.mean(ddpm_loss_history[-ddpm_es_smooth_window:]))
        else:
            smoothed_loss = avg_loss
        
        # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆv2.3æ–°å¢ï¼‰
        if ddpm_scheduler is not None:
            ddpm_scheduler.step()
            current_lr = optimizer_ddpm.param_groups[0]['lr']
            if (epoch + 1) % 100 == 0:  # æ¯100è½®è®°å½•ä¸€æ¬¡å­¦ä¹ ç‡
                logger.info(f"[TabDDPM] Epoch [{epoch+1}/{n_epochs_ddpm}] | Loss: {avg_loss:.4f} | Smoothed: {smoothed_loss:.4f} | LR: {current_lr:.6f}")
            else:
                logger.info(f"[TabDDPM] Epoch [{epoch+1}/{n_epochs_ddpm}] | Loss: {avg_loss:.4f} | Smoothed: {smoothed_loss:.4f}")
        else:
            logger.info(f"[TabDDPM] Epoch [{epoch+1}/{n_epochs_ddpm}] | Loss: {avg_loss:.4f} | Smoothed: {smoothed_loss:.4f}")

        # ä½¿ç”¨å¹³æ»‘æŸå¤±è¿›è¡Œæ—©åœåˆ¤æ–­
        improved = (ddpm_best_loss - smoothed_loss) > ddpm_es_min_delta
        if improved:
            ddpm_best_loss = smoothed_loss
            ddpm_best_epoch = int(epoch + 1)
            ddpm_best_state = {k: v.detach().cpu().clone() for k, v in tabddpm.state_dict().items()}
            ddpm_no_improve = 0
        else:
            ddpm_no_improve += 1

        if ddpm_use_early_stopping and (epoch + 1) >= ddpm_es_warmup_epochs and ddpm_no_improve >= ddpm_es_patience:
            log_early_stopping(logger, epoch+1, ddpm_best_epoch, ddpm_best_loss, smoothed_loss, ddpm_no_improve, ddpm_es_patience)
            break

    if ddpm_best_state is not None:
        tabddpm.load_state_dict(ddpm_best_state)
        tabddpm.to(config.DEVICE)

    # ç”Ÿæˆå¢å¼ºç‰¹å¾
    logger.info("æ­¥éª¤ 2.3: ç”Ÿæˆå¢å¼ºç‰¹å¾")
    Z_syn, y_syn, w_syn = tabddpm.augment_feature_dataset(Z_clean, y_clean, weights_clean)

    n_train_original = int(Z_all.shape[0])
    n_syn = int(Z_syn.shape[0] - int(Z_clean.shape[0]))
    if n_syn > 0:
        Z_syn_only = Z_syn[-n_syn:]
        y_syn_only = y_syn[-n_syn:]
        w_syn_only = np.ones((len(y_syn_only),), dtype=np.float32)
    else:
        Z_syn_only = np.zeros((0, Z_all.shape[1]), dtype=Z_all.dtype)
        y_syn_only = np.zeros((0,), dtype=np.asarray(y_all).dtype)
        w_syn_only = np.zeros((0,), dtype=np.asarray(weights_all).dtype)

    Z_augmented = np.concatenate([Z_all, Z_syn_only], axis=0)
    y_augmented = np.concatenate([np.asarray(y_all), np.asarray(y_syn_only)], axis=0)
    sample_weights = np.concatenate([np.asarray(weights_all, dtype=np.float32), np.asarray(w_syn_only, dtype=np.float32)], axis=0)
    logger.info(f"âœ“ ç‰¹å¾å¢å¼ºå®Œæˆ: åŸå§‹={n_train_original} + åˆæˆ={len(Z_syn_only)} â†’ æ€»è®¡={len(Z_augmented)}")

    # ä¿å­˜æ¨¡å‹å’Œæ•°æ®
    tabddpm_path = os.path.join(config.DATA_AUGMENTATION_DIR, "models", "tabddpm_feature.pth")
    torch.save(tabddpm.state_dict(), tabddpm_path)
    
    augmented_data_path = os.path.join(config.DATA_AUGMENTATION_DIR, "models", "augmented_features.npz")
    is_original_mask = np.zeros(len(Z_augmented), dtype=bool)
    is_original_mask[:n_train_original] = True
    np.savez(augmented_data_path, Z_augmented=Z_augmented, y_augmented=y_augmented,
             is_original=is_original_mask, n_original=n_train_original, sample_weights=sample_weights)
    
    log_output_paths(logger, {
        "çŸ«æ­£ç»“æœ": correction_results_path,
        "TabDDPMæ¨¡å‹": tabddpm_path,
        "å¢å¼ºç‰¹å¾": augmented_data_path
    })
    
    log_stage_end(logger, "Stage 2", {
        "åŸå§‹æ ·æœ¬": n_train_original,
        "å¢å¼ºåæ ·æœ¬": len(Z_augmented),
        "åˆæˆæ ·æœ¬": n_syn
    })

    return Z_augmented, y_augmented, sample_weights, correction_stats, tabddpm, n_train_original


def stage3_finetune_classifier(backbone, X_train, y_train, sample_weights, config, logger, 
                               n_original=None, backbone_path=None, X_train_real=None, use_mixed_stream=None):
    """
    Stage 3: åˆ†ç±»å™¨å¾®è°ƒ
    
    Args:
        backbone: é¢„è®­ç»ƒçš„éª¨å¹²ç½‘ç»œ
        X_train: (N, L, D) è®­ç»ƒåºåˆ— æˆ– (N, D) å¢å¼ºç‰¹å¾
        y_train: (N,) è®­ç»ƒæ ‡ç­¾
        sample_weights: (N,) æ ·æœ¬æƒé‡
        config: é…ç½®å¯¹è±¡
        logger: æ—¥å¿—è®°å½•å™¨
        n_original: åŸå§‹æ ·æœ¬æ•°
        backbone_path: éª¨å¹²ç½‘ç»œè·¯å¾„
        X_train_real: åŸå§‹åºåˆ—ï¼ˆç”¨äºæ··åˆè®­ç»ƒï¼‰
        use_mixed_stream: æ˜¯å¦ä½¿ç”¨æ··åˆè®­ç»ƒ
        
    Returns:
        classifier: è®­ç»ƒå¥½çš„åˆ†ç±»å™¨
        history: è®­ç»ƒå†å²
        optimal_threshold: æœ€ä¼˜é˜ˆå€¼
    """
    log_stage_start(logger, "STAGE 3: åˆ†ç±»å™¨å¾®è°ƒ", "è®­ç»ƒåŒæµMLPåˆ†ç±»å™¨è¿›è¡Œæœ€ç»ˆå¨èƒæ£€æµ‹")
    config.log_stage_config(logger, "Stage 3")
    
    # ç¡®å®šè¾“å…¥ç±»å‹
    if use_mixed_stream is None:
        use_mixed_stream = bool(getattr(config, 'STAGE3_MIXED_STREAM', False))
    
    has_real_sequences = X_train_real is not None and len(X_train_real) > 0
    
    # è‡ªåŠ¨æ£€æµ‹è¾“å…¥æ˜¯ç‰¹å¾è¿˜æ˜¯åºåˆ—
    input_is_features = bool(getattr(config, 'CLASSIFIER_INPUT_IS_FEATURES', False))
    try:
        if hasattr(X_train, 'ndim'):
            if int(X_train.ndim) == 2:
                input_is_features = True
            elif int(X_train.ndim) == 3:
                input_is_features = False
    except Exception:
        pass

    # æ˜¾ç¤ºæ•°æ®ä¿¡æ¯
    logger.info("")
    logger.info("ğŸ“Š è®­ç»ƒæ•°æ®ä¿¡æ¯:")
    logger.info(f"  - æ•°æ®å½¢çŠ¶: {X_train.shape}")
    logger.info(f"  - æ ·æœ¬æ€»æ•°: {len(X_train)}")
    if n_original is not None and n_original != len(X_train):
        logger.info(f"  - åŸå§‹æ ·æœ¬: {n_original}")
        logger.info(f"  - å¢å¼ºæ ·æœ¬: {len(X_train) - n_original}")
    logger.info(f"  - æ­£å¸¸æ ·æœ¬: {(y_train == 0).sum()}")
    logger.info(f"  - æ¶æ„æ ·æœ¬: {(y_train == 1).sum()}")
    if input_is_features:
        logger.info(f"  - è¾“å…¥ç±»å‹: ç‰¹å¾å‘é‡ (2D)")
    else:
        logger.info(f"  - è¾“å…¥ç±»å‹: åŸå§‹åºåˆ— (3D)")
    logger.info("")

    # åŸå§‹æ ·æœ¬ï¼šæƒé‡>0.8 çš„åšæœ‰æ ‡ç­¾ç›‘ç£è®­ç»ƒï¼ˆæ ·æœ¬æƒé‡=2.0ï¼‰ï¼›<=0.8 çš„è¿›å…¥æ— æ ‡ç­¾åŠç›‘ç£è®­ç»ƒ
    sw = np.asarray(sample_weights, dtype=np.float32) if hasattr(sample_weights, '__len__') else None
    sup_thr = float(getattr(config, 'STAGE3_SUP_WEIGHT_THRESHOLD', 0.8))
    real_sup_weight = float(getattr(config, 'STAGE3_REAL_SUP_WEIGHT', 2.0))
    syn_weight = float(getattr(config, 'STAGE3_SYN_WEIGHT', 1.0))
    unlabeled_scale = float(getattr(config, 'STAGE3_UNLABELED_LOSS_SCALE', 1.0))

    orig_n = 0
    if n_original is not None:
        try:
            orig_n = min(int(n_original), int(len(y_train)))
        except Exception:
            orig_n = 0

    orig_sup_mask = None
    orig_unlab_mask = None
    if sw is not None and orig_n > 0:
        orig_sup_mask = (sw[:orig_n] > sup_thr)
        orig_unlab_mask = ~orig_sup_mask
        logger.info(f"ï¿½ Stage3 åŸå§‹æ ·æœ¬ç›‘ç£/æ— æ ‡ç­¾åˆ’åˆ†: supervised={int(orig_sup_mask.sum())}, unlabeled={int(orig_unlab_mask.sum())}, threshold={sup_thr}")

    # æ··åˆè®­ç»ƒæ¨¡å¼æ£€æŸ¥
    if use_mixed_stream and not has_real_sequences:
        logger.info("âš ï¸ æ··åˆè®­ç»ƒéœ€è¦é¢å¤–çš„åŸå§‹åºåˆ—(X_train_real)ï¼Œå½“å‰æœªæä¾›")
        use_mixed_stream = False
    
    if use_mixed_stream and not input_is_features:
        logger.info("âš ï¸ æ··åˆè®­ç»ƒéœ€è¦å¢å¼ºç‰¹å¾ä½œä¸ºä¸»è¾“å…¥ï¼Œå½“å‰è¾“å…¥æ˜¯åŸå§‹åºåˆ—")
        use_mixed_stream = False

    finetune_backbone_requested = bool(getattr(config, 'FINETUNE_BACKBONE', False))
    
    # ç‰¹å¾è¾“å…¥æ—¶ä¸èƒ½å¾®è°ƒéª¨å¹²ç½‘ç»œ
    if input_is_features and not use_mixed_stream:
        if finetune_backbone_requested:
            logger.info("âš ï¸ è¾“å…¥ä¸ºç‰¹å¾å‘é‡ï¼Œéª¨å¹²å¾®è°ƒå°†è¢«ç¦ç”¨ï¼ˆæ— æ³•åå‘ä¼ æ’­åˆ°éª¨å¹²ç½‘ç»œï¼‰")
        finetune_backbone_requested = False
    
    if use_mixed_stream:
        finetune_backbone_requested = True
        logger.info("ğŸ”€ æ··åˆè®­ç»ƒæ¨¡å¼å·²å¯ç”¨")
        log_data_stats(logger, {
            "åŸå§‹åºåˆ—": f"{len(X_train_real)} ä¸ªæ ·æœ¬",
            "å¢å¼ºç‰¹å¾": f"{len(X_train)} ä¸ªæ ·æœ¬",
            "åŸå§‹åºåˆ—æ‰¹æ¬¡": config.STAGE3_MIXED_REAL_BATCH_SIZE,
            "å¢å¼ºç‰¹å¾æ‰¹æ¬¡": config.STAGE3_MIXED_SYN_BATCH_SIZE
        }, "æ··åˆè®­ç»ƒé…ç½®")
    
    finetune_scope = str(getattr(config, 'FINETUNE_BACKBONE_SCOPE', 'projection')).lower()
    finetune_backbone_lr = float(getattr(config, 'FINETUNE_BACKBONE_LR', 1e-5))
    finetune_backbone_warmup_epochs = int(getattr(config, 'FINETUNE_BACKBONE_WARMUP_EPOCHS', 0))
    
    if backbone_path is None:
        backbone_path = os.path.join(config.FEATURE_EXTRACTION_DIR, "models", "backbone_pretrained.pth")
    
    log_input_paths(logger, {
        "å¢å¼ºç‰¹å¾": os.path.join(config.DATA_AUGMENTATION_DIR, "models", "augmented_features.npz"),
        "éª¨å¹²ç½‘ç»œæ¨¡å‹": backbone_path
    })

    if input_is_features:
        config.CLASSIFIER_INPUT_IS_FEATURES = True

    # åˆ›å»ºåˆ†ç±»å™¨
    classifier = MEDAL_Classifier(backbone, config).to(config.DEVICE)
    criterion = DualStreamLoss(config)
    logger.info("âœ“ åŒæµåˆ†ç±»å™¨åˆ›å»ºå®Œæˆ")
    logger.info(f"ğŸ”§ FocalLoss: alpha={config.FOCAL_ALPHA}, gamma={config.FOCAL_GAMMA}")

    # éª¨å¹²ç½‘ç»œå¾®è°ƒç­–ç•¥
    backbone_param_candidates = []
    if finetune_backbone_requested:
        if finetune_scope == 'all':
            backbone_param_candidates = list(backbone.parameters())
        elif finetune_scope == 'projection':
            if hasattr(backbone, 'projection'):
                backbone_param_candidates = list(backbone.projection.parameters())
            else:
                logger.warning("âš ï¸ backboneæ²¡æœ‰projectionå±‚ï¼Œå›é€€åˆ°å†»ç»“æ¨¡å¼")
                finetune_backbone_requested = False

    backbone_finetune_active = False
    backbone_finetune_started = False

    def _set_backbone_trainable(enable: bool) -> None:
        nonlocal backbone_finetune_active, backbone_finetune_started, finetune_backbone_requested
        backbone.freeze()
        if not enable or not finetune_backbone_requested:
            backbone.eval()
            backbone_finetune_active = False
            return

        if finetune_scope == 'all':
            backbone.unfreeze()
        elif finetune_scope == 'projection':
            if hasattr(backbone, 'projection'):
                for p in backbone.projection.parameters():
                    p.requires_grad = True

        backbone.train()
        backbone_finetune_active = True
        backbone_finetune_started = True

    if finetune_backbone_requested and finetune_backbone_warmup_epochs > 0:
        _set_backbone_trainable(False)
        logger.info(f"ğŸ§Š éª¨å¹²ç½‘ç»œå‰{finetune_backbone_warmup_epochs}è½®å†»ç»“ï¼Œä¹‹åå¾®è°ƒ(scope={finetune_scope})")
    elif finetune_backbone_requested:
        _set_backbone_trainable(True)
        logger.info(f"ğŸ”¥ éª¨å¹²ç½‘ç»œå¾®è°ƒå·²å¯ç”¨ (scope={finetune_scope})")
    else:
        _set_backbone_trainable(False)

    # ä¼˜åŒ–å™¨
    param_groups = [{'params': classifier.dual_mlp.parameters(), 'lr': float(config.FINETUNE_LR)}]
    if finetune_backbone_requested and len(backbone_param_candidates) > 0:
        param_groups.append({'params': backbone_param_candidates, 'lr': finetune_backbone_lr})
        logger.info(f"âœ“ ä¼˜åŒ–å™¨åŒ…å«éª¨å¹²ç½‘ç»œå‚æ•°: {len(backbone_param_candidates)} | lr={finetune_backbone_lr}")

    optimizer = optim.AdamW(param_groups, weight_decay=config.PRETRAIN_WEIGHT_DECAY)
    logger.info("âœ“ ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    # éªŒè¯é›†åˆ’åˆ†
    val_split = float(getattr(config, 'FINETUNE_VAL_SPLIT', 0.0))
    X_val_split = None
    y_val_split = None
    sample_weights_val_split = None

    if val_split > 0:
        X_train_split, X_val_split, y_train_split, y_val_split, sample_weights_split, sample_weights_val_split = train_test_split(
            X_train, y_train, sample_weights, test_size=val_split,
            random_state=int(getattr(config, 'SEED', 42)), stratify=y_train
        )
        logger.info(f"ğŸ“Š è®­ç»ƒé›†: {len(X_train_split)}, éªŒè¯é›†: {len(X_val_split)}")
    else:
        X_train_split = X_train
        y_train_split = y_train
        sample_weights_split = sample_weights
        logger.info("ğŸ“Š å…¨é‡è®­ç»ƒï¼ˆä¸åˆ’åˆ†éªŒè¯é›†ï¼‰")
    
    # æ•°æ®åŠ è½½å™¨
    # æ„å»ºç›‘ç£/æ— æ ‡ç­¾æ•°æ®ï¼ˆæ”¯æŒ 2D ç‰¹å¾ æˆ– mixed-stream çš„ 3D åŸå§‹åºåˆ—ï¼‰
    def _sym_kl(p, q):
        p = p.clamp_min(1e-8)
        q = q.clamp_min(1e-8)
        return (p * (p.log() - q.log())).sum(dim=1) + (q * (q.log() - p.log())).sum(dim=1)

    X_np = X_train_split
    y_np = np.asarray(y_train_split)
    sw_np = np.asarray(sample_weights_split, dtype=np.float32)

    # åˆæˆæ ·æœ¬ï¼ˆé»˜è®¤ä½äºå°¾éƒ¨ï¼šåŸå§‹n_original + syn_onlyï¼‰
    if orig_n > 0:
        syn_idx_start = orig_n
    else:
        syn_idx_start = 0

    syn_mask = np.zeros(len(sw_np), dtype=bool)
    if syn_idx_start < len(sw_np):
        syn_mask[syn_idx_start:] = True

    sup_mask = syn_mask.copy()
    unlab_mask = np.zeros(len(sw_np), dtype=bool)
    if orig_n > 0 and orig_sup_mask is not None:
        sup_mask[:orig_n] = orig_sup_mask
        unlab_mask[:orig_n] = orig_unlab_mask
    else:
        # æ—  n_original ä¿¡æ¯æ—¶ï¼šæŒ‰æƒé‡é˜ˆå€¼åˆ’åˆ†ï¼ˆä¿å®ˆï¼‰
        if sw is not None:
            sup_mask = (sw_np > sup_thr)
            unlab_mask = ~sup_mask

    # ç›‘ç£æ ·æœ¬æƒé‡ï¼šåŸå§‹é«˜æƒé‡=2.0ï¼Œåˆæˆ=1.0
    sw_sup = np.ones(int(sup_mask.sum()), dtype=np.float32) * syn_weight
    try:
        if orig_n > 0 and orig_sup_mask is not None:
            n_orig_sup = int(orig_sup_mask.sum())
            if n_orig_sup > 0:
                sw_sup[:n_orig_sup] = real_sup_weight
    except Exception:
        pass

    X_sup = X_np[sup_mask]
    y_sup = y_np[sup_mask]

    train_dataset = TensorDataset(
        torch.FloatTensor(X_sup),
        torch.LongTensor(y_sup),
        torch.FloatTensor(sw_sup)
    )

    unlab_dataset = None
    if int(unlab_mask.sum()) > 0:
        X_unlab = X_np[unlab_mask]
        unlab_dataset = TensorDataset(torch.FloatTensor(X_unlab))
    
    # å¹³è¡¡é‡‡æ ·ï¼ˆåŸºäº supervised æ•°æ®é›†ï¼‰
    use_balanced_sampling = getattr(config, 'USE_BALANCED_SAMPLING', True)
    if use_balanced_sampling:
        from torch.utils.data import WeightedRandomSampler
        target_ratio = float(getattr(config, 'BALANCED_SAMPLING_RATIO', 1.0))
        try:
            class_counts = np.bincount(np.asarray(y_sup, dtype=int))
            if len(class_counts) < 2:
                class_counts = np.pad(class_counts, (0, 2 - len(class_counts)), constant_values=0)
            weight_benign = target_ratio * class_counts[1] / max(class_counts[0], 1)
            weight_malicious = 1.0
            class_weights = np.array([weight_benign, weight_malicious])
            sample_sampling_weights = class_weights[np.asarray(y_sup, dtype=int)]
            sampler = WeightedRandomSampler(
                weights=torch.as_tensor(sample_sampling_weights, dtype=torch.double),
                num_samples=len(sample_sampling_weights), replacement=True
            )
            train_loader = DataLoader(train_dataset, batch_size=config.FINETUNE_BATCH_SIZE, sampler=sampler)
            logger.info(f"ğŸŒ¡ï¸ æ¸©å®¤è®­ç»ƒ: å¹³è¡¡é‡‡æ · (ç›®æ ‡æ¯”ä¾‹ {target_ratio}:1)")
        except Exception:
            train_loader = DataLoader(train_dataset, batch_size=config.FINETUNE_BATCH_SIZE, shuffle=True)
    else:
        train_loader = DataLoader(train_dataset, batch_size=config.FINETUNE_BATCH_SIZE, shuffle=True)
    
    # æ··åˆè®­ç»ƒæ•°æ®åŠ è½½å™¨ï¼ˆåŸå§‹åºåˆ—åˆ†ä¸º supervised / unlabeledï¼‰
    real_loader = None
    unlab_real_loader = None
    if use_mixed_stream and has_real_sequences:
        n_real = len(X_train_real)
        real_n = min(int(orig_n), int(n_real)) if orig_n > 0 else min(len(y_train_split), int(n_real))

        y_real_all = y_np[:real_n]
        if orig_sup_mask is None or orig_unlab_mask is None or real_n == 0:
            real_sup_sel = np.ones(real_n, dtype=bool)
            real_unlab_sel = np.zeros(real_n, dtype=bool)
        else:
            real_sup_sel = orig_sup_mask[:real_n]
            real_unlab_sel = orig_unlab_mask[:real_n]

        X_real_sup = X_train_real[:real_n][real_sup_sel]
        y_real_sup = y_real_all[real_sup_sel]
        w_real_sup = np.ones(len(y_real_sup), dtype=np.float32) * real_sup_weight

        real_dataset = TensorDataset(
            torch.FloatTensor(X_real_sup),
            torch.LongTensor(y_real_sup),
            torch.FloatTensor(w_real_sup)
        )
        real_batch_size = int(getattr(config, 'STAGE3_MIXED_REAL_BATCH_SIZE', 32))
        real_loader = DataLoader(real_dataset, batch_size=real_batch_size, shuffle=True)

        if int(real_unlab_sel.sum()) > 0:
            X_real_unlab = X_train_real[:real_n][real_unlab_sel]
            unlab_real_dataset = TensorDataset(torch.FloatTensor(X_real_unlab))
            unlab_real_loader = DataLoader(unlab_real_dataset, batch_size=real_batch_size, shuffle=True)

        syn_batch_size = int(getattr(config, 'STAGE3_MIXED_SYN_BATCH_SIZE', 96))
        train_loader = DataLoader(train_dataset, batch_size=syn_batch_size, shuffle=True)
        
        logger.info(f"âœ“ æ··åˆè®­ç»ƒåŠ è½½å™¨: åŸå§‹={len(real_loader)}æ‰¹, å¢å¼º={len(train_loader)}æ‰¹")
        if unlab_real_loader is not None:
            logger.info(f"âœ“ æ— æ ‡ç­¾åŸå§‹åŠ è½½å™¨: {len(unlab_real_loader)}æ‰¹")
    
    logger.info(f"âœ“ æ•°æ®åŠ è½½å™¨å‡†å¤‡å®Œæˆ ({len(train_loader)} ä¸ªæ‰¹æ¬¡)")

    unlab_iter = None

    # è®­ç»ƒå†å²
    history = {
        'train_loss': [], 'supervision': [], 'train_f1': [],
        'val_f1': [], 'val_threshold': [],
        'train_f1_star': [], 'train_threshold_star': []
    }
    
    classifier.train()
    best_f1 = -1.0
    best_epoch = -1
    best_state = None
    best_threshold = float(getattr(config, 'MALICIOUS_THRESHOLD', 0.5))
    finetuned_backbone_path = None

    keep_last_k_minloss = int(getattr(config, 'KEEP_LAST_K_MINLOSS_EPOCHS', 10))
    last_k_loss_states = []
    
    # æ—©åœé…ç½®
    use_early_stopping = bool(getattr(config, 'FINETUNE_EARLY_STOPPING', False))
    es_warmup_epochs = int(getattr(config, 'FINETUNE_ES_WARMUP_EPOCHS', 30))
    es_patience = int(getattr(config, 'FINETUNE_ES_PATIENCE', 25))
    es_min_delta = float(getattr(config, 'FINETUNE_ES_MIN_DELTA', 0.0))  # æ”¹ä¸º0.0ï¼Œä½¿ç”¨çº¯è€å¿ƒå€¼ç­–ç•¥
    no_improve_count = 0
    best_es_metric = -1.0
    allow_train_metric_es = bool(getattr(config, 'FINETUNE_ES_ALLOW_TRAIN_METRIC', False))
    
    if use_early_stopping and X_val_split is None and not allow_train_metric_es:
        use_early_stopping = False
        logger.info("âš ï¸ æ—©åœå·²ç¦ç”¨ï¼šæ— éªŒè¯é›†ä¸”ä¸å…è®¸ä½¿ç”¨è®­ç»ƒé›†æŒ‡æ ‡")
    
    real_loss_scale = float(getattr(config, 'STAGE3_MIXED_REAL_LOSS_SCALE', 2.0))
    syn_loss_scale = float(getattr(config, 'STAGE3_MIXED_SYN_LOSS_SCALE', 1.0))
    
    log_subsection_header(logger, "å¼€å§‹è®­ç»ƒ")
    
    for epoch in range(config.FINETUNE_EPOCHS):
        # éª¨å¹²ç½‘ç»œå¾®è°ƒæ¿€æ´»
        if finetune_backbone_requested and (not backbone_finetune_active) and (epoch >= finetune_backbone_warmup_epochs):
            _set_backbone_trainable(True)
            logger.info(f"ğŸ”¥ éª¨å¹²ç½‘ç»œå¾®è°ƒåœ¨epoch {epoch+1}æ¿€æ´» (scope={finetune_scope})")
        
        epoch_loss = 0.0
        epoch_losses = {'total': 0.0, 'supervision': 0.0, 'stream_a': 0.0, 'stream_b': 0.0}
        all_train_probs = []
        all_train_labels = []
        
        if use_mixed_stream and real_loader is not None:
            import itertools
            max_batches = max(len(real_loader), len(train_loader))
            real_iter = itertools.cycle(real_loader)
            syn_iter = itertools.cycle(train_loader)
            unlab_iter = itertools.cycle(unlab_real_loader) if unlab_real_loader is not None else None
            
            for batch_idx in range(max_batches):
                X_real, y_real, w_real = next(real_iter)
                X_real = X_real.to(config.DEVICE)
                y_real = y_real.to(config.DEVICE)
                w_real = w_real.to(config.DEVICE)
                
                Z_syn, y_syn, w_syn = next(syn_iter)
                Z_syn = Z_syn.to(config.DEVICE)
                y_syn = y_syn.to(config.DEVICE)
                w_syn = w_syn.to(config.DEVICE)
                
                optimizer.zero_grad()
                
                if backbone_finetune_active:
                    z_real = backbone(X_real, return_sequence=False)
                else:
                    with torch.no_grad():
                        z_real = backbone(X_real, return_sequence=False)
                
                loss_real, loss_dict_real = criterion(classifier.dual_mlp, z_real, y_real, w_real, epoch, config.FINETUNE_EPOCHS)
                loss_syn, loss_dict_syn = criterion(classifier.dual_mlp, Z_syn, y_syn, w_syn, epoch, config.FINETUNE_EPOCHS)

                loss_unlab = 0.0
                if unlab_iter is not None and unlabeled_scale > 0:
                    (X_unlab_real,) = next(unlab_iter)
                    X_unlab_real = X_unlab_real.to(config.DEVICE)
                    if backbone_finetune_active:
                        z_unlab = backbone(X_unlab_real, return_sequence=False)
                    else:
                        with torch.no_grad():
                            z_unlab = backbone(X_unlab_real, return_sequence=False)
                    logits_a, logits_b = classifier.dual_mlp(z_unlab, return_separate=True)
                    p_a = torch.softmax(logits_a, dim=1)
                    p_b = torch.softmax(logits_b, dim=1)
                    loss_unlab = _sym_kl(p_a, p_b).mean()

                loss = real_loss_scale * loss_real + syn_loss_scale * loss_syn + unlabeled_scale * loss_unlab
                loss.backward()
                optimizer.step()
                
                epoch_loss += float(loss.item())
                epoch_losses['total'] += float(real_loss_scale * loss_dict_real['total'] + syn_loss_scale * loss_dict_syn['total'])
                epoch_losses['supervision'] += float(real_loss_scale * loss_dict_real['supervision'] + syn_loss_scale * loss_dict_syn['supervision'])
                epoch_losses['stream_a'] += float(real_loss_scale * loss_dict_real['stream_a'] + syn_loss_scale * loss_dict_syn['stream_a'])
                epoch_losses['stream_b'] += float(real_loss_scale * loss_dict_real['stream_b'] + syn_loss_scale * loss_dict_syn['stream_b'])
                
                with torch.no_grad():
                    logits_a_syn, logits_b_syn = classifier.dual_mlp(Z_syn, return_separate=True)
                    logits_avg_syn = (logits_a_syn + logits_b_syn) / 2.0
                    probs_syn = torch.softmax(logits_avg_syn, dim=1)
                    all_train_probs.append(probs_syn.cpu().numpy())
                    all_train_labels.append(y_syn.cpu().numpy())
            
            n_batches = max_batches
        else:
            for X_batch, y_batch, w_batch in train_loader:
                X_batch = X_batch.to(config.DEVICE)
                y_batch = y_batch.to(config.DEVICE)
                w_batch = w_batch.to(config.DEVICE)

                optimizer.zero_grad()

                if input_is_features:
                    z = X_batch
                else:
                    if backbone_finetune_active:
                        z = backbone(X_batch, return_sequence=False)
                    else:
                        with torch.no_grad():
                            z = backbone(X_batch, return_sequence=False)

                loss, loss_dict = criterion(classifier.dual_mlp, z, y_batch, w_batch, epoch, config.FINETUNE_EPOCHS)

                # æ— æ ‡ç­¾åŠç›‘ç£ï¼šå¯¹ä½æƒé‡åŸå§‹æ ·æœ¬åšåŒå¤´ä¸€è‡´æ€§ï¼ˆä»…ç‰¹å¾æ¨¡å¼æœ‰æ•ˆï¼‰
                loss_unlab = 0.0
                if unlab_dataset is not None and unlabeled_scale > 0:
                    try:
                        (X_unlab_feat,) = next(unlab_iter)
                    except Exception:
                        unlab_iter = iter(DataLoader(unlab_dataset, batch_size=config.FINETUNE_BATCH_SIZE, shuffle=True))
                        (X_unlab_feat,) = next(unlab_iter)
                    X_unlab_feat = X_unlab_feat.to(config.DEVICE)
                    logits_a, logits_b = classifier.dual_mlp(X_unlab_feat, return_separate=True)
                    p_a = torch.softmax(logits_a, dim=1)
                    p_b = torch.softmax(logits_b, dim=1)
                    loss_unlab = _sym_kl(p_a, p_b).mean()

                (loss + unlabeled_scale * loss_unlab).backward()
                optimizer.step()

                epoch_loss += float(loss_dict['total'])
                epoch_losses['total'] += float(loss_dict['total'])
                epoch_losses['supervision'] += float(loss_dict['supervision'])
                epoch_losses['stream_a'] += float(loss_dict.get('stream_a', 0.0))
                epoch_losses['stream_b'] += float(loss_dict.get('stream_b', 0.0))

                with torch.no_grad():
                    logits_a, logits_b = classifier.dual_mlp(z, return_separate=True)
                    logits_avg = (logits_a + logits_b) / 2.0
                    probs = torch.softmax(logits_avg, dim=1)
                    all_train_probs.append(probs.cpu().numpy())
                    all_train_labels.append(y_batch.cpu().numpy())
            
            n_batches = int(max(len(train_loader), 1))

        epoch_loss /= n_batches
        for key in epoch_losses:
            epoch_losses[key] /= n_batches

        # è®°å½•æœ€åKè½®çš„lossæœ€å°æ¨¡å‹ï¼ˆä»…ç”¨äºè®­ç»ƒç»“æŸåé€‰å–ä¸€ä¸ªä¿å­˜ï¼‰
        if keep_last_k_minloss > 0:
            current_state = {k: v.detach().cpu().clone() for k, v in classifier.state_dict().items()}
            last_k_loss_states.append((int(epoch + 1), float(epoch_loss), current_state))
            if len(last_k_loss_states) > keep_last_k_minloss:
                last_k_loss_states = last_k_loss_states[-keep_last_k_minloss:]
        
        # è®¡ç®—è®­ç»ƒé›†F1
        train_probs = np.concatenate(all_train_probs)
        train_labels = np.concatenate(all_train_labels)
        train_preds = (train_probs[:, 1] >= 0.5).astype(int)
        train_f1 = f1_score(train_labels, train_preds, pos_label=1, zero_division=0)

        monitor_pr_auc = bool(getattr(config, 'MONITOR_PR_AUC', False))
        train_ap = None
        if monitor_pr_auc:
            try:
                from sklearn.metrics import average_precision_score
                train_ap = float(average_precision_score(train_labels, train_probs[:, 1]))
            except Exception as e:
                logger.warning(f"âš  è®¡ç®—è®­ç»ƒé›†PR-AUCå¤±è´¥: {e}")

        train_f1_star = None
        train_threshold_star = None
        if X_val_split is None:
            train_threshold_star, _train_metric, _ = find_optimal_threshold(train_labels, train_probs, metric='f1_binary', positive_class=1)
            train_preds_star = (train_probs[:, 1] >= float(train_threshold_star)).astype(int)
            train_f1_star = f1_score(train_labels, train_preds_star, pos_label=1, zero_division=0)

        val_f1 = None
        val_threshold = None
        val_ap = None

        if X_val_split is not None:
            classifier.eval()
            all_val_probs = []
            all_val_labels = []
            with torch.no_grad():
                val_dataset = TensorDataset(
                    torch.FloatTensor(X_val_split),
                    torch.LongTensor(y_val_split),
                    torch.FloatTensor(sample_weights_val_split)
                )
                val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
                for X_batch, y_batch, _w_batch in val_loader:
                    X_batch = X_batch.to(config.DEVICE)
                    if input_is_features:
                        z = X_batch
                    else:
                        z = backbone(X_batch, return_sequence=False)
                    logits_a, logits_b = classifier.dual_mlp(z, return_separate=True)
                    logits_avg = (logits_a + logits_b) / 2.0
                    probs = torch.softmax(logits_avg, dim=1)
                    all_val_probs.append(probs.cpu().numpy())
                    all_val_labels.append(y_batch.cpu().numpy())

            val_probs = np.concatenate(all_val_probs)
            val_labels = np.concatenate(all_val_labels)
            if monitor_pr_auc:
                try:
                    from sklearn.metrics import average_precision_score
                    val_ap = float(average_precision_score(val_labels, val_probs[:, 1]))
                except Exception as e:
                    logger.warning(f"âš  è®¡ç®—éªŒè¯é›†PR-AUCå¤±è´¥: {e}")
            val_threshold, val_metric, _ = find_optimal_threshold(val_labels, val_probs, metric='f1_binary', positive_class=1)
            val_preds = (val_probs[:, 1] >= float(val_threshold)).astype(int)
            val_f1 = f1_score(val_labels, val_preds, pos_label=1, zero_division=0)

            if val_f1 > best_f1:
                best_f1 = float(val_f1)
                best_epoch = int(epoch + 1)
                best_threshold = float(val_threshold)
                best_state = {k: v.detach().cpu().clone() for k, v in classifier.state_dict().items()}
        else:
            if train_f1_star is not None and train_f1_star > best_f1:
                best_f1 = float(train_f1_star)
                best_epoch = int(epoch + 1)
                best_threshold = float(train_threshold_star)
                best_state = {k: v.detach().cpu().clone() for k, v in classifier.state_dict().items()}
        
        classifier.train()
        
        # ä¿å­˜å†å²
        history['train_loss'].append(epoch_loss)
        history['supervision'].append(epoch_losses['supervision'])
        history['train_f1'].append(train_f1)
        history['val_f1'].append(val_f1 if val_f1 is not None else np.nan)
        history['val_threshold'].append(val_threshold if val_threshold is not None else np.nan)
        history['train_f1_star'].append(train_f1_star if train_f1_star is not None else np.nan)
        history['train_threshold_star'].append(train_threshold_star if train_threshold_star is not None else np.nan)
        if monitor_pr_auc:
            if 'train_ap' not in history:
                history['train_ap'] = []
            if 'val_ap' not in history:
                history['val_ap'] = []
            history['train_ap'].append(train_ap if train_ap is not None else np.nan)
            history['val_ap'].append(val_ap if val_ap is not None else np.nan)
        
        # è¾“å‡ºæ—¥å¿—ï¼ˆå…ˆè¾“å‡ºå†æ—©åœï¼Œä¿è¯è§¦å‘æ—©åœçš„æœ€åä¸€è½®ä¹Ÿæœ‰epochæ—¥å¿—ï¼‰
        progress = (epoch + 1) / config.FINETUNE_EPOCHS * 100
        if val_f1 is not None:
            msg = (
                f"[Stage 3] Epoch [{epoch+1}/{config.FINETUNE_EPOCHS}] ({progress:.1f}%) | "
                f"Loss: {epoch_loss:.4f} | "
                f"L(total={epoch_losses['total']:.4f}, sup={epoch_losses['supervision']:.4f}, a={epoch_losses['stream_a']:.4f}, b={epoch_losses['stream_b']:.4f}) | "
                f"TrF1: {train_f1:.4f} | ValF1*: {val_f1:.4f} | Th: {val_threshold:.4f}"
            )
            if monitor_pr_auc:
                msg += f" | TrAP: {float(train_ap) if train_ap is not None else float('nan'):.4f} | ValAP: {float(val_ap) if val_ap is not None else float('nan'):.4f}"
            logger.info(msg)
        else:
            train_f1_star_disp = float(train_f1_star) if train_f1_star is not None else float('nan')
            train_th_star_disp = float(train_threshold_star) if train_threshold_star is not None else float('nan')
            msg = (
                f"[Stage 3] Epoch [{epoch+1}/{config.FINETUNE_EPOCHS}] ({progress:.1f}%) | "
                f"Loss: {epoch_loss:.4f} | "
                f"L(total={epoch_losses['total']:.4f}, sup={epoch_losses['supervision']:.4f}, a={epoch_losses['stream_a']:.4f}, b={epoch_losses['stream_b']:.4f}) | "
                f"TrF1: {train_f1:.4f} | TrF1*: {train_f1_star_disp:.4f} | Th: {train_th_star_disp:.4f}"
            )
            if monitor_pr_auc:
                msg += f" | TrAP: {float(train_ap) if train_ap is not None else float('nan'):.4f}"
            logger.info(msg)

        # æ—©åœæ£€æŸ¥
        if use_early_stopping and (epoch + 1) >= es_warmup_epochs:
            current_metric = val_f1 if val_f1 is not None else train_f1_star
            if current_metric is not None:
                current_metric = float(current_metric)
                if current_metric > (best_es_metric + es_min_delta):
                    best_es_metric = current_metric
                    no_improve_count = 0
                else:
                    no_improve_count += 1
                    if no_improve_count >= es_patience:
                        log_early_stopping(logger, epoch+1, best_epoch, best_f1, current_metric, no_improve_count, es_patience)
                        break

    # è¾“å‡ºé˜¶æ®µæ€»ç»“
    actual_epochs = len(history['train_loss'])
    log_stage_end(logger, "Stage 3", {
        "æœ€ç»ˆæŸå¤±": f"{history['train_loss'][-1]:.4f}",
        "æœ€ç»ˆF1": f"{history['train_f1'][-1]:.4f}",
        "æœ€ä½³F1": f"{best_f1:.4f} (epoch {best_epoch})" if best_epoch > 0 else "N/A",
        "å®é™…è®­ç»ƒè½®æ•°": f"{actual_epochs}/{config.FINETUNE_EPOCHS}"
    })
    
    # ç”Ÿæˆç‰¹å¾å¯è§†åŒ–
    classifier.eval()
    if backbone_finetune_active:
        backbone.eval()
    with torch.no_grad():
        if input_is_features:
            train_features = np.asarray(X_train, dtype=np.float32)
        else:
            X_train_tensor = torch.FloatTensor(X_train).to(config.DEVICE)
            features_list = []
            batch_size = 64
            for i in range(0, len(X_train_tensor), batch_size):
                X_batch = X_train_tensor[i:i+batch_size]
                z_batch = backbone(X_batch, return_sequence=False)
                features_list.append(z_batch.cpu().numpy())
            train_features = np.concatenate(features_list, axis=0)
    
    feature_dist_path = os.path.join(config.CLASSIFICATION_DIR, "figures", "feature_distribution_stage3.png")
    plot_feature_space(train_features, y_train, feature_dist_path,
                      title="Stage 3: Feature Distribution", method='tsne')
    
    optimal_threshold = float(best_threshold)
    
    # ä¿å­˜æ¨¡å‹
    if backbone_finetune_started:
        finetuned_backbone_path = os.path.join(config.CLASSIFICATION_DIR, "models", "backbone_finetuned.pth")
        torch.save(backbone.state_dict(), finetuned_backbone_path)

    if best_state is not None:
        best_path = os.path.join(config.CLASSIFICATION_DIR, "models", "classifier_best_f1.pth")
        torch.save(best_state, best_path)

    # ä¿å­˜æœ€å10è½®(é»˜è®¤)ä¸­lossæœ€å°çš„æ¨¡å‹
    minloss_path = None
    if keep_last_k_minloss > 0 and len(last_k_loss_states) > 0:
        try:
            min_epoch, min_loss, min_state = min(last_k_loss_states, key=lambda t: float(t[1]))
            minloss_path = os.path.join(
                config.CLASSIFICATION_DIR,
                "models",
                f"classifier_last{int(keep_last_k_minloss)}_minloss_epoch{int(min_epoch)}.pth"
            )
            torch.save(min_state, minloss_path)
            logger.info(f"âœ“ å·²ä¿å­˜æœ€å{int(keep_last_k_minloss)}è½®ä¸­lossæœ€å°æ¨¡å‹: epoch={int(min_epoch)} loss={float(min_loss):.6f}")
        except Exception as e:
            logger.warning(f"âš  ä¿å­˜æœ€å{int(keep_last_k_minloss)}è½®lossæœ€å°æ¨¡å‹å¤±è´¥: {e}")

    final_path = os.path.join(config.CLASSIFICATION_DIR, "models", "classifier_final.pth")
    final_state = {k: v.detach().cpu().clone() for k, v in classifier.state_dict().items()}
    torch.save(final_state, final_path)
    
    history_path = os.path.join(config.CLASSIFICATION_DIR, "models", "training_history.npz")
    np.savez(history_path, **{k: np.array(v) for k, v in history.items()})
    
    # ä¿å­˜å…ƒæ•°æ®
    if finetuned_backbone_path is not None:
        backbone_path = finetuned_backbone_path
    elif backbone_path is None:
        backbone_path = os.path.join(config.FEATURE_EXTRACTION_DIR, "models", "backbone_pretrained.pth")
    
    metadata_path = os.path.join(config.CLASSIFICATION_DIR, "models", "model_metadata.json")
    metadata = {
        'backbone_path': backbone_path,
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'n_samples': len(X_train),
        'n_original': n_original if n_original is not None else len(X_train),
        'finetune_epochs': config.FINETUNE_EPOCHS,
        'input_is_features': input_is_features,
        'feature_dim': int(getattr(config, 'OUTPUT_DIM', config.MODEL_DIM)),
    }
    import json
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=2)
    
    output_files = {
        "æœ€ä½³æ¨¡å‹": os.path.join(config.CLASSIFICATION_DIR, "models", "classifier_best_f1.pth"),
        "æœ€ç»ˆæ¨¡å‹": final_path,
        "è®­ç»ƒå†å²": history_path,
        "æ¨¡å‹å…ƒæ•°æ®": metadata_path
    }
    if minloss_path is not None:
        output_files[f"æœ€å{int(keep_last_k_minloss)}è½®lossæœ€å°æ¨¡å‹"] = minloss_path
    if finetuned_backbone_path:
        output_files["å¾®è°ƒéª¨å¹²ç½‘ç»œ"] = finetuned_backbone_path
    
    log_output_paths(logger, output_files)
    
    return classifier, history, optimal_threshold


def main(args):
    """ä¸»è®­ç»ƒå‡½æ•°"""
    
    # åˆå§‹åŒ–
    rng_fp_before_seed = _rng_fingerprint_short()
    set_seed(config.SEED)
    rng_fp_after_seed = _rng_fingerprint_short()
    config.create_dirs()
    logger = setup_logger(os.path.join(config.OUTPUT_ROOT, "logs"), name='train')

    logger.info(f"ğŸ”§ RNGæŒ‡çº¹(seedå‰): {rng_fp_before_seed}")
    logger.info(f"ğŸ”§ RNGæŒ‡çº¹(seedå): {rng_fp_after_seed} ({_seed_snapshot()})")
    
    log_section_header(logger, "ğŸš€ MEDAL-Lite è®­ç»ƒæµç¨‹")
    logger.info(f"æ—¶é—´æˆ³: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # GPUä¿¡æ¯
    if torch.cuda.is_available():
        logger.info(f"âœ“ GPU: {torch.cuda.get_device_name(0)}")
        logger.info(f"  æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
        logger.info(f"  è®¾å¤‡: {config.DEVICE}")
    else:
        logger.warning("âš  ä½¿ç”¨CPUè®­ç»ƒ")
    
    # è·å–é˜¶æ®µèŒƒå›´
    start_stage = getattr(args, 'start_stage', 1)
    end_stage = getattr(args, 'end_stage', 3)
    
    if isinstance(start_stage, str):
        try:
            start_stage = int(start_stage)
        except ValueError:
            logger.error(f"âŒ æ— æ•ˆçš„èµ·å§‹é˜¶æ®µ: {start_stage}")
            return

    if isinstance(end_stage, str):
        try:
            end_stage = int(end_stage)
        except ValueError:
            logger.error(f"âŒ æ— æ•ˆçš„ç»“æŸé˜¶æ®µ: {end_stage}")
            return

    if end_stage < start_stage:
        logger.error(f"âŒ æ— æ•ˆé˜¶æ®µèŒƒå›´: start_stage={start_stage} > end_stage={end_stage}")
        return
    
    # åŠ è½½æ•°æ®é›†
    X_train = None
    y_train_clean = None
    y_train_noisy = None
    
    if start_stage <= 3:
        log_section_header(logger, "ğŸ“¦ æ•°æ®é›†åŠ è½½")
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(åŠ è½½è®­ç»ƒæ•°æ®å‰): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        log_data_stats(logger, {
            "æ­£å¸¸æµé‡è·¯å¾„": config.BENIGN_TRAIN,
            "æ¶æ„æµé‡è·¯å¾„": config.MALICIOUS_TRAIN,
            "åºåˆ—é•¿åº¦": config.SEQUENCE_LENGTH
        }, "è®­ç»ƒé›†é…ç½®")
        
        if PREPROCESS_AVAILABLE and check_preprocessed_exists('train'):
            logger.info("âœ“ å‘ç°é¢„å¤„ç†æ–‡ä»¶ï¼Œç›´æ¥åŠ è½½...")
            X_train, y_train_clean, train_files = load_preprocessed('train')
            X_train = normalize_burstsize_inplace(X_train)
        else:
            logger.info("å¼€å§‹åŠ è½½è®­ç»ƒæ•°æ®é›†ï¼ˆä»PCAPæ–‡ä»¶ï¼‰...")
            X_train, y_train_clean, train_files = load_dataset(
                benign_dir=config.BENIGN_TRAIN,
                malicious_dir=config.MALICIOUS_TRAIN,
                sequence_length=config.SEQUENCE_LENGTH
            )
            X_train = normalize_burstsize_inplace(X_train)

        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(åŠ è½½è®­ç»ƒæ•°æ®å): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        
        if X_train is None:
            logger.error("âŒ è®­ç»ƒæ•°æ®é›†åŠ è½½å¤±è´¥!")
            return
        
        log_data_stats(logger, {
            "æ•°æ®å½¢çŠ¶": X_train.shape,
            "æ­£å¸¸æ ·æœ¬": (y_train_clean==0).sum(),
            "æ¶æ„æ ·æœ¬": (y_train_clean==1).sum()
        }, "è®­ç»ƒæ•°æ®é›†")
        
        if start_stage >= 2 and start_stage != 3:
            logger.info(f"ğŸ”€ æ³¨å…¥æ ‡ç­¾å™ªå£° ({config.LABEL_NOISE_RATE*100:.0f}%)...")
            y_train_noisy, noise_mask = inject_label_noise(y_train_clean, config.LABEL_NOISE_RATE)
            logger.info(f"âœ“ å™ªå£°æ ‡ç­¾åˆ›å»ºå®Œæˆ: {noise_mask.sum()} ä¸ªæ ‡ç­¾è¢«ç¿»è½¬")
        else:
            y_train_noisy = None
    
    # æ„å»ºéª¨å¹²ç½‘ç»œ
    logger.info(f"ğŸ”§ RNGæŒ‡çº¹(æ„å»ºbackboneå‰): {_rng_fingerprint_short()} ({_seed_snapshot()})")
    backbone = build_backbone(config, logger=logger)
    backbone = backbone.to(config.DEVICE)
    logger.info(f"ğŸ”§ RNGæŒ‡çº¹(æ„å»ºbackboneå): {_rng_fingerprint_short()} ({_seed_snapshot()})")

    # Stage 1: é¢„è®­ç»ƒéª¨å¹²ç½‘ç»œ
    if start_stage <= 1:
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(Stage1è°ƒç”¨å‰): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        use_instance_contrastive = getattr(config, 'USE_INSTANCE_CONTRASTIVE', False)
        contrastive_method = getattr(config, 'CONTRASTIVE_METHOD', 'infonce')
        
        method_lower = str(contrastive_method).lower()
        if use_instance_contrastive and method_lower == 'nnclr':
            batch_size = getattr(config, 'PRETRAIN_BATCH_SIZE_NNCLR', 64)
        elif use_instance_contrastive and method_lower == 'simsiam':
            batch_size = getattr(config, 'PRETRAIN_BATCH_SIZE_SIMSIAM', config.PRETRAIN_BATCH_SIZE)
        else:
            batch_size = config.PRETRAIN_BATCH_SIZE
        
        dataset = TensorDataset(torch.FloatTensor(X_train))
        train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        backbone, pretrain_history = stage1_pretrain_backbone(backbone, train_loader, config, logger)
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(Stage1è¿”å›å): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        
        if end_stage <= 1:
            logger.info("âœ… å·²å®Œæˆåˆ° Stage 1")
            return backbone
    else:
        # åŠ è½½é¢„è®­ç»ƒéª¨å¹²ç½‘ç»œ
        if hasattr(args, 'backbone_path') and args.backbone_path:
            backbone_path = args.backbone_path
        else:
            backbone_path = os.path.join(config.FEATURE_EXTRACTION_DIR, "models", "backbone_pretrained.pth")
        
        retrain_backbone = bool(getattr(args, 'retrain_backbone', False))
        if retrain_backbone:
            logger.warning("âš  ä½¿ç”¨éšæœºåˆå§‹åŒ–éª¨å¹²ç½‘ç»œ")
            backbone.freeze()
        elif os.path.exists(backbone_path):
            logger.info(f"âœ“ åŠ è½½éª¨å¹²ç½‘ç»œ: {backbone_path}")
            logger.info(f"ğŸ”§ RNGæŒ‡çº¹(åŠ è½½backboneæƒé‡å‰): {_rng_fingerprint_short()} ({_seed_snapshot()})")
            try:
                backbone_state = torch.load(backbone_path, map_location=config.DEVICE, weights_only=True)
            except TypeError:
                backbone_state = torch.load(backbone_path, map_location=config.DEVICE)
            load_state_dict_shape_safe(backbone, backbone_state, logger, prefix="backbone")
            logger.info(f"ğŸ”§ RNGæŒ‡çº¹(åŠ è½½backboneæƒé‡å): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        else:
            logger.error(f"âŒ æ‰¾ä¸åˆ°éª¨å¹²ç½‘ç»œ: {backbone_path}")
            return
    
    # Stage 2: æ ‡ç­¾çŸ«æ­£ + æ•°æ®å¢å¼º
    if start_stage <= 2 and end_stage >= 2:
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(Stage2è°ƒç”¨å‰): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        if y_train_noisy is None and y_train_clean is not None:
            logger.info(f"ğŸ”€ æ³¨å…¥æ ‡ç­¾å™ªå£° ({config.LABEL_NOISE_RATE*100:.0f}%)...")
            y_train_noisy, noise_mask = inject_label_noise(y_train_clean, config.LABEL_NOISE_RATE)
            logger.info(f"âœ“ å™ªå£°æ ‡ç­¾åˆ›å»ºå®Œæˆ: {noise_mask.sum()} ä¸ªæ ‡ç­¾è¢«ç¿»è½¬")
        
        stage2_mode = getattr(args, 'stage2_mode', 'standard')
        X_augmented, y_augmented, sample_weights, correction_stats, tabddpm, n_original = stage2_label_correction_and_augmentation(
            backbone, X_train, y_train_noisy, y_train_clean, config, logger, stage2_mode=stage2_mode
        )
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(Stage2è¿”å›å): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        
        if end_stage <= 2:
            logger.info("âœ… å·²å®Œæˆåˆ° Stage 2")
            return backbone
    elif end_stage >= 3:
        # è·³è¿‡Stage 2
        augmented_data_path = os.path.join(config.DATA_AUGMENTATION_DIR, "models", "augmented_features.npz")
        finetune_backbone_enabled = bool(getattr(config, 'FINETUNE_BACKBONE', False))
        
        if int(start_stage) == 3 and X_train is not None:
            logger.info("âœ… Stage 3-only: è·³è¿‡ Stage 2")
            
            if finetune_backbone_enabled:
                X_augmented = X_train
                y_augmented = y_train_clean
                sample_weights = np.ones(len(X_train), dtype=np.float32)
                n_original = len(X_train)
                logger.info(f"âœ“ ä½¿ç”¨åŸå§‹åºåˆ—: {len(X_train)} ä¸ªæ ·æœ¬")
            else:
                X_train_tensor = torch.FloatTensor(X_train).to(config.DEVICE)
                with torch.no_grad():
                    Z_clean = backbone(X_train_tensor, return_sequence=False).detach().cpu().numpy().astype(np.float32)
                X_augmented = Z_clean
                y_augmented = y_train_clean
                sample_weights = np.ones(len(Z_clean), dtype=np.float32)
                n_original = len(Z_clean)
                logger.info(f"âœ“ ä½¿ç”¨ç‰¹å¾å‘é‡: {len(Z_clean)} ä¸ªæ ·æœ¬")
        elif os.path.exists(augmented_data_path):
            logger.info(f"âœ“ åŠ è½½å¢å¼ºç‰¹å¾: {augmented_data_path}")
            data = np.load(augmented_data_path)
            X_augmented = data['Z_augmented']
            y_augmented = data['y_augmented']
            sample_weights = data['sample_weights'] if 'sample_weights' in data else np.ones(len(X_augmented))
            n_original = int(data['n_original']) if 'n_original' in data else len(X_augmented)
        else:
            logger.error(f"âŒ æ‰¾ä¸åˆ°å¢å¼ºæ•°æ®: {augmented_data_path}")
            return

    # Stage 3: åˆ†ç±»å™¨å¾®è°ƒ
    if end_stage >= 3 and start_stage <= 3:
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(Stage3è°ƒç”¨å‰): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        if hasattr(args, 'backbone_path') and args.backbone_path:
            actual_backbone_path = args.backbone_path
        else:
            actual_backbone_path = os.path.join(config.FEATURE_EXTRACTION_DIR, "models", "backbone_pretrained.pth")
        
        X_train_real = None
        try:
            real_kept_path = os.path.join(config.DATA_AUGMENTATION_DIR, "models", "real_kept_data.npz")
            if os.path.exists(real_kept_path):
                real_pack = np.load(real_kept_path)
                X_train_real = real_pack.get('X_real', None)
        except Exception:
            X_train_real = None

        classifier, finetune_history, optimal_threshold = stage3_finetune_classifier(
            backbone, X_augmented, y_augmented, sample_weights, config, logger,
            n_original=n_original, backbone_path=actual_backbone_path, X_train_real=X_train_real,
            use_mixed_stream=bool(getattr(config, 'STAGE3_MIXED_STREAM', False))
        )
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(Stage3è¿”å›å): {_rng_fingerprint_short()} ({_seed_snapshot()})")
    else:
        logger.info("â­ï¸ è·³è¿‡ Stage 3")
        return backbone
    
    # ç»˜åˆ¶è®­ç»ƒå†å²
    history_fig_path = os.path.join(config.CLASSIFICATION_DIR, "figures", "training_history.png")
    plot_training_history(finetune_history, history_fig_path)
    
    # æœ€ç»ˆæ€»ç»“
    log_final_summary(logger, "è®­ç»ƒå®Œæˆ", {
        "Stage 1": f"éª¨å¹²ç½‘ç»œé¢„è®­ç»ƒ - {config.PRETRAIN_EPOCHS} epochs",
        "Stage 2": "æ ‡ç­¾çŸ«æ­£+æ•°æ®å¢å¼º - å®Œæˆ",
        "Stage 3": f"åˆ†ç±»å™¨å¾®è°ƒ - {config.FINETUNE_EPOCHS} epochs"
    }, {
        "ç‰¹å¾æå–": config.FEATURE_EXTRACTION_DIR,
        "æ ‡ç­¾çŸ«æ­£": config.LABEL_CORRECTION_DIR,
        "æ•°æ®å¢å¼º": config.DATA_AUGMENTATION_DIR,
        "åˆ†ç±»å™¨": config.CLASSIFICATION_DIR
    })
    
    return classifier


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="MEDAL-Lite è®­ç»ƒè„šæœ¬")
    
    parser.add_argument("--noise_rate", type=float, default=None, help="æ ‡ç­¾å™ªå£°ç‡ï¼ˆé»˜è®¤ä½¿ç”¨config.LABEL_NOISE_RATEï¼‰")
    parser.add_argument("--start_stage", type=str, default="1", choices=["1", "2", "3"], help="èµ·å§‹é˜¶æ®µ")
    parser.add_argument("--end_stage", type=str, default="3", choices=["1", "2", "3"], help="ç»“æŸé˜¶æ®µ")
    parser.add_argument("--backbone_path", type=str, default=None, help="éª¨å¹²ç½‘ç»œè·¯å¾„")
    parser.add_argument("--retrain_backbone", action="store_true", help="é‡æ–°è®­ç»ƒéª¨å¹²ç½‘ç»œ")
    parser.add_argument("--stage2_mode", type=str, default="standard", choices=["standard", "clean_augment_only"], help="Stage 2æ¨¡å¼")
    
    args = parser.parse_args()
    if args.noise_rate is not None:
        config.LABEL_NOISE_RATE = args.noise_rate
    
    main(args)
