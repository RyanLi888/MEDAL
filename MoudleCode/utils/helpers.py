"""
Helper functions for MEDAL-Lite model
"""
import torch
import numpy as np
import random
import os
import logging
from datetime import datetime

from MoudleCode.utils.logging_utils import setup_logger


def set_seed(seed=42):
    """Set random seed for reproducibility"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False


def count_parameters(model):
    """Count trainable parameters in model"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def inject_label_noise(labels, noise_rate, num_classes=2):
    """
    Inject symmetric label noise
    
    Args:
        labels: numpy array of original labels
        noise_rate: float, percentage of labels to flip (0-1)
        num_classes: int, number of classes
    
    Returns:
        noisy_labels: numpy array with injected noise
        noise_mask: boolean array indicating which labels were flipped
    """
    n_samples = len(labels)
    n_noisy = int(n_samples * noise_rate)
    
    # Create noise mask
    noise_indices = np.random.choice(n_samples, n_noisy, replace=False)
    noise_mask = np.zeros(n_samples, dtype=bool)
    noise_mask[noise_indices] = True
    
    # Create noisy labels
    noisy_labels = labels.copy()
    
    # Symmetric noise: flip to other classes uniformly
    for idx in noise_indices:
        current_label = labels[idx]
        other_labels = [l for l in range(num_classes) if l != current_label]
        noisy_labels[idx] = np.random.choice(other_labels)
    
    return noisy_labels, noise_mask


def calculate_metrics(y_true, y_pred, y_prob=None, positive_class=1):
    """
    Calculate classification metrics
    
    Args:
        y_true: true labels
        y_pred: predicted labels
        y_prob: predicted probabilities (optional)
    
    Returns:
        dict of metrics
    """
    from sklearn.metrics import (
        accuracy_score, precision_score, recall_score, 
        f1_score, confusion_matrix, roc_auc_score
    )
    
    # è®ºæ–‡å£å¾„ï¼šæ¶æ„ç±»(positive_class)ä¸ºæ­£ç±»çš„å•ç±»æŒ‡æ ‡
    precision_pos = precision_score(y_true, y_pred, pos_label=positive_class, zero_division=0)
    recall_pos = recall_score(y_true, y_pred, pos_label=positive_class, zero_division=0)
    f1_pos = f1_score(y_true, y_pred, pos_label=positive_class, zero_division=0)
    
    metrics = {
        'accuracy': accuracy_score(y_true, y_pred),
        'precision_macro': precision_score(y_true, y_pred, average='macro', zero_division=0),
        'recall_macro': recall_score(y_true, y_pred, average='macro', zero_division=0),
        'f1_macro': f1_score(y_true, y_pred, average='macro', zero_division=0),
        'f1_weighted': f1_score(y_true, y_pred, average='weighted', zero_division=0),
        'precision_pos': precision_pos,
        'recall_pos': recall_pos,
        'f1_pos': f1_pos,
        'confusion_matrix': confusion_matrix(y_true, y_pred)
    }
    
    # Add AUC if probabilities are provided
    if y_prob is not None:
        try:
            if y_prob.shape[1] == 2:  # Binary classification
                metrics['auc'] = roc_auc_score(y_true, y_prob[:, 1])
            else:
                metrics['auc'] = roc_auc_score(y_true, y_prob, multi_class='ovr', average='macro')
        except:
            metrics['auc'] = 0.0
    
    return metrics


def print_metrics(metrics, logger=None):
    """Pretty print metrics"""
    msg = "\n" + "="*50 + "\n"
    msg += "Performance Metrics:\n"
    msg += "="*50 + "\n"
    msg += f"Accuracy:  {metrics['accuracy']:.4f}\n"
    msg += f"Precision (pos=1): {metrics['precision_pos']:.4f}\n"
    msg += f"Recall    (pos=1): {metrics['recall_pos']:.4f}\n"
    msg += f"F1 (pos=1):        {metrics['f1_pos']:.4f}  # è®ºæ–‡å£å¾„\n"
    msg += f"F1-Macro:          {metrics['f1_macro']:.4f}\n"
    
    if 'auc' in metrics:
        msg += f"AUC:       {metrics['auc']:.4f}\n"
    
    msg += "\nConfusion Matrix:\n"
    msg += str(metrics['confusion_matrix']) + "\n"
    msg += "="*50 + "\n"
    
    if logger:
        logger.info(msg)
    else:
        print(msg)
    
    return msg


def find_optimal_threshold(y_true, y_prob, metric='f1_binary', positive_class=1):
    """
    åŸºäºŽéªŒè¯é›†è‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜å†³ç­–é˜ˆå€¼
    
    æ ¸å¿ƒæŒ‡æ ‡ï¼šBinary F1-Score (é’ˆå¯¹æ¶æ„ç±»ï¼Œpositive_class=1)
    F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
    
    ç­–ç•¥æ”¹è¿›ï¼šä½¿ç”¨ Precision-Recall æ›²çº¿ç›´æŽ¥å¯»æ‰¾ F1 å³°å€¼
    - ä¹‹å‰çš„æ–¹æ³•ï¼ˆçº¦ç™»æŒ‡æ•°ï¼‰å€¾å‘äºŽä¼˜åŒ– Accuracyï¼Œç‰ºç‰²æ¶æ„ç±»å¬å›žçŽ‡
    - æ–°æ–¹æ³•ç›´æŽ¥æœ€å¤§åŒ– Binary F1-Scoreï¼Œåœ¨ Precision å’Œ Recall ä¹‹é—´æ‰¾åˆ°æœ€ä½³å¹³è¡¡
    
    Args:
        y_true: (N,) çœŸå®žæ ‡ç­¾
        y_prob: (N, 2) æˆ– (N,) é¢„æµ‹æ¦‚çŽ‡
               - å¦‚æžœæ˜¯ (N, 2)ï¼Œå– positive_class åˆ—çš„æ¦‚çŽ‡
               - å¦‚æžœæ˜¯ (N,)ï¼Œç›´æŽ¥ä½¿ç”¨
        metric: str, ä¼˜åŒ–æŒ‡æ ‡
                - 'f1_binary': Binary F1-Score (é’ˆå¯¹ positive_class)ã€é»˜è®¤ï¼ŒæŽ¨èã€‘
                - 'youden': çº¦ç™»æŒ‡æ•° (TPR - FPR)
                - 'f1_macro': å®å¹³å‡F1
        positive_class: int, æ­£ç±»æ ‡ç­¾ï¼ˆé»˜è®¤1ï¼Œæ¶æ„æµé‡ï¼‰
    
    Returns:
        optimal_threshold: float, æœ€ä¼˜é˜ˆå€¼
        optimal_metric: float, æœ€ä¼˜æŒ‡æ ‡å€¼
        threshold_metrics: dict, æ‰€æœ‰é˜ˆå€¼å¯¹åº”çš„æŒ‡æ ‡å€¼
    """
    from sklearn.metrics import precision_recall_curve, confusion_matrix, f1_score
    
    # æå–æ­£ç±»æ¦‚çŽ‡
    if len(y_prob.shape) == 2:
        if y_prob.shape[1] == 2:
            prob_positive = y_prob[:, positive_class]
        else:
            raise ValueError(f"y_prob shape {y_prob.shape} not supported")
    else:
        prob_positive = y_prob
    
    if metric == 'f1_binary':
        # ðŸš€ ç­–ç•¥ä¼˜åŒ–ï¼šä½¿ç”¨ Precision-Recall æ›²çº¿ç›´æŽ¥å¯»æ‰¾ F1 å³°å€¼
        precision, recall, thresholds = precision_recall_curve(y_true, prob_positive)
        
        # è®¡ç®—æ¯ä¸ªé˜ˆå€¼å¯¹åº”çš„ F1 Score
        # F1 = 2 * (P * R) / (P + R)
        # æ³¨æ„ï¼šprecision_recall_curve è¿”å›žçš„æ•°ç»„æ¯” thresholds å¤šä¸€ä¸ªå…ƒç´ 
        f1_scores = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1] + 1e-8)
        
        # æ‰¾åˆ° F1 æœ€é«˜çš„ç‚¹
        best_idx = np.argmax(f1_scores)
        best_threshold = thresholds[best_idx]
        best_f1 = f1_scores[best_idx]
        
        # æž„å»ºè¿”å›žçš„ metrics_dictï¼ˆåŒ…å«æ‰€æœ‰é˜ˆå€¼ç‚¹ï¼‰
        metrics_dict = {
            'threshold': thresholds.tolist(),
            'precision': precision[:-1].tolist(),
            'recall': recall[:-1].tolist(),
            'f1_binary': f1_scores.tolist()
        }
        
        return best_threshold, best_f1, metrics_dict
    
    else:
        # å¯¹äºŽå…¶ä»–æŒ‡æ ‡ï¼Œä½¿ç”¨åŽŸæœ‰çš„ç½‘æ ¼æœç´¢æ–¹æ³•
        thresholds = np.arange(0.01, 1.0, 0.01)
        metrics_dict = {
            'threshold': [],
            'youden_j': [],
            'tpr': [],
            'fpr': [],
            'f1_binary': [],
            'f1_macro': [],
            'precision': [],
            'recall': []
        }
        
        best_threshold = 0.5
        best_metric = -np.inf
        
        for threshold in thresholds:
            # æ ¹æ®é˜ˆå€¼ç”Ÿæˆé¢„æµ‹
            y_pred = (prob_positive >= threshold).astype(int)
            
            # è®¡ç®—æ··æ·†çŸ©é˜µ
            tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()
            
            # è®¡ç®—å„é¡¹æŒ‡æ ‡
            tpr = tp / (tp + fn) if (tp + fn) > 0 else 0.0
            fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            recall = tpr
            
            # çº¦ç™»æŒ‡æ•°
            youden_j = tpr - fpr
            
            # Binary F1åˆ†æ•°
            f1_binary = f1_score(y_true, y_pred, pos_label=positive_class, zero_division=0)
            
            # Macro F1åˆ†æ•°
            f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)
            
            # ä¿å­˜æŒ‡æ ‡
            metrics_dict['threshold'].append(threshold)
            metrics_dict['youden_j'].append(youden_j)
            metrics_dict['tpr'].append(tpr)
            metrics_dict['fpr'].append(fpr)
            metrics_dict['f1_binary'].append(f1_binary)
            metrics_dict['f1_macro'].append(f1_macro)
            metrics_dict['precision'].append(precision)
            metrics_dict['recall'].append(recall)
            
            # æ ¹æ®é€‰æ‹©çš„æŒ‡æ ‡æ‰¾åˆ°æœ€ä¼˜é˜ˆå€¼
            if metric == 'youden':
                current_metric = youden_j
            elif metric == 'f1_macro':
                current_metric = f1_macro
            else:
                raise ValueError(f"Unknown metric: {metric}. Use 'f1_binary', 'youden', or 'f1_macro'")
            
            if current_metric > best_metric:
                best_metric = current_metric
                best_threshold = threshold
        
        return best_threshold, best_metric, metrics_dict


def save_checkpoint(model, optimizer, epoch, metrics, filepath):
    """Save model checkpoint"""
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'metrics': metrics
    }
    
    torch.save(checkpoint, filepath)


def load_checkpoint(filepath, model, optimizer=None):
    """Load model checkpoint"""
    try:
        checkpoint = torch.load(filepath, map_location='cpu', weights_only=True)
    except TypeError:
        checkpoint = torch.load(filepath, map_location='cpu')
    
    model.load_state_dict(checkpoint['model_state_dict'])
    
    if optimizer is not None and 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    return checkpoint.get('epoch', 0), checkpoint.get('metrics', {})


class EarlyStopping:
    """Early stopping to stop training when validation performance stops improving"""
    
    def __init__(self, patience=10, min_delta=0.0, mode='max'):
        """
        Args:
            patience: how many epochs to wait before stopping
            min_delta: minimum change to qualify as an improvement
            mode: 'max' or 'min' - whether higher or lower is better
        """
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        
    def __call__(self, score):
        if self.best_score is None:
            self.best_score = score
            return False
        
        if self.mode == 'max':
            if score > self.best_score + self.min_delta:
                self.best_score = score
                self.counter = 0
            else:
                self.counter += 1
        else:  # mode == 'min'
            if score < self.best_score - self.min_delta:
                self.best_score = score
                self.counter = 0
            else:
                self.counter += 1
        
        if self.counter >= self.patience:
            self.early_stop = True
            return True
        
        return False


def encode_tcp_flags(flags_dict):
    """
    Encode TCP flags to normalized float value
    
    Args:
        flags_dict: dict with keys 'SYN', 'FIN', 'RST', 'PSH', 'ACK'
    
    Returns:
        float in [0, 1]
    """
    syn = 1.0 if flags_dict.get('SYN', False) else 0.0
    fin = 1.0 if flags_dict.get('FIN', False) else 0.0
    rst = 1.0 if flags_dict.get('RST', False) else 0.0
    psh = 1.0 if flags_dict.get('PSH', False) else 0.0
    ack = 1.0 if flags_dict.get('ACK', False) else 0.0
    
    val = (syn * 16 + fin * 8 + rst * 4 + psh * 2 + ack) / 31.0
    return val

