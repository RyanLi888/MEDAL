"""
MEDAL-Lite Training Script
Implements the complete 3-stage training pipeline
"""
import sys
import os
from pathlib import Path

# Ensure project root is on sys.path when running as a script
project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

import torch
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import numpy as np
import argparse
from datetime import datetime

from MoudleCode.utils.config import config
from MoudleCode.utils.helpers import (
    set_seed, setup_logger, inject_label_noise,
    calculate_metrics, print_metrics, save_checkpoint, find_optimal_threshold
)
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, f1_score, silhouette_score
from MoudleCode.utils.visualization import (
    plot_feature_space, plot_noise_correction_comparison, plot_training_history
)
from MoudleCode.preprocessing.pcap_parser import load_dataset
from MoudleCode.feature_extraction.backbone import (
    MicroBiMambaBackbone, SimMTMLoss
)
from MoudleCode.feature_extraction.traffic_augmentation import DualViewAugmentation
from MoudleCode.feature_extraction.instance_contrastive import (
    InstanceContrastiveLearning, HybridPretrainingLoss
)
from MoudleCode.utils.checkpoint import load_state_dict_shape_safe

# å¯¼å…¥é¢„å¤„ç†æ¨¡å—
try:
    from scripts.utils.preprocess import check_preprocessed_exists, load_preprocessed, preprocess_train
    PREPROCESS_AVAILABLE = True
except ImportError:
    PREPROCESS_AVAILABLE = False
from MoudleCode.label_correction.hybrid_court import HybridCourt
from MoudleCode.data_augmentation.tabddpm import TabDDPM
from MoudleCode.classification.dual_stream import MEDAL_Classifier, DualStreamLoss

def stage1_pretrain_backbone(backbone, train_loader, config, logger):
    """
    Stage 1: Pre-train backbone with SimMTM only (unsupervised)
    
    Args:
        backbone: MicroBiMambaBackbone model
        train_loader: DataLoader with X only (no labels needed for SimMTM)
        config: configuration object
        logger: logger
        
    Returns:
        backbone: pre-trained backbone
    """
    # Clear GPU cache before starting
    import torch
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        logger.info("âœ“ GPU ç¼“å­˜å·²æ¸…ç†")
    
    logger.info("="*70)
    logger.info("STAGE 1: Pre-training Backbone")
    logger.info("="*70)
    logger.info(f"ç›®æ ‡: è®­ç»ƒMicro-Bi-Mambaéª¨å¹²ç½‘ç»œï¼Œå­¦ä¹ æµé‡ç‰¹å¾è¡¨ç¤º")
    logger.info(f"è®­ç»ƒè½®æ•°: {config.PRETRAIN_EPOCHS} epochs")
    
    # æ˜¾ç¤ºå®é™…ä½¿ç”¨çš„æ‰¹æ¬¡å¤§å°
    actual_batch_size = train_loader.batch_size
    logger.info(f"æ‰¹æ¬¡å¤§å°: {actual_batch_size}")
    
    logger.info(f"å­¦ä¹ ç‡: {config.PRETRAIN_LR}")
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨å®ä¾‹å¯¹æ¯”å­¦ä¹ 
    use_instance_contrastive = getattr(config, 'USE_INSTANCE_CONTRASTIVE', False)
    contrastive_method = getattr(config, 'CONTRASTIVE_METHOD', 'infonce')
    if use_instance_contrastive:
        method = contrastive_method
        logger.info(f"ä¼˜åŒ–ç›®æ ‡: SimMTM + {str(method).upper()} (å®ä¾‹å¯¹æ¯”å­¦ä¹ )")
        if str(method).lower() in ['infonce', 'nnclr']:
            logger.info(f"  - æ¸©åº¦: {config.INFONCE_TEMPERATURE}")
        logger.info(f"  - æƒé‡: {config.INFONCE_LAMBDA}")
        
        # NNCLR é»˜è®¤å¯ç”¨æ¢¯åº¦ç´¯ç§¯ï¼ˆç”± PRETRAIN_GRADIENT_ACCUMULATION_STEPS æ§åˆ¶ï¼‰
        if str(method).lower() == 'nnclr':
            gradient_accumulation_steps = int(getattr(config, 'PRETRAIN_GRADIENT_ACCUMULATION_STEPS', 1))
            if gradient_accumulation_steps > 1:
                effective_batch_size = actual_batch_size * gradient_accumulation_steps
                logger.info(f"  - æ¢¯åº¦ç´¯ç§¯: {gradient_accumulation_steps} æ­¥")
                logger.info(f"  - æœ‰æ•ˆæ‰¹æ¬¡: {actual_batch_size} Ã— {gradient_accumulation_steps} = {effective_batch_size}")
    else:
        logger.info(f"ä¼˜åŒ–ç›®æ ‡: SimMTM (æ©ç é‡æ„)")
    logger.info("")
    logger.info("ğŸ“¥ è¾“å…¥æ•°æ®è·¯å¾„:")
    logger.info(f"  âœ“ è®­ç»ƒæ•°æ®: {config.BENIGN_TRAIN} (æ­£å¸¸), {config.MALICIOUS_TRAIN} (æ¶æ„)")
    logger.info("")
    
    backbone.train()
    backbone.to(config.DEVICE)
    
    # åˆ›å»ºå¢å¼ºå™¨å’ŒæŸå¤±å‡½æ•°
    if use_instance_contrastive:
        # å®ä¾‹å¯¹æ¯”å­¦ä¹ æ¨¡å¼
        augmentation = DualViewAugmentation(config)
        instance_contrastive = InstanceContrastiveLearning(backbone, config).to(config.DEVICE)
        simmtm_loss_fn = SimMTMLoss(mask_rate=config.SIMMTM_MASK_RATE)
        hybrid_loss_fn = HybridPretrainingLoss(
            simmtm_loss=simmtm_loss_fn,
            instance_contrastive=instance_contrastive,
            lambda_infonce=config.INFONCE_LAMBDA
        )
        logger.info(f"âœ“ æ··åˆæŸå¤±å‡½æ•°åˆå§‹åŒ–å®Œæˆ (SimMTM + InfoNCE)")
        
        # ä¼˜åŒ–å™¨åŒ…æ‹¬projection head
        optimizer = optim.AdamW(
            list(backbone.parameters()) + list(instance_contrastive.projection_head.parameters()),
            lr=config.PRETRAIN_LR,
            weight_decay=config.PRETRAIN_WEIGHT_DECAY
        )
    else:
        # åŸå§‹SimMTMæ¨¡å¼
        simmtm_loss_fn = SimMTMLoss(mask_rate=config.SIMMTM_MASK_RATE)
        logger.info(f"âœ“ æŸå¤±å‡½æ•°åˆå§‹åŒ–å®Œæˆ (SimMTMæ©ç ç‡: {config.SIMMTM_MASK_RATE})")
        
        optimizer = optim.AdamW(
            backbone.parameters(),
            lr=config.PRETRAIN_LR,
            weight_decay=config.PRETRAIN_WEIGHT_DECAY
        )
    
    pretrain_min_lr = float(getattr(config, 'PRETRAIN_MIN_LR', 1e-5))
    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=config.PRETRAIN_EPOCHS, eta_min=pretrain_min_lr
    )
    logger.info(f"âœ“ ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨åˆå§‹åŒ–å®Œæˆ")
    logger.info("")
    logger.info("å¼€å§‹è®­ç»ƒ...")
    logger.info("-"*70)
    
    # Training loop
    if use_instance_contrastive:
        history = {'loss': [], 'simmtm': [], 'infonce': []}
    else:
        history = {'loss': [], 'simmtm': []}
    
    use_early_stopping = bool(getattr(config, 'PRETRAIN_EARLY_STOPPING', True))
    es_warmup_epochs = int(getattr(config, 'PRETRAIN_ES_WARMUP_EPOCHS', 50))
    es_patience = int(getattr(config, 'PRETRAIN_ES_PATIENCE', 20))
    es_min_delta = float(getattr(config, 'PRETRAIN_ES_MIN_DELTA', 0.01))

    best_loss = float('inf')
    best_epoch = -1
    best_state = None
    no_improve = 0
    
    # Gradient accumulation (ç”¨äº NNCLRï¼Œä¿æŒæœ‰æ•ˆæ‰¹æ¬¡ä¸å˜)
    use_gradient_accumulation = (
        use_instance_contrastive and
        str(contrastive_method).lower() == 'nnclr' and
        int(getattr(config, 'PRETRAIN_GRADIENT_ACCUMULATION_STEPS', 1)) > 1
    )

    if use_gradient_accumulation:
        gradient_accumulation_steps = int(getattr(config, 'PRETRAIN_GRADIENT_ACCUMULATION_STEPS', 2))
    else:
        gradient_accumulation_steps = 1  # ç¦ç”¨æ¢¯åº¦ç´¯ç§¯

    if use_instance_contrastive:
        method_lower = str(contrastive_method).lower()
        temperature = float(getattr(config, 'INFONCE_TEMPERATURE', getattr(config, 'SUPCON_TEMPERATURE', 0.1)))
        lambda_infonce = float(getattr(config, 'INFONCE_LAMBDA', 1.0))
        effective_batch_size = int(actual_batch_size) * int(gradient_accumulation_steps)
        logger.info("[Stage 1] Contrastive config:")
        logger.info(f"  - method: {str(contrastive_method).upper()}")
        logger.info(f"  - temperature: {temperature}")
        logger.info(f"  - lambda_infonce: {lambda_infonce}")
        logger.info(f"  - per-step batch_size: {int(actual_batch_size)}")
        logger.info(f"  - gradient_accumulation_steps: {int(gradient_accumulation_steps)}")
        logger.info(f"  - effective_batch_size: {effective_batch_size}")
        if method_lower in ['infonce', 'simclr', 'nnclr']:
            approx_negatives = max(2 * int(actual_batch_size) - 2, 0)
            logger.info(f"  - negatives per anchor (within step): ~{approx_negatives}")
        if method_lower == 'nnclr':
            nnclr_queue_size = int(getattr(config, 'NNCLR_QUEUE_SIZE', 4096))
            logger.info(f"  - nnclr queue size: {nnclr_queue_size}")

    for epoch in range(config.PRETRAIN_EPOCHS):
        epoch_loss = 0.0
        epoch_simmtm = 0.0
        epoch_infonce = 0.0
        
        for batch_idx, batch_data in enumerate(train_loader):
            # TensorDataset with single tensor returns the tensor directly
            # TensorDataset with multiple tensors returns a tuple
            if isinstance(batch_data, (list, tuple)):
                X_batch = batch_data[0]  # Get first element (X)
            else:
                X_batch = batch_data  # Already a single tensor
            X_batch = X_batch.to(config.DEVICE)
            
            # Only zero grad at the start of accumulation
            if batch_idx % gradient_accumulation_steps == 0:
                optimizer.zero_grad()
            
            if use_instance_contrastive:
                # å¢å¼ºè§†å›¾
                x_view1, x_view2 = augmentation(X_batch)
                
                # æ··åˆæŸå¤±
                loss, loss_dict = hybrid_loss_fn(
                    backbone=backbone,
                    x_original=X_batch,
                    x_view1=x_view1,
                    x_view2=x_view2,
                    epoch=epoch
                )
                
                epoch_simmtm += loss_dict['simmtm']
                epoch_infonce += loss_dict['infonce']
            else:
                # åŸå§‹SimMTMæ¨¡å¼
                loss = simmtm_loss_fn(backbone, X_batch)
                epoch_simmtm += loss.item()
            
            # Scale loss by accumulation steps
            loss = loss / gradient_accumulation_steps
            loss.backward()
            
            # Only step optimizer after accumulation
            if (batch_idx + 1) % gradient_accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):
                optimizer.step()
            
            epoch_loss += loss.item() * gradient_accumulation_steps  # Restore original scale for logging
        
        scheduler.step()
        
        # Average losses
        n_batches = len(train_loader)
        epoch_loss /= n_batches
        epoch_simmtm /= n_batches
        
        history['loss'].append(epoch_loss)
        history['simmtm'].append(epoch_simmtm)
        
        if use_instance_contrastive:
            epoch_infonce /= n_batches
            history['infonce'].append(epoch_infonce)
        
        # æ¯ä¸ªepochéƒ½è¾“å‡ºæ—¥å¿—ï¼ˆä¾¿äºç›‘æ§ï¼‰
        progress = (epoch + 1) / config.PRETRAIN_EPOCHS * 100
        if use_instance_contrastive:
            method_name = str(contrastive_method).upper()
            logger.info(f"[Stage 1] Epoch [{epoch+1}/{config.PRETRAIN_EPOCHS}] ({progress:.1f}%) | "
                       f"Loss: {epoch_loss:.4f} | "
                       f"SimMTM: {epoch_simmtm:.4f} | "
                       f"{method_name}: {epoch_infonce:.4f} | "
                       f"LR: {scheduler.get_last_lr()[0]:.6f}")
        else:
            logger.info(f"[Stage 1] Epoch [{epoch+1}/{config.PRETRAIN_EPOCHS}] ({progress:.1f}%) | "
                       f"Loss: {epoch_loss:.4f} | "
                       f"SimMTM: {epoch_simmtm:.4f} | "
                       f"LR: {scheduler.get_last_lr()[0]:.6f}")

        improved = (best_loss - epoch_loss) > es_min_delta
        if improved:
            best_loss = float(epoch_loss)
            best_epoch = int(epoch + 1)
            best_state = {k: v.detach().cpu().clone() for k, v in backbone.state_dict().items()}
            no_improve = 0
        else:
            no_improve += 1

        if use_early_stopping and (epoch + 1) >= es_warmup_epochs and no_improve >= es_patience:
            logger.info(
                f"[Stage 1] EarlyStopping triggered at epoch {epoch+1}: "
                f"best_loss={best_loss:.4f} (epoch {best_epoch}), "
                f"no_improve={no_improve}, min_delta={es_min_delta}"
            )
            break

    if best_state is not None:
        load_state_dict_shape_safe(backbone, best_state, logger, prefix="backbone(best)")
        backbone.to(config.DEVICE)
    
    logger.info("-"*70)
    logger.info("âœ“ Stage 1 å®Œæˆ: éª¨å¹²ç½‘ç»œé¢„è®­ç»ƒå®Œæˆ")
    logger.info(f"  æœ€ç»ˆæŸå¤±: {history['loss'][-1]:.4f}")
    if best_epoch > 0:
        logger.info(f"  æœ€ä½³æŸå¤±: {best_loss:.4f} (epoch {best_epoch})")
    logger.info(f"  è®­ç»ƒäº† {len(history['loss'])} ä¸ªepoch")
    logger.info("")
    
    # ç”Ÿæˆbackboneæ–‡ä»¶åï¼šbackbone_{method}_{dim}d_{n_samples}.pth
    # method: SimMTM æˆ– SimCLR (å¦‚æœå¯ç”¨å®ä¾‹å¯¹æ¯”å­¦ä¹ )
    # dim: æ¨¡å‹ç»´åº¦ (MODEL_DIM)
    n_samples = len(train_loader.dataset)
    model_dim = config.MODEL_DIM
    if use_instance_contrastive:
        method = getattr(config, 'CONTRASTIVE_METHOD', 'infonce')
        method_name = f"SimMTM_{str(method).upper()}"
    else:
        method_name = "SimMTM"
    
    backbone_filename = f"backbone_{method_name}_{model_dim}d_{n_samples}.pth"
    
    logger.info("ğŸ“ è¾“å‡ºæ–‡ä»¶è·¯å¾„:")
    # Save backbone to feature_extraction module directory
    backbone_path = os.path.join(config.FEATURE_EXTRACTION_DIR, "models", backbone_filename)
    torch.save(backbone.state_dict(), backbone_path)
    logger.info(f"  âœ“ éª¨å¹²ç½‘ç»œæ¨¡å‹: {backbone_path}")
    logger.info(f"    (å‘½åæ ¼å¼: backbone_{{æ–¹æ³•}}_{{ç»´åº¦}}d_{{æ ·æœ¬æ•°}}.pth)")
    
    # åŒæ—¶ä¿å­˜ä¸€ä¸ªé»˜è®¤åç§°çš„å‰¯æœ¬ï¼ˆå‘åå…¼å®¹ï¼‰
    default_backbone_path = os.path.join(config.FEATURE_EXTRACTION_DIR, "models", "backbone_pretrained.pth")
    torch.save(backbone.state_dict(), default_backbone_path)
    logger.info(f"  âœ“ é»˜è®¤å‰¯æœ¬: {default_backbone_path}")
    logger.info(f"    (ç”¨äºå‘åå…¼å®¹)")
    
    return backbone, history


def stage2_label_correction_and_augmentation(backbone, X_train, y_train_noisy, y_train_clean, config, logger, stage2_mode='standard'):
    """
    Stage 2: Label correction and data augmentation
    
    Args:
        backbone: Pre-trained frozen backbone
        X_train: (N, L, D) training sequences
        y_train_noisy: (N,) noisy labels
        y_train_clean: (N,) clean labels (for evaluation only)
        config: configuration object
        logger: logger
        
    Returns:
        X_augmented: augmented dataset
        y_augmented: augmented labels
        correction_stats: statistics about correction
    """
    logger.info("")
    logger.info("="*70)
    logger.info("STAGE 2: Label Correction & Data Augmentation")
    logger.info("="*70)
    logger.info(f"ç›®æ ‡: çŸ«æ­£æ ‡ç­¾å™ªå£°å¹¶ç”Ÿæˆå¢å¼ºæ ·æœ¬")
    logger.info(f"æ­¥éª¤: 1) ç‰¹å¾æå– 2) Hybrid Courtæ ‡ç­¾çŸ«æ­£ 3) TabDDPMæ•°æ®å¢å¼º")
    logger.info("")
    logger.info("ğŸ“¥ è¾“å…¥æ•°æ®è·¯å¾„:")
    logger.info(f"  âœ“ è®­ç»ƒæ•°æ®: {config.BENIGN_TRAIN} (æ­£å¸¸), {config.MALICIOUS_TRAIN} (æ¶æ„)")
    backbone_path = os.path.join(config.FEATURE_EXTRACTION_DIR, "models", "backbone_pretrained.pth")
    logger.info(f"  âœ“ éª¨å¹²ç½‘ç»œæ¨¡å‹: {backbone_path}")
    logger.info("")
    
    # Freeze backbone and extract features
    backbone.to(config.DEVICE)  # ç¡®ä¿ backbone åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
    backbone.freeze()
    backbone.eval()
    logger.info("âœ“ éª¨å¹²ç½‘ç»œå·²å†»ç»“ï¼Œå¼€å§‹ç‰¹å¾æå–...")
    
    logger.info(f"æ­£åœ¨ä» {len(X_train)} ä¸ªè®­ç»ƒæ ·æœ¬ä¸­æå–ç‰¹å¾...")
    with torch.no_grad():
        X_tensor = torch.FloatTensor(X_train).to(config.DEVICE)
        
        # Extract features in batches
        features_list = []
        batch_size = 64
        total_batches = (len(X_tensor) + batch_size - 1) // batch_size
        
        for i in range(0, len(X_tensor), batch_size):
            batch_idx = i // batch_size + 1
            X_batch = X_tensor[i:i+batch_size]
            z_batch = backbone(X_batch, return_sequence=False)
            features_list.append(z_batch.cpu().numpy())
            
            if batch_idx % 10 == 0 or batch_idx == total_batches:
                progress = batch_idx / total_batches * 100
                logger.info(f"  ç‰¹å¾æå–è¿›åº¦: {batch_idx}/{total_batches} batches ({progress:.1f}%)")
        
        features = np.concatenate(features_list, axis=0)
    
    logger.info(f"âœ“ ç‰¹å¾æå–å®Œæˆ: {features.shape} (æ ·æœ¬æ•°Ã—ç‰¹å¾ç»´åº¦)")
    logger.info("")
    
    # ç”Ÿæˆç‰¹å¾åˆ†å¸ƒå¯è§†åŒ–ï¼ˆStage 2ç‰¹å¾æå–åï¼‰
    logger.info("ğŸ“Š ç”Ÿæˆç‰¹å¾åˆ†å¸ƒå¯è§†åŒ–...")
    feature_dist_path = os.path.join(config.LABEL_CORRECTION_DIR, "figures", "feature_distribution_stage2.png")
    plot_feature_space(features, y_train_clean, feature_dist_path,
                      title="Stage 2: Feature Distribution (After Backbone Extraction)", method='tsne')
    logger.info(f"  âœ“ ç‰¹å¾åˆ†å¸ƒå›¾ (t-SNE): {feature_dist_path}")
    logger.info("")

    logger.info("="*70)
    logger.info("ğŸ“ ç‰¹å¾å¯åˆ†æ€§è¯„ä¼° (Feature Separability)")
    logger.info("="*70)
    try:
        X_tr, X_te, y_tr, y_te = train_test_split(
            features, y_train_clean,
            test_size=0.2,
            stratify=y_train_clean,
            random_state=config.SEED
        )
        sep_clf = LogisticRegression(max_iter=1000, class_weight='balanced')
        sep_clf.fit(X_tr, y_tr)
        te_proba = sep_clf.predict_proba(X_te)[:, 1]
        te_auc = roc_auc_score(y_te, te_proba)
        te_pred = (te_proba >= 0.5).astype(int)
        te_f1 = f1_score(y_te, te_pred, pos_label=1, zero_division=0)
        sil = silhouette_score(features, y_train_clean) if len(np.unique(y_train_clean)) > 1 else np.nan
        logger.info(f"  LogisticRegression ROC-AUC: {te_auc:.4f}")
        logger.info(f"  LogisticRegression F1@0.5:  {te_f1:.4f}")
        logger.info(f"  Silhouette Score:          {sil:.4f}")
    except Exception as e:
        logger.warning(f"âš  ç‰¹å¾å¯åˆ†æ€§è¯„ä¼°å¤±è´¥: {e}")
    logger.info("")
    logger.info("ğŸ“ è¾“å‡ºæ–‡ä»¶è·¯å¾„:")
    # Save extracted features
    features_path = os.path.join(config.FEATURE_EXTRACTION_DIR, "models", "train_features.npy")
    np.save(features_path, features)
    logger.info(f"  âœ“ æå–çš„ç‰¹å¾: {features_path}")
    
    # Visualize feature space before correction
    save_path = os.path.join(config.FEATURE_EXTRACTION_DIR, "figures", "feature_space_before_correction.png")
    plot_feature_space(features, y_train_clean, save_path, 
                      title="Feature Space (Ground Truth Labels)", method='tsne')
    logger.info(f"  âœ“ ç‰¹å¾ç©ºé—´å¯è§†åŒ–: {save_path}")
    
    # Label correction with Hybrid Court
    logger.info("")
    logger.info("æ­¥éª¤ 2.1: Hybrid Court æ ‡ç­¾å™ªå£°çŸ«æ­£")
    logger.info(f"  è¾“å…¥: {len(y_train_noisy)} ä¸ªæ ·æœ¬ï¼Œå™ªå£°ç‡: {config.LABEL_NOISE_RATE*100:.0f}%")
    logger.info("  æ–¹æ³•: CL (ç½®ä¿¡å­¦ä¹ ) + MADE (å¯†åº¦ä¼°è®¡) + KNN (è¯­ä¹‰æŠ•ç¥¨)")
    logger.info("  å¼€å§‹çŸ«æ­£...")
    
    hybrid_court = HybridCourt(config)

    if stage2_mode == 'clean_augment_only':
        suspected_noise, pred_labels, pred_probs = hybrid_court.cl.fit_predict(features, y_train_clean)
        hybrid_court.made.fit(features, device=config.DEVICE)
        is_dense, density_scores = hybrid_court.made.predict_density(features, device=config.DEVICE)
        hybrid_court.knn.fit(features)
        neighbor_labels, neighbor_consistency = hybrid_court.knn.predict_semantic_label(features, y_train_clean)
        y_corrected = y_train_clean.copy()
        action_mask = np.zeros(len(y_train_clean), dtype=int)
        confidence = pred_probs.max(axis=1)
        correction_weight = np.ones(len(y_train_clean), dtype=np.float32)
        cl_confidence = pred_probs.max(axis=1)
    else:
        y_corrected, action_mask, confidence, correction_weight, density_scores, neighbor_consistency, pred_probs = hybrid_court.correct_labels(
            features, y_train_noisy, device=config.DEVICE
        )
        cl_confidence = pred_probs.max(axis=1)
    
    logger.info("âœ“ æ ‡ç­¾çŸ«æ­£å®Œæˆ")
    
    # Save correction results
    correction_results_path = os.path.join(config.LABEL_CORRECTION_DIR, "models", "correction_results.npz")
    np.savez(correction_results_path,
             y_noisy=y_train_noisy if stage2_mode != 'clean_augment_only' else y_train_clean,
             y_corrected=y_corrected,
             action_mask=action_mask,
             confidence=confidence,
             correction_weight=correction_weight,
             density_scores=density_scores,
             neighbor_consistency=neighbor_consistency,
             pred_probs=pred_probs)
    logger.info("")
    logger.info("ğŸ“ è¾“å‡ºæ–‡ä»¶è·¯å¾„:")
    logger.info(f"  âœ“ æ ‡ç­¾çŸ«æ­£ç»“æœ: {correction_results_path}")
    
    # Visualize correction results
    save_path = os.path.join(config.LABEL_CORRECTION_DIR, "figures", "noise_correction_comparison.png")
    plot_noise_correction_comparison(y_train_clean, y_train_noisy, y_corrected, action_mask, save_path)
    logger.info(f"  âœ“ æ ‡ç­¾çŸ«æ­£å¯¹æ¯”å›¾: {save_path}")
    
    # Calculate correction accuracy
    keep_mask = action_mask != 2  # Exclude dropped samples
    correction_accuracy = (y_corrected[keep_mask] == y_train_clean[keep_mask]).mean()
    logger.info(f"\nLabel Correction Accuracy: {correction_accuracy*100:.2f}%")
    
    correction_stats = {
        'accuracy': correction_accuracy,
        'n_keep': (action_mask == 0).sum(),
        'n_flip': (action_mask == 1).sum(),
        'n_drop': (action_mask == 2).sum(),
        'n_reweight': (action_mask == 3).sum()
    }
    
    # ========================
    # ç”Ÿæˆè¯¦ç»†çš„æ ‡ç­¾çŸ«æ­£åˆ†ææŠ¥å‘Š
    # ========================
    logger.info("")
    logger.info("="*70)
    logger.info("ğŸ“Š ç”Ÿæˆæ ‡ç­¾çŸ«æ­£åˆ†ææŠ¥å‘Š")
    logger.info("="*70)
    
    from datetime import datetime
    
    # å‡†å¤‡åˆ†ææ•°æ®
    n_total = len(y_train_noisy if stage2_mode != 'clean_augment_only' else y_train_clean)
    n_keep = correction_stats['n_keep']
    n_flip = correction_stats['n_flip']
    n_drop = correction_stats['n_drop']
    n_reweight = correction_stats['n_reweight']
    
    # è®¡ç®—å„ç±»åˆ«çš„çŸ«æ­£æƒ…å†µ
    y_noisy_input = y_train_noisy if stage2_mode != 'clean_augment_only' else y_train_clean
    
    # çœŸå®å™ªå£°æ ·æœ¬ï¼ˆground truthï¼‰
    true_noise_mask = (y_noisy_input != y_train_clean)
    n_true_noise = true_noise_mask.sum()
    
    # è¢«è¯†åˆ«ä¸ºå™ªå£°çš„æ ·æœ¬ï¼ˆé¢„æµ‹ï¼‰
    predicted_noise_mask = (action_mask == 1) | (action_mask == 2)
    n_predicted_noise = predicted_noise_mask.sum()
    
    # æ­£ç¡®è¯†åˆ«çš„å™ªå£°ï¼ˆTrue Positiveï¼‰
    tp_noise = (true_noise_mask & predicted_noise_mask).sum()
    # é”™è¯¯è¯†åˆ«çš„å™ªå£°ï¼ˆFalse Positiveï¼‰
    fp_noise = (~true_noise_mask & predicted_noise_mask).sum()
    # æ¼æ£€çš„å™ªå£°ï¼ˆFalse Negativeï¼‰
    fn_noise = (true_noise_mask & ~predicted_noise_mask).sum()
    # æ­£ç¡®ä¿ç•™çš„å¹²å‡€æ ·æœ¬ï¼ˆTrue Negativeï¼‰
    tn_noise = (~true_noise_mask & ~predicted_noise_mask).sum()
    
    # è®¡ç®—å™ªå£°æ£€æµ‹æŒ‡æ ‡
    noise_precision = tp_noise / n_predicted_noise if n_predicted_noise > 0 else 0
    noise_recall = tp_noise / n_true_noise if n_true_noise > 0 else 0
    noise_f1 = 2 * noise_precision * noise_recall / (noise_precision + noise_recall) if (noise_precision + noise_recall) > 0 else 0
    
    # çŸ«æ­£åçš„å‡†ç¡®ç‡
    final_accuracy = (y_corrected[keep_mask] == y_train_clean[keep_mask]).mean()
    
    # å„åŠ¨ä½œçš„å‡†ç¡®ç‡
    keep_samples = (action_mask == 0)
    flip_samples = (action_mask == 1)
    reweight_samples = (action_mask == 3)
    
    keep_accuracy = (y_corrected[keep_samples] == y_train_clean[keep_samples]).mean() if keep_samples.sum() > 0 else 0
    flip_accuracy = (y_corrected[flip_samples] == y_train_clean[flip_samples]).mean() if flip_samples.sum() > 0 else 0
    reweight_accuracy = (y_corrected[reweight_samples] == y_train_clean[reweight_samples]).mean() if reweight_samples.sum() > 0 else 0
    
    # ç”ŸæˆMarkdownæŠ¥å‘Š
    report_path = os.path.join(config.LABEL_CORRECTION_DIR, "models", "correction_analysis_report.md")
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# æ ‡ç­¾å™ªå£°çŸ«æ­£åˆ†ææŠ¥å‘Š\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"**å™ªå£°ç‡**: {config.LABEL_NOISE_RATE*100:.1f}%\n\n")
        f.write(f"**çŸ«æ­£æ–¹æ³•**: Hybrid Court (CL + MADE + KNN)\n\n")
        
        f.write("---\n\n")
        f.write("## 1. æ•°æ®é›†æ¦‚è§ˆ\n\n")
        f.write(f"- **æ€»æ ·æœ¬æ•°**: {n_total}\n")
        f.write(f"- **çœŸå®å™ªå£°æ ·æœ¬æ•°**: {n_true_noise} ({n_true_noise/n_total*100:.2f}%)\n")
        f.write(f"- **å¹²å‡€æ ·æœ¬æ•°**: {n_total - n_true_noise} ({(n_total-n_true_noise)/n_total*100:.2f}%)\n\n")
        
        f.write("### ç±»åˆ«åˆ†å¸ƒ\n\n")
        f.write("| æ ‡ç­¾ç±»å‹ | æ­£å¸¸æ ·æœ¬ | æ¶æ„æ ·æœ¬ | æ€»è®¡ |\n")
        f.write("|---------|---------|---------|------|\n")
        f.write(f"| çœŸå®æ ‡ç­¾ | {(y_train_clean==0).sum()} | {(y_train_clean==1).sum()} | {len(y_train_clean)} |\n")
        f.write(f"| å™ªå£°æ ‡ç­¾ | {(y_noisy_input==0).sum()} | {(y_noisy_input==1).sum()} | {len(y_noisy_input)} |\n")
        f.write(f"| çŸ«æ­£åæ ‡ç­¾ | {(y_corrected==0).sum()} | {(y_corrected==1).sum()} | {len(y_corrected)} |\n\n")
        
        f.write("---\n\n")
        f.write("## 2. çŸ«æ­£åŠ¨ä½œç»Ÿè®¡\n\n")
        f.write("| åŠ¨ä½œ | æ ·æœ¬æ•° | å æ¯” | å‡†ç¡®ç‡ | è¯´æ˜ |\n")
        f.write("|------|--------|------|--------|------|\n")
        f.write(f"| **ä¿æŒ (Keep)** | {n_keep} | {n_keep/n_total*100:.2f}% | {keep_accuracy*100:.2f}% | æ ‡ç­¾å¯ä¿¡ï¼Œä¿æŒä¸å˜ |\n")
        f.write(f"| **ç¿»è½¬ (Flip)** | {n_flip} | {n_flip/n_total*100:.2f}% | {flip_accuracy*100:.2f}% | æ ‡ç­¾é”™è¯¯ï¼Œç¿»è½¬æ ‡ç­¾ |\n")
        f.write(f"| **ä¸¢å¼ƒ (Drop)** | {n_drop} | {n_drop/n_total*100:.2f}% | - | ä¸ç¡®å®šæ€§é«˜ï¼Œä¸¢å¼ƒæ ·æœ¬ |\n")
        f.write(f"| **é‡åŠ æƒ (Reweight)** | {n_reweight} | {n_reweight/n_total*100:.2f}% | {reweight_accuracy*100:.2f}% | å¯ç–‘ä½†ä¿ç•™ï¼Œé™ä½æƒé‡ |\n")
        f.write(f"| **æ€»è®¡** | {n_total} | 100.00% | - | - |\n\n")
        
        f.write("### åŠ¨ä½œåˆ†å¸ƒå¯è§†åŒ–\n\n")
        f.write("```\n")
        f.write(f"ä¿æŒ:   {'â–ˆ' * int(n_keep/n_total*50)} {n_keep/n_total*100:.1f}%\n")
        f.write(f"ç¿»è½¬:   {'â–ˆ' * int(n_flip/n_total*50)} {n_flip/n_total*100:.1f}%\n")
        f.write(f"ä¸¢å¼ƒ:   {'â–ˆ' * int(n_drop/n_total*50)} {n_drop/n_total*100:.1f}%\n")
        f.write(f"é‡åŠ æƒ: {'â–ˆ' * int(n_reweight/n_total*50)} {n_reweight/n_total*100:.1f}%\n")
        f.write("```\n\n")
        
        f.write("---\n\n")
        f.write("## 3. å™ªå£°æ£€æµ‹æ€§èƒ½\n\n")
        f.write("### æ··æ·†çŸ©é˜µï¼ˆå™ªå£°æ£€æµ‹ï¼‰\n\n")
        f.write("| | é¢„æµ‹ä¸ºå¹²å‡€ | é¢„æµ‹ä¸ºå™ªå£° |\n")
        f.write("|---|-----------|----------|\n")
        f.write(f"| **çœŸå®å¹²å‡€** | {tn_noise} (TN) | {fp_noise} (FP) |\n")
        f.write(f"| **çœŸå®å™ªå£°** | {fn_noise} (FN) | {tp_noise} (TP) |\n\n")
        
        f.write("### å™ªå£°æ£€æµ‹æŒ‡æ ‡\n\n")
        f.write(f"- **Precision (ç²¾ç¡®ç‡)**: {noise_precision*100:.2f}% - è¯†åˆ«ä¸ºå™ªå£°çš„æ ·æœ¬ä¸­ï¼ŒçœŸæ­£æ˜¯å™ªå£°çš„æ¯”ä¾‹\n")
        f.write(f"- **Recall (å¬å›ç‡)**: {noise_recall*100:.2f}% - æ‰€æœ‰çœŸå®å™ªå£°ä¸­ï¼Œè¢«æ­£ç¡®è¯†åˆ«çš„æ¯”ä¾‹\n")
        f.write(f"- **F1-Score**: {noise_f1*100:.2f}% - ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡\n\n")
        
        f.write("### æ€§èƒ½è¯„ä¼°\n\n")
        if noise_f1 >= 0.8:
            f.write("âœ… **ä¼˜ç§€** - å™ªå£°æ£€æµ‹æ€§èƒ½å¾ˆå¥½ï¼Œå¤§éƒ¨åˆ†å™ªå£°è¢«æ­£ç¡®è¯†åˆ«\n\n")
        elif noise_f1 >= 0.6:
            f.write("âš ï¸ **è‰¯å¥½** - å™ªå£°æ£€æµ‹æ€§èƒ½å°šå¯ï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´\n\n")
        else:
            f.write("âŒ **éœ€æ”¹è¿›** - å™ªå£°æ£€æµ‹æ€§èƒ½è¾ƒå·®ï¼Œå»ºè®®è°ƒæ•´å‚æ•°æˆ–æ–¹æ³•\n\n")
        
        f.write("---\n\n")
        f.write("## 4. çŸ«æ­£æ•ˆæœè¯„ä¼°\n\n")
        f.write(f"- **çŸ«æ­£å‰å‡†ç¡®ç‡**: {(y_noisy_input == y_train_clean).mean()*100:.2f}%\n")
        f.write(f"- **çŸ«æ­£åå‡†ç¡®ç‡**: {final_accuracy*100:.2f}%\n")
        f.write(f"- **å‡†ç¡®ç‡æå‡**: {(final_accuracy - (y_noisy_input == y_train_clean).mean())*100:.2f}%\n\n")
        
        improvement = final_accuracy - (y_noisy_input == y_train_clean).mean()
        if improvement > 0.1:
            f.write("âœ… **æ˜¾è‘—æ”¹å–„** - æ ‡ç­¾çŸ«æ­£æ˜¾è‘—æå‡äº†æ•°æ®è´¨é‡\n\n")
        elif improvement > 0.05:
            f.write("âœ… **æœ‰æ•ˆæ”¹å–„** - æ ‡ç­¾çŸ«æ­£æœ‰æ•ˆæå‡äº†æ•°æ®è´¨é‡\n\n")
        elif improvement > 0:
            f.write("âš ï¸ **è½»å¾®æ”¹å–„** - æ ‡ç­¾çŸ«æ­£ç•¥å¾®æå‡äº†æ•°æ®è´¨é‡\n\n")
        else:
            f.write("âŒ **æ— æ”¹å–„** - æ ‡ç­¾çŸ«æ­£æœªèƒ½æå‡æ•°æ®è´¨é‡ï¼Œå»ºè®®æ£€æŸ¥å‚æ•°\n\n")
        
        f.write("---\n\n")
        f.write("## 5. å„ç»„ä»¶è´¡çŒ®åˆ†æ\n\n")
        
        # CLç»„ä»¶åˆ†æ
        cl_suspected = (pred_probs.max(axis=1) < 0.5).sum()
        f.write(f"### CL (ç½®ä¿¡å­¦ä¹ )\n\n")
        f.write(f"- **è¯†åˆ«å¯ç–‘æ ·æœ¬æ•°**: {cl_suspected}\n")
        f.write(f"- **å¹³å‡ç½®ä¿¡åº¦**: {pred_probs.max(axis=1).mean():.4f}\n")
        f.write(f"- **ä½ç½®ä¿¡åº¦æ ·æœ¬ (<0.5)**: {cl_suspected} ({cl_suspected/n_total*100:.2f}%)\n\n")
        
        # MADEç»„ä»¶åˆ†æ
        dense_samples = (density_scores > 0.5).sum()
        f.write(f"### MADE (å¯†åº¦ä¼°è®¡)\n\n")
        f.write(f"- **é«˜å¯†åº¦æ ·æœ¬æ•°**: {dense_samples} ({dense_samples/n_total*100:.2f}%)\n")
        f.write(f"- **å¹³å‡å¯†åº¦åˆ†æ•°**: {density_scores.mean():.4f}\n")
        f.write(f"- **ä½å¯†åº¦æ ·æœ¬ (<0.3)**: {(density_scores < 0.3).sum()} ({(density_scores < 0.3).sum()/n_total*100:.2f}%)\n\n")
        
        # KNNç»„ä»¶åˆ†æ
        high_consistency = (neighbor_consistency > 0.7).sum()
        f.write(f"### KNN (è¯­ä¹‰æŠ•ç¥¨)\n\n")
        f.write(f"- **é«˜ä¸€è‡´æ€§æ ·æœ¬æ•°**: {high_consistency} ({high_consistency/n_total*100:.2f}%)\n")
        f.write(f"- **å¹³å‡é‚»å±…ä¸€è‡´æ€§**: {neighbor_consistency.mean():.4f}\n")
        f.write(f"- **ä½ä¸€è‡´æ€§æ ·æœ¬ (<0.5)**: {(neighbor_consistency < 0.5).sum()} ({(neighbor_consistency < 0.5).sum()/n_total*100:.2f}%)\n\n")
        
        f.write("---\n\n")
        f.write("## 6. æ ·æœ¬æƒé‡åˆ†å¸ƒ\n\n")
        f.write(f"- **å¹³å‡æƒé‡**: {correction_weight.mean():.4f}\n")
        f.write(f"- **æƒé‡æ ‡å‡†å·®**: {correction_weight.std():.4f}\n")
        f.write(f"- **æœ€å°æƒé‡**: {correction_weight.min():.4f}\n")
        f.write(f"- **æœ€å¤§æƒé‡**: {correction_weight.max():.4f}\n\n")
        
        f.write("### æƒé‡åˆ†å¸ƒç»Ÿè®¡\n\n")
        f.write("| æƒé‡èŒƒå›´ | æ ·æœ¬æ•° | å æ¯” |\n")
        f.write("|---------|--------|------|\n")
        f.write(f"| [0.0, 0.2) | {((correction_weight >= 0.0) & (correction_weight < 0.2)).sum()} | {((correction_weight >= 0.0) & (correction_weight < 0.2)).sum()/n_total*100:.2f}% |\n")
        f.write(f"| [0.2, 0.4) | {((correction_weight >= 0.2) & (correction_weight < 0.4)).sum()} | {((correction_weight >= 0.2) & (correction_weight < 0.4)).sum()/n_total*100:.2f}% |\n")
        f.write(f"| [0.4, 0.6) | {((correction_weight >= 0.4) & (correction_weight < 0.6)).sum()} | {((correction_weight >= 0.4) & (correction_weight < 0.6)).sum()/n_total*100:.2f}% |\n")
        f.write(f"| [0.6, 0.8) | {((correction_weight >= 0.6) & (correction_weight < 0.8)).sum()} | {((correction_weight >= 0.6) & (correction_weight < 0.8)).sum()/n_total*100:.2f}% |\n")
        f.write(f"| [0.8, 1.0] | {((correction_weight >= 0.8) & (correction_weight <= 1.0)).sum()} | {((correction_weight >= 0.8) & (correction_weight <= 1.0)).sum()/n_total*100:.2f}% |\n\n")
        
        f.write("---\n\n")
        f.write("## 7. å»ºè®®ä¸æ€»ç»“\n\n")
        
        if final_accuracy >= 0.95:
            f.write("### âœ… çŸ«æ­£æ•ˆæœä¼˜ç§€\n\n")
            f.write("- æ ‡ç­¾çŸ«æ­£æ•ˆæœå¾ˆå¥½ï¼Œæ•°æ®è´¨é‡é«˜\n")
            f.write("- å¯ä»¥ç›´æ¥ç”¨äºåç»­è®­ç»ƒ\n\n")
        elif final_accuracy >= 0.85:
            f.write("### âœ… çŸ«æ­£æ•ˆæœè‰¯å¥½\n\n")
            f.write("- æ ‡ç­¾çŸ«æ­£æ•ˆæœè¾ƒå¥½ï¼Œå¤§éƒ¨åˆ†å™ªå£°è¢«å¤„ç†\n")
            f.write("- å»ºè®®å…³æ³¨ä½ç½®ä¿¡åº¦æ ·æœ¬\n\n")
        else:
            f.write("### âš ï¸ çŸ«æ­£æ•ˆæœä¸€èˆ¬\n\n")
            f.write("- æ ‡ç­¾çŸ«æ­£æ•ˆæœæœ‰é™ï¼Œä»æœ‰è¾ƒå¤šå™ªå£°\n")
            f.write("- å»ºè®®è°ƒæ•´ä»¥ä¸‹å‚æ•°ï¼š\n")
            f.write("  - å¢åŠ CLçš„ç½®ä¿¡åº¦é˜ˆå€¼\n")
            f.write("  - è°ƒæ•´MADEçš„å¯†åº¦é˜ˆå€¼\n")
            f.write("  - å¢åŠ KNNçš„é‚»å±…æ•°é‡\n\n")
        
        f.write("### å…³é”®å‘ç°\n\n")
        if noise_precision > 0.8 and noise_recall < 0.6:
            f.write("- **é«˜ç²¾ç¡®ç‡ï¼Œä½å¬å›ç‡**: çŸ«æ­£ç­–ç•¥ä¿å®ˆï¼Œæ¼æ£€äº†éƒ¨åˆ†å™ªå£°\n")
            f.write("  - å»ºè®®: é™ä½ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œå¢åŠ å™ªå£°æ£€æµ‹çš„æ•æ„Ÿåº¦\n\n")
        elif noise_precision < 0.6 and noise_recall > 0.8:
            f.write("- **ä½ç²¾ç¡®ç‡ï¼Œé«˜å¬å›ç‡**: çŸ«æ­£ç­–ç•¥æ¿€è¿›ï¼Œè¯¯åˆ¤äº†éƒ¨åˆ†å¹²å‡€æ ·æœ¬\n")
            f.write("  - å»ºè®®: æé«˜ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œå‡å°‘è¯¯åˆ¤\n\n")
        elif noise_precision > 0.7 and noise_recall > 0.7:
            f.write("- **å¹³è¡¡çš„æ£€æµ‹æ€§èƒ½**: ç²¾ç¡®ç‡å’Œå¬å›ç‡éƒ½è¾ƒé«˜ï¼ŒçŸ«æ­£ç­–ç•¥åˆç†\n\n")
        
        if n_drop > n_total * 0.2:
            f.write("- **ä¸¢å¼ƒæ ·æœ¬è¿‡å¤š**: è¶…è¿‡20%çš„æ ·æœ¬è¢«ä¸¢å¼ƒ\n")
            f.write("  - å»ºè®®: æ£€æŸ¥æ•°æ®è´¨é‡æˆ–è°ƒæ•´ä¸¢å¼ƒé˜ˆå€¼\n\n")
        
        f.write("---\n\n")
        f.write("## 8. è¾“å‡ºæ–‡ä»¶\n\n")
        f.write(f"- **çŸ«æ­£ç»“æœ**: `{correction_results_path}`\n")
        f.write(f"- **å¯¹æ¯”å›¾**: `{os.path.join(config.LABEL_CORRECTION_DIR, 'figures', 'noise_correction_comparison.png')}`\n")
        f.write(f"- **ç‰¹å¾ç©ºé—´å›¾**: `{os.path.join(config.FEATURE_EXTRACTION_DIR, 'figures', 'feature_space_before_correction.png')}`\n")
        f.write(f"- **æœ¬æŠ¥å‘Š**: `{report_path}`\n\n")
        
        f.write("---\n\n")
        f.write("*æŠ¥å‘Šç”±MEDAL-Liteè‡ªåŠ¨ç”Ÿæˆ*\n")
    
    logger.info(f"  âœ“ æ ‡ç­¾çŸ«æ­£åˆ†ææŠ¥å‘Š: {report_path}")
    logger.info("")
    
    # ä½¿ç”¨å…¨éƒ¨å¹²å‡€æ ·æœ¬è¿›è¡Œè®­ç»ƒä¸å¢å¼ºï¼ˆå–æ¶ˆéªŒè¯é›†ï¼‰
    X_clean = X_train[keep_mask]
    y_clean = y_corrected[keep_mask]
    weights_clean = correction_weight[keep_mask]
    action_clean = action_mask[keep_mask]
    density_clean = density_scores[keep_mask]
    cl_conf_clean = cl_confidence[keep_mask]
    knn_conf_clean = neighbor_consistency[keep_mask]
    
    # Data augmentation with TabDDPM
    logger.info("")
    logger.info("æ­¥éª¤ 2.2: TabDDPM æ•°æ®å¢å¼ºæ¨¡å‹è®­ç»ƒ")
    logger.info(f"  ç›®æ ‡: å­¦ä¹ æµé‡ç‰¹å¾åˆ†å¸ƒï¼Œç”Ÿæˆç¬¦åˆåè®®é€»è¾‘çš„åˆæˆæ ·æœ¬")
    ddpm_epochs = int(getattr(config, 'DDPM_EPOCHS', 100))
    logger.info(f"  è®­ç»ƒè½®æ•°: {ddpm_epochs} epochs")
    logger.info(f"  å¼•å¯¼ç­–ç•¥: æ¶æ„æ ·æœ¬(w={config.GUIDANCE_MALICIOUS}), è‰¯æ€§æ ·æœ¬(w={config.GUIDANCE_BENIGN})")
    logger.info("  å¼€å§‹è®­ç»ƒ...")
    
    tabddpm = TabDDPM(config).to(config.DEVICE)

    # Fit scaler on packet-level features (exclude zero-padding packets)
    X_packets_all = X_clean.reshape(-1, X_clean.shape[-1])
    vm_idx = getattr(config, 'VALID_MASK_INDEX', None)
    if vm_idx is not None and int(vm_idx) >= 0 and int(vm_idx) < X_packets_all.shape[1]:
        packet_mask = X_packets_all[:, int(vm_idx)] > 0.5
    else:
        packet_mask = np.any(X_packets_all != 0.0, axis=1)
    X_packets_valid = X_packets_all[packet_mask]
    if X_packets_valid.shape[0] == 0:
        X_packets_valid = X_packets_all
        packet_mask = None
    tabddpm.fit_scaler(X_packets_valid)
    
    # Train TabDDPM
    optimizer_ddpm = optim.AdamW(tabddpm.parameters(), lr=1e-4)
    n_epochs_ddpm = ddpm_epochs
	
    ddpm_use_early_stopping = bool(getattr(config, 'DDPM_EARLY_STOPPING', True))
    ddpm_es_warmup_epochs = int(getattr(config, 'DDPM_ES_WARMUP_EPOCHS', 20))
    ddpm_es_patience = int(getattr(config, 'DDPM_ES_PATIENCE', 30))
    ddpm_es_min_delta = float(getattr(config, 'DDPM_ES_MIN_DELTA', 0.001))
	
    ddpm_best_loss = float('inf')
    ddpm_best_epoch = -1
    ddpm_best_state = None
    ddpm_no_improve = 0
    
    # ä½¿ç”¨ packet-level æ•°æ®è®­ç»ƒ TabDDPMï¼ˆé¿å…åªçœ‹ç¬¬ä¸€ä¸ªåŒ…å¯¼è‡´åˆ†å¸ƒåç§»ï¼‰
    # åŒæ—¶è¿‡æ»¤æ‰ padding=0 çš„æ— æ•ˆåŒ…ï¼Œé¿å…æ¨¡å‹å­¦åˆ°å¤§é‡â€œå…¨é›¶åŒ…â€å¯¼è‡´ Length/IAT/BurstSize åˆ†å¸ƒæ¼‚ç§»
    X_packets = X_packets_valid  # (n_valid, D)
    y_packets_all = np.repeat(y_clean, X_clean.shape[1])    # (N*L,)
    y_packets = y_packets_all[packet_mask] if packet_mask is not None else y_packets_all
    dataset_ddpm = TensorDataset(
        torch.FloatTensor(X_packets),
        torch.LongTensor(y_packets)
    )
    loader_ddpm = DataLoader(dataset_ddpm, batch_size=2048, shuffle=True)
    
    tabddpm.train()
    for epoch in range(n_epochs_ddpm):
        epoch_loss = 0.0
        for X_batch, y_batch in loader_ddpm:
            x_0 = X_batch.to(config.DEVICE)  # (B, D)
            y_batch = y_batch.to(config.DEVICE)
            
            optimizer_ddpm.zero_grad()
            loss = tabddpm.compute_loss(
                x_0, y_batch,
                mask_prob=config.MASK_PROBABILITY,
                mask_lambda=config.MASK_LAMBDA,
                p_uncond=0.2  # Classifier-free guidance training
            )
            loss.backward()
            optimizer_ddpm.step()
            
            epoch_loss += loss.item()
        
        # æ¯ä¸ªepochè¾“å‡º
        avg_loss = epoch_loss / len(loader_ddpm)
        progress = (epoch + 1) / n_epochs_ddpm * 100
        logger.info(f"[TabDDPM] Epoch [{epoch+1}/{n_epochs_ddpm}] ({progress:.1f}%) | Loss: {avg_loss:.4f}")
	
        improved = (ddpm_best_loss - float(avg_loss)) > ddpm_es_min_delta
        if improved:
            ddpm_best_loss = float(avg_loss)
            ddpm_best_epoch = int(epoch + 1)
            ddpm_best_state = {k: v.detach().cpu().clone() for k, v in tabddpm.state_dict().items()}
            ddpm_no_improve = 0
        else:
            ddpm_no_improve += 1
	
        if ddpm_use_early_stopping and (epoch + 1) >= ddpm_es_warmup_epochs and ddpm_no_improve >= ddpm_es_patience:
            logger.info(
                f"[TabDDPM] EarlyStopping triggered at epoch {epoch+1}: "
                f"best_loss={ddpm_best_loss:.4f} (epoch {ddpm_best_epoch}), "
                f"no_improve={ddpm_no_improve}, min_delta={ddpm_es_min_delta}"
            )
            break

    if ddpm_best_state is not None:
        tabddpm.load_state_dict(ddpm_best_state)
        tabddpm.to(config.DEVICE)
    
    # Generate augmented samples (å¢å¼ºå…¨éƒ¨å¹²å‡€æ ·æœ¬)
    logger.info("-"*70)
    logger.info("âœ“ TabDDPM è®­ç»ƒå®Œæˆ")
    if ddpm_best_epoch > 0:
        logger.info(f"  æœ€ä½³TabDDPMæŸå¤±: {ddpm_best_loss:.4f} (epoch {ddpm_best_epoch})")
    logger.info("")
    logger.info("æ­¥éª¤ 2.3: ç”Ÿæˆå¢å¼ºæ ·æœ¬")
    logger.info(f"  è®­ç»ƒé›†æ ·æœ¬æ•°: {len(X_clean)}")
    multipliers = np.ceil(weights_clean.astype(np.float32) * 10.0).astype(int)
    multipliers = np.maximum(multipliers, 2)
    expected_synthetic = int(multipliers.sum())
    logger.info("  å¢å¼ºç­–ç•¥: per-sample å€æ•° = max(2, ceil(sample_weight*10))")
    logger.info(f"  å€æ•°ç»Ÿè®¡: min={multipliers.min()}, max={multipliers.max()}, mean={multipliers.mean():.2f}")
    logger.info(f"  é¢„æœŸç”Ÿæˆ: ~{expected_synthetic} ä¸ªåˆæˆæ ·æœ¬")
    logger.info("  å¼€å§‹ç”Ÿæˆ...")
    
    X_augmented, y_augmented, sample_weights = tabddpm.augment_dataset(
        X_clean, y_clean, action_clean, weights_clean,
        density_clean, cl_conf_clean, knn_conf_clean,
        augmentation_ratio=config.AUGMENTATION_RATIO_MIN
    )
    
    n_train_original = len(X_clean)
    n_synthetic = len(X_augmented) - n_train_original
    
    logger.info(f"âœ“ æ•°æ®å¢å¼ºå®Œæˆ: ä» {n_train_original} ä¸ªè®­ç»ƒæ ·æœ¬å¢å¼ºåˆ° {len(X_augmented)} ä¸ªæ ·æœ¬")
    logger.info(f"  åŸå§‹è®­ç»ƒæ•°æ®: {n_train_original} ä¸ªæ ·æœ¬")
    logger.info(f"  åˆæˆæ•°æ®: {n_synthetic} ä¸ªæ ·æœ¬")
    logger.info("")
    
    # ========================
    # æ­¥éª¤ 2.4: ç”Ÿæˆè´¨é‡è¯Šæ–­
    # ========================
    logger.info("æ­¥éª¤ 2.4: ç”Ÿæˆè´¨é‡è¯Šæ–­ (Generation Quality Assessment)")
    logger.info("-"*70)
    logger.info("è¯„ä¼°ç»´åº¦: 1) Fidelity (çœŸå®æ€§) 2) Protocol Validity (åè®®æœ‰æ•ˆæ€§)")
    logger.info("")
    
    # åˆ†ç¦»åŸå§‹æ•°æ®å’Œåˆæˆæ•°æ®
    X_train_original = X_augmented[:n_train_original]
    X_train_synthetic = X_augmented[n_train_original:]
    y_train_original = y_augmented[:n_train_original]
    y_train_synthetic = y_augmented[n_train_original:]
    
    # 1. Fidelity: ç‰¹å¾åˆ†å¸ƒå¯¹æ¯”ï¼ˆå‡å€¼/æ–¹å·®ï¼‰
    logger.info("1ï¸âƒ£  Fidelity æ£€æŸ¥: ç‰¹å¾åˆ†å¸ƒå¯¹æ¯”")
    feature_names = ['Length', 'Log-IAT', 'Direction', 'BurstSize', 'CumulativeLen', 'ValidMask']
    logger.info(f"{'ç‰¹å¾':<12} | {'çœŸå®å‡å€¼':<12} | {'åˆæˆå‡å€¼':<12} | {'å·®å¼‚%':<10} | {'çœŸå®æ ‡å‡†å·®':<12} | {'åˆæˆæ ‡å‡†å·®':<12}")
    logger.info("-"*85)

    if vm_idx is not None and int(vm_idx) >= 0 and int(vm_idx) < X_train_original.shape[-1]:
        pad_mask_real = X_train_original[:, :, int(vm_idx)] > 0.5
        pad_mask_syn = X_train_synthetic[:, :, int(vm_idx)] > 0.5
    else:
        pad_mask_real = np.any(X_train_original != 0.0, axis=-1)
        pad_mask_syn = np.any(X_train_synthetic != 0.0, axis=-1)

    diff_pcts = []
    for i, name in enumerate(feature_names):
        real_data = X_train_original[:, :, i][pad_mask_real]
        syn_data = X_train_synthetic[:, :, i][pad_mask_syn]
        
        real_mean = real_data.mean() if real_data.size > 0 else np.nan
        syn_mean = syn_data.mean() if syn_data.size > 0 else np.nan
        real_std = real_data.std() if real_data.size > 0 else np.nan
        syn_std = syn_data.std() if syn_data.size > 0 else np.nan
        
        # è®¡ç®—ç›¸å¯¹å·®å¼‚ç™¾åˆ†æ¯”
        if np.isfinite(real_mean) and np.isfinite(syn_mean) and np.isfinite(real_std):
            denom = max(abs(real_mean), float(real_std), 1e-8)
            diff_pct = abs(real_mean - syn_mean) / denom * 100
        else:
            diff_pct = np.nan
        diff_pcts.append(float(diff_pct))
        
        # åˆ¤æ–­è´¨é‡ï¼ˆå·®å¼‚<10%ä¸ºä¼˜ç§€ï¼Œ<20%ä¸ºè‰¯å¥½ï¼Œ>20%ä¸ºéœ€å…³æ³¨ï¼‰
        quality_marker = "âœ“" if diff_pct < 10 else ("âš " if diff_pct < 20 else "âŒ")
        
        logger.info(f"{name:<12} | {real_mean:>11.4f} | {syn_mean:>11.4f} | {diff_pct:>8.2f}% {quality_marker} | "
                   f"{real_std:>11.4f} | {syn_std:>11.4f}")
    
    logger.info("")


    # 1.1 Quantiles: P50/P90/P99 (helps diagnose tail drift, esp. for skewed features like BurstSize)
    logger.info("1ï¸âƒ£ 1ï¸âƒ£  Quantile æ£€æŸ¥: P50/P90/P99 (çœŸå® vs åˆæˆ)")
    logger.info(f"{'ç‰¹å¾':<12} | {'çœŸå®P50':<10} | {'åˆæˆP50':<10} | {'çœŸå®P90':<10} | {'åˆæˆP90':<10} | {'çœŸå®P99':<10} | {'åˆæˆP99':<10}")
    logger.info("-"*92)
    for i, name in enumerate(feature_names):
        real_data = X_train_original[:, :, i][pad_mask_real]
        syn_data = X_train_synthetic[:, :, i][pad_mask_syn]

        if real_data.size > 0:
            real_p50, real_p90, real_p99 = np.percentile(real_data, [50, 90, 99])
        else:
            real_p50, real_p90, real_p99 = np.nan, np.nan, np.nan

        if syn_data.size > 0:
            syn_p50, syn_p90, syn_p99 = np.percentile(syn_data, [50, 90, 99])
        else:
            syn_p50, syn_p90, syn_p99 = np.nan, np.nan, np.nan

        logger.info(
            f"{name:<12} | {real_p50:>9.4f} | {syn_p50:>9.4f} | {real_p90:>9.4f} | {syn_p90:>9.4f} | {real_p99:>9.4f} | {syn_p99:>9.4f}"
        )

    logger.info("")

    max_diff_pct = float(np.nanmax(diff_pcts)) if len(diff_pcts) > 0 else float('nan')
    if np.isnan(max_diff_pct):
        fidelity_level = 'æœªçŸ¥'
    elif max_diff_pct < 10:
        fidelity_level = 'ä¼˜ç§€'
    elif max_diff_pct < 20:
        fidelity_level = 'è‰¯å¥½'
    else:
        fidelity_level = 'éœ€å…³æ³¨'
    
    # 2. Protocol Validity: åè®®çº¦æŸæ£€æŸ¥
    logger.info("2ï¸âƒ£  Protocol Validity æ£€æŸ¥: ç‰©ç†çº¦æŸéªŒè¯")
    logger.info("æ£€æŸ¥é¡¹: è´Ÿæ•°åŒ…é•¿ã€è´Ÿæ•°çªå‘å¤§å°ã€å¼‚å¸¸IATã€å¼‚å¸¸æ–¹å‘ã€å¼‚å¸¸ç´¯ç§¯é•¿åº¦/æ©ç ")
    logger.info("")
    
    # æ£€æŸ¥å„ç§è¿è§„æƒ…å†µ
    invalid_length = (X_train_synthetic[:, :, 0] < 0).sum()
    invalid_iat = (np.isnan(X_train_synthetic[:, :, 1]) | np.isinf(X_train_synthetic[:, :, 1]) | (X_train_synthetic[:, :, 1] < 0)).sum()
    invalid_direction = ((X_train_synthetic[:, :, 2] < -1) | (X_train_synthetic[:, :, 2] > 1)).sum()
    invalid_burst = (X_train_synthetic[:, :, 3] < 0).sum()
    invalid_cum = ((X_train_synthetic[:, :, 4] < 0) | (X_train_synthetic[:, :, 4] > 1)).sum()
    invalid_mask = ((X_train_synthetic[:, :, 5] < 0) | (X_train_synthetic[:, :, 5] > 1)).sum()
    
    total_synthetic_values = X_train_synthetic.size
    invalid_total = invalid_length + invalid_burst + invalid_iat + invalid_direction + invalid_cum + invalid_mask
    validity_rate = (1 - invalid_total / total_synthetic_values) * 100
    
    logger.info(f"  âŒ è´Ÿæ•°åŒ…é•¿ (Length < 0):     {invalid_length:>6} ä¸ªå€¼")
    logger.info(f"  âŒ è´Ÿæ•°çªå‘å¤§å° (BurstSize < 0): {invalid_burst:>6} ä¸ªå€¼")
    logger.info(f"  âŒ å¼‚å¸¸IAT (NaN/Inf):         {invalid_iat:>6} ä¸ªå€¼")
    logger.info(f"  âŒ å¼‚å¸¸æ–¹å‘ (Direction âˆ‰[-1,1]): {invalid_direction:>6} ä¸ªå€¼")
    logger.info(f"  âŒ å¼‚å¸¸ç´¯ç§¯é•¿åº¦ (CumulativeLen âˆ‰[0,1]): {invalid_cum:>6} ä¸ªå€¼")
    logger.info(f"  âŒ å¼‚å¸¸æ©ç  (ValidMask âˆ‰[0,1]): {invalid_mask:>6} ä¸ªå€¼")
    logger.info(f"  âœ“ æ€»æœ‰æ•ˆç‡: {validity_rate:.2f}% ({total_synthetic_values - invalid_total}/{total_synthetic_values})")
    logger.info("")
    
    # 3. Class-wise åˆ†å¸ƒæ£€æŸ¥
    logger.info("3ï¸âƒ£  Class-wise åˆ†å¸ƒæ£€æŸ¥: ç±»åˆ«å¹³è¡¡æ€§")
    logger.info(f"  åŸå§‹æ•°æ® - æ­£å¸¸: {(y_train_original==0).sum()}, æ¶æ„: {(y_train_original==1).sum()}")
    logger.info(f"  åˆæˆæ•°æ® - æ­£å¸¸: {(y_train_synthetic==0).sum()}, æ¶æ„: {(y_train_synthetic==1).sum()}")
    logger.info(f"  å¢å¼ºå   - æ­£å¸¸: {(y_augmented==0).sum()}, æ¶æ„: {(y_augmented==1).sum()}")
    logger.info("")
    
    # 4. ç»“æ„æ„ŸçŸ¥æ£€æŸ¥ï¼šä¾èµ–ç‰¹å¾çš„åæ–¹å·®
    logger.info("4ï¸âƒ£  Structure-Aware æ£€æŸ¥: ä¾èµ–ç‰¹å¾åæ–¹å·®")
    logger.info("æ£€æŸ¥ Length-IAT-BurstSize ä¹‹é—´çš„ç›¸å…³æ€§æ˜¯å¦ä¿æŒ")
    
    # æå–ä¾èµ–ç‰¹å¾ï¼ˆç´¢å¼• 0, 1, 4ï¼‰
    dep_indices = [0, 1, 3]  # Length, Log-IAT, BurstSize
    real_dep = X_train_original[:, :, dep_indices][pad_mask_real].reshape(-1, 3)
    syn_dep = X_train_synthetic[:, :, dep_indices][pad_mask_syn].reshape(-1, 3)
    
    # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
    real_corr = np.corrcoef(real_dep.T) if real_dep.shape[0] > 1 else np.full((3, 3), np.nan)
    syn_corr = np.corrcoef(syn_dep.T) if syn_dep.shape[0] > 1 else np.full((3, 3), np.nan)
    
    logger.info("  çœŸå®æ•°æ®ç›¸å…³ç³»æ•°çŸ©é˜µ:")
    logger.info(f"    Length-IAT:    {real_corr[0, 1]:>7.4f}")
    logger.info(f"    Length-Burst:  {real_corr[0, 2]:>7.4f}")
    logger.info(f"    IAT-Burst:     {real_corr[1, 2]:>7.4f}")
    logger.info("  åˆæˆæ•°æ®ç›¸å…³ç³»æ•°çŸ©é˜µ:")
    logger.info(f"    Length-IAT:    {syn_corr[0, 1]:>7.4f}")
    logger.info(f"    Length-Burst:  {syn_corr[0, 2]:>7.4f}")
    logger.info(f"    IAT-Burst:     {syn_corr[1, 2]:>7.4f}")
    
    # è®¡ç®—ç›¸å…³æ€§å·®å¼‚
    corr_diff = np.nanmean(np.abs(real_corr - syn_corr))
    logger.info(f"  å¹³å‡ç›¸å…³æ€§å·®å¼‚: {corr_diff:.4f} {'âœ“' if corr_diff < 0.1 else 'âš '}")
    logger.info("")
    
    logger.info("-"*70)
    logger.info("âœ“ ç”Ÿæˆè´¨é‡è¯Šæ–­å®Œæˆ")
    logger.info(f"  æ€»ä½“è¯„ä¼°: Fidelity={fidelity_level}, "
               f"Validity={validity_rate:.1f}%, "
               f"Structure={'ä¿æŒ' if corr_diff < 0.1 else 'éƒ¨åˆ†ä¿æŒ'}")
    logger.info("")
    
    # 5. å¯è§†åŒ–: Real vs Synthetic å¯¹æ¯”å›¾ (t-SNE)
    logger.info("5ï¸âƒ£  å¯è§†åŒ–: Real vs Synthetic ç‰¹å¾ç©ºé—´å¯¹æ¯” (t-SNE)")
    from MoudleCode.utils.visualization import plot_real_vs_synthetic_comparison
    
    comparison_save_path = os.path.join(config.DATA_AUGMENTATION_DIR, "figures", "real_vs_synthetic_tsne.png")
    plot_real_vs_synthetic_comparison(
        X_train_original, X_train_synthetic,
        y_train_original, y_train_synthetic,
        comparison_save_path,
        title='TabDDPM Generation Quality: Real vs Synthetic',
        method='tsne'
    )
    logger.info(f"  âœ“ t-SNEå¯¹æ¯”å›¾: {comparison_save_path}")
    logger.info("    (è“è‰²=çœŸå®è‰¯æ€§, çº¢è‰²=çœŸå®æ¶æ„, æµ…è‰²=åˆæˆæ ·æœ¬)")
    logger.info("    ç†æƒ³ç»“æœ: åˆæˆæ ·æœ¬åº”è¦†ç›–åœ¨çœŸå®æ ·æœ¬ä¹‹ä¸Š")
    logger.info("")
    logger.info("ğŸ“ è¾“å‡ºæ–‡ä»¶è·¯å¾„:")
    # Save TabDDPM model
    tabddpm_path = os.path.join(config.DATA_AUGMENTATION_DIR, "models", "tabddpm.pth")
    torch.save(tabddpm.state_dict(), tabddpm_path)
    logger.info(f"  âœ“ TabDDPMæ¨¡å‹: {tabddpm_path}")
    
    # Save augmented data with metadata
    augmented_data_path = os.path.join(config.DATA_AUGMENTATION_DIR, "models", "augmented_data.npz")
    # Create mask to identify original vs synthetic samples
    is_original_mask = np.zeros(len(X_augmented), dtype=bool)
    is_original_mask[:n_train_original] = True
    
    np.savez(augmented_data_path,
             X_augmented=X_augmented,
             y_augmented=y_augmented,
             is_original=is_original_mask,
             n_original=n_train_original,
             sample_weights=sample_weights)
    logger.info(f"  âœ“ å¢å¼ºæ•°æ®: {augmented_data_path}")
    logger.info(f"    (åŒ…å«åŸå§‹/åˆæˆæ•°æ®æ ‡è®°)")
    
    logger.info(f"Augmented dataset: {len(X_augmented)} samples (training)")
    logger.info("Stage 2 complete: Labels corrected and data augmented")
    
    return X_augmented, y_augmented, sample_weights, correction_stats, tabddpm, n_train_original


def stage3_finetune_classifier(backbone, X_train, y_train, sample_weights, config, logger, n_original=None, backbone_path=None):
    """
    Stage 3: Fine-tune dual-stream classifier
    
    Args:
        backbone: Frozen pre-trained backbone
        X_train: (N, L, D) training sequences (augmented, includes original + synthetic)
        y_train: (N,) training labels
        sample_weights: (N,) lifecycle weights from Stage 2 (original + synthetic)
        config: configuration object
        logger: logger
        n_original: int, number of original samples (first n_original in X_train)
                    If None, assumes all samples are original
        backbone_path: str, path to the backbone model used (for metadata recording)
        
    Returns:
        classifier: trained MEDAL classifier
        history: training history
        optimal_threshold: optimal decision threshold
    """
    logger.info("")
    logger.info("="*70)
    logger.info("STAGE 3: Fine-tuning Dual-Stream Classifier")
    logger.info("="*70)
    logger.info(f"ç›®æ ‡: è®­ç»ƒåŒæµMLPåˆ†ç±»å™¨è¿›è¡Œæœ€ç»ˆå¨èƒæ£€æµ‹")
    logger.info(f"è®­ç»ƒè½®æ•°: {config.FINETUNE_EPOCHS} epochs")
    logger.info(f"æ‰¹æ¬¡å¤§å°: {config.FINETUNE_BATCH_SIZE}")
    logger.info(f"å­¦ä¹ ç‡: {config.FINETUNE_LR}")
    logger.info(f"è®­ç»ƒæ ·æœ¬æ•°: {len(X_train)} (åŒ…å«å¢å¼ºæ ·æœ¬)")
    logger.info(f"æŸå¤±ç»„ä»¶: åŠ æƒç›‘ç£ + è½¯æ­£äº¤çº¦æŸ + ä¸€è‡´æ€§æŸå¤±")
    logger.info("")
    logger.info("ğŸ“¥ è¾“å…¥æ•°æ®è·¯å¾„:")
    augmented_data_path = os.path.join(config.DATA_AUGMENTATION_DIR, "models", "augmented_data.npz")
    logger.info(f"  âœ“ å¢å¼ºæ•°æ®: {augmented_data_path}")

    finetune_backbone = bool(getattr(config, 'FINETUNE_BACKBONE', False))
    finetune_scope = str(getattr(config, 'FINETUNE_BACKBONE_SCOPE', 'projection')).lower()
    finetune_backbone_lr = float(getattr(config, 'FINETUNE_BACKBONE_LR', 1e-5))
    
    # ä½¿ç”¨ä¼ å…¥çš„backbone_pathå‚æ•°ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
    if backbone_path is None:
        backbone_path_display = os.path.join(config.FEATURE_EXTRACTION_DIR, "models", "backbone_pretrained.pth")
    else:
        backbone_path_display = backbone_path
    logger.info(f"  âœ“ éª¨å¹²ç½‘ç»œæ¨¡å‹: {backbone_path_display}")
    logger.info("")
    
    # Create classifier
    classifier = MEDAL_Classifier(backbone, config).to(config.DEVICE)
    logger.info("âœ“ åŒæµåˆ†ç±»å™¨åˆ›å»ºå®Œæˆ (MLP_A + MLP_B)")
    
    # Loss function
    criterion = DualStreamLoss(config)
    logger.info("âœ“ æŸå¤±å‡½æ•°åˆå§‹åŒ–å®Œæˆ")

    logger.info(
        f"ğŸ”§ LogitMargin: enabled={bool(getattr(config, 'USE_LOGIT_MARGIN', False))} "
        f"m={float(getattr(config, 'LOGIT_MARGIN_M', 0.0))} "
        f"warmup_epochs={int(getattr(config, 'LOGIT_MARGIN_WARMUP_EPOCHS', 0))}"
    )
    logger.info(
        f"ğŸ”§ MarginLoss: enabled={bool(getattr(config, 'USE_MARGIN_LOSS', False))} "
        f"m={float(getattr(config, 'MARGIN_M', 0.0))} "
        f"s={float(getattr(config, 'MARGIN_S', 1.0))} "
        f"w={float(getattr(config, 'MARGIN_LOSS_WEIGHT', 0.0))}"
    )

    # Stage 3 online augmentation (input-level)
    use_stage3_online_aug = bool(getattr(config, 'STAGE3_ONLINE_AUGMENTATION', False))
    augmentor = None
    if use_stage3_online_aug:
        from MoudleCode.feature_extraction.traffic_augmentation import TrafficAugmentation
        augmentor = TrafficAugmentation(config)
        logger.info("âœ“ Stage 3 åœ¨çº¿è¡¨å¾å¢å¼ºå·²å¯ç”¨ (TrafficAugmentation)")
    else:
        logger.info("âœ“ Stage 3 åœ¨çº¿è¡¨å¾å¢å¼ºæœªå¯ç”¨")
    
    # Stage 3 ST-Mixup (Spatio-Temporal Mixup)
    use_st_mixup = bool(getattr(config, 'STAGE3_USE_ST_MIXUP', False))
    st_mixup = None
    if use_st_mixup:
        from MoudleCode.feature_extraction.st_mixup import create_st_mixup
        st_mixup_mode = str(getattr(config, 'STAGE3_ST_MIXUP_MODE', 'intra_class'))
        st_mixup = create_st_mixup(config, mode=st_mixup_mode)
        logger.info(f"âœ“ Stage 3 ST-Mixupå·²å¯ç”¨ (mode={st_mixup_mode})")
        logger.info(f"  - Alpha: {config.STAGE3_ST_MIXUP_ALPHA} (æ··åˆå¼ºåº¦)")
        logger.info(f"  - Warmup: {config.STAGE3_ST_MIXUP_WARMUP_EPOCHS} epochs (æ¸è¿›å¼å¯ç”¨)")
        logger.info(f"  - Max Prob: {config.STAGE3_ST_MIXUP_MAX_PROB} (æœ€å¤§æ··åˆæ¦‚ç‡)")
        logger.info(f"  - Time Shift: {config.STAGE3_ST_MIXUP_TIME_SHIFT_RATIO} (æ—¶é—´åç§»æ¯”ä¾‹)")
        if st_mixup_mode == 'intra_class':
            logger.info(f"  - ç­–ç•¥: ç±»å†…æ··åˆï¼ˆåªæ··åˆåŒç±»æ ·æœ¬ï¼Œé¿å…è¯­ä¹‰å†²çªï¼‰")
        elif st_mixup_mode == 'selective':
            logger.info(f"  - ç­–ç•¥: é€‰æ‹©æ€§æ··åˆï¼ˆåªæ··åˆå›°éš¾æ ·æœ¬ï¼‰")
            logger.info(f"  - Uncertainty Threshold: {config.STAGE3_ST_MIXUP_UNCERTAINTY_THRESHOLD}")
    else:
        logger.info("âœ“ Stage 3 ST-Mixupæœªå¯ç”¨")
    
    # -------------------- Backbone finetuning policy --------------------
    if finetune_backbone:
        # Default: freeze everything first
        backbone.freeze()
        if finetune_scope == 'all':
            backbone.unfreeze()
            logger.info("ğŸ”¥ Stage 3: Backbone finetuning enabled (scope=all)")
        elif finetune_scope == 'projection':
            # Only train the bidirectional projection head
            if hasattr(backbone, 'projection'):
                for p in backbone.projection.parameters():
                    p.requires_grad = True
                logger.info("ğŸ”¥ Stage 3: Backbone finetuning enabled (scope=projection)")
            else:
                logger.warning("âš ï¸  FINETUNE_BACKBONE_SCOPE='projection' but backbone has no attribute 'projection'. Falling back to frozen backbone.")
                finetune_backbone = False
        else:
            logger.warning(f"âš ï¸  Unknown FINETUNE_BACKBONE_SCOPE='{finetune_scope}'. Falling back to frozen backbone.")
            finetune_backbone = False
    else:
        backbone.freeze()

    # Set backbone mode
    if finetune_backbone:
        backbone.train()
    else:
        backbone.eval()

    # -------------------- Optimizer (classifier + optional backbone) --------------------
    param_groups = [
        {'params': classifier.dual_mlp.parameters(), 'lr': float(config.FINETUNE_LR)}
    ]
    if finetune_backbone:
        backbone_params = [p for p in backbone.parameters() if p.requires_grad]
        if len(backbone_params) == 0:
            logger.warning("âš ï¸  FINETUNE_BACKBONE=True but no trainable backbone params found; backbone will remain frozen.")
            finetune_backbone = False
            backbone.eval()
        else:
            param_groups.append({'params': backbone_params, 'lr': finetune_backbone_lr})
            logger.info(f"âœ“ ä¼˜åŒ–å™¨åŒ…å«éª¨å¹²ç½‘ç»œå¯è®­ç»ƒå‚æ•°: {len(backbone_params)} | backbone_lr={finetune_backbone_lr}")

    optimizer = optim.AdamW(
        param_groups,
        weight_decay=config.PRETRAIN_WEIGHT_DECAY
    )
    if finetune_backbone:
        logger.info("âœ“ ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ (åˆ†ç±»å™¨ + éª¨å¹²ç½‘ç»œå¾®è°ƒ)")
    else:
        logger.info("âœ“ ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ (ä»…ä¼˜åŒ–åˆ†ç±»å™¨å‚æ•°ï¼Œéª¨å¹²ç½‘ç»œå·²å†»ç»“)")
    
    logger.info("")
    logger.info("ğŸ’¡ è®­ç»ƒæµç¨‹è¯´æ˜:")
    if finetune_backbone:
        logger.info("  æ¯ä¸ªbatch: åŸå§‹æ•°æ® â†’ éª¨å¹²ç½‘ç»œ(å¯è®­ç»ƒ) â†’ ç‰¹å¾ â†’ åˆ†ç±»å™¨(è®­ç»ƒ) â†’ æŸå¤±")
        logger.info("  éª¨å¹²ç½‘ç»œå‚æ•°ä¼šéšåˆ†ç±»å™¨ä¸€èµ·æ›´æ–°")
    else:
        logger.info("  æ¯ä¸ªbatch: åŸå§‹æ•°æ® â†’ éª¨å¹²ç½‘ç»œ(å†»ç»“) â†’ ç‰¹å¾ â†’ åˆ†ç±»å™¨(è®­ç»ƒ) â†’ æŸå¤±")
        logger.info("  éª¨å¹²ç½‘ç»œå®æ—¶æå–ç‰¹å¾ï¼Œä½†å‚æ•°ä¸æ›´æ–°")
    logger.info("")
    logger.info("="*70)
    val_split = float(getattr(config, 'FINETUNE_VAL_SPLIT', 0.2))
    val_per_class = int(getattr(config, 'FINETUNE_VAL_PER_CLASS', 0))
    if val_per_class > 0:
        logger.info(f"ğŸ“Š è®­ç»ƒ/éªŒè¯åˆ’åˆ† (val_per_class={val_per_class} per class)")
    elif val_split > 0:
        logger.info(f"ğŸ“Š è®­ç»ƒ/éªŒè¯åˆ’åˆ† (val_split={val_split:.2f})")
    else:
        logger.info("ğŸ“Š å…¨é‡è®­ç»ƒï¼ˆä¸åˆ’åˆ†éªŒè¯é›†ï¼‰")
    logger.info("="*70)
    logger.info(f"  æ ·æœ¬æ€»æ•°: {len(X_train)} ä¸ªæ ·æœ¬ (åŒ…å«åŸå§‹ + åˆæˆ)")
    logger.info("")

    X_val_split = None
    y_val_split = None
    sample_weights_val_split = None

    if val_per_class > 0:
        rng = np.random.RandomState(int(getattr(config, 'SEED', 42)))
        y_np = np.asarray(y_train).astype(int)
        all_idx = np.arange(len(y_np))

        idx0 = all_idx[y_np == 0]
        idx1 = all_idx[y_np == 1]
        n0 = int(min(val_per_class, len(idx0)))
        n1 = int(min(val_per_class, len(idx1)))
        if n0 == 0 or n1 == 0:
            logger.warning("âš ï¸  FINETUNE_VAL_PER_CLASS is set but one class has 0 samples; falling back to no validation split.")
            X_train_split = X_train
            y_train_split = y_train
            sample_weights_split = sample_weights
        else:
            val_idx0 = rng.choice(idx0, size=n0, replace=False)
            val_idx1 = rng.choice(idx1, size=n1, replace=False)
            val_idx = np.concatenate([val_idx0, val_idx1])
            train_idx = np.setdiff1d(all_idx, val_idx, assume_unique=False)

            X_train_split = X_train[train_idx]
            y_train_split = y_train[train_idx]
            sample_weights_split = sample_weights[train_idx]
            X_val_split = X_train[val_idx]
            y_val_split = y_train[val_idx]
            sample_weights_val_split = sample_weights[val_idx]

            logger.info(f"  è®­ç»ƒé›†: {len(X_train_split)}")
            logger.info(f"  éªŒè¯é›†: {len(X_val_split)} (per_class={val_per_class})")
            logger.info("")
    elif val_split > 0:
        from sklearn.model_selection import train_test_split
        X_train_split, X_val_split, y_train_split, y_val_split, sample_weights_split, sample_weights_val_split = train_test_split(
            X_train,
            y_train,
            sample_weights,
            test_size=val_split,
            random_state=int(getattr(config, 'SEED', 42)),
            stratify=y_train
        )
        logger.info(f"  è®­ç»ƒé›†: {len(X_train_split)}")
        logger.info(f"  éªŒè¯é›†: {len(X_val_split)}")
        logger.info("")
    else:
        X_train_split = X_train
        y_train_split = y_train
        sample_weights_split = sample_weights
    
    # ==================== æ¸©å®¤è®­ç»ƒç­–ç•¥ï¼šå¼ºåˆ¶1:1å¹³è¡¡é‡‡æ · ====================
    use_balanced_sampling = getattr(config, 'USE_BALANCED_SAMPLING', True)
    use_hard_mining = bool(getattr(config, 'STAGE3_HARD_MINING', False))
    hard_mining_warmup_epochs = int(getattr(config, 'STAGE3_HARD_MINING_WARMUP_EPOCHS', 5))
    hard_mining_freq_epochs = int(getattr(config, 'STAGE3_HARD_MINING_FREQ_EPOCHS', 3))
    hard_mining_topk_ratio = float(getattr(config, 'STAGE3_HARD_MINING_TOPK_RATIO', 0.2))
    hard_mining_multiplier = float(getattr(config, 'STAGE3_HARD_MINING_MULTIPLIER', 3.0))
    hard_mining_pos_prob_max = float(getattr(config, 'STAGE3_HARD_MINING_POS_PROB_MAX', 0.70))
    hard_mining_neg_prob_min = float(getattr(config, 'STAGE3_HARD_MINING_NEG_PROB_MIN', 0.60))
    
    # Dataset is created once; sampler may be rebuilt during training when hard mining is enabled.
    train_dataset = TensorDataset(
        torch.FloatTensor(X_train_split),
        torch.LongTensor(y_train_split),
        torch.FloatTensor(sample_weights_split)
    )

    def _build_train_loader_with_sampler(_weights_np: np.ndarray):
        from torch.utils.data import WeightedRandomSampler
        sampler = WeightedRandomSampler(
            weights=torch.as_tensor(_weights_np, dtype=torch.double),
            num_samples=len(_weights_np),
            replacement=True
        )
        return DataLoader(
            train_dataset,
            batch_size=config.FINETUNE_BATCH_SIZE,
            sampler=sampler
        )

    def _compute_sampling_weights_from_model():
        classifier.eval()
        if finetune_backbone:
            backbone.eval()
        probs_list = []
        labels_list = []
        with torch.no_grad():
            eval_loader = DataLoader(train_dataset, batch_size=getattr(config, 'TEST_BATCH_SIZE', 256), shuffle=False)
            for X_b, y_b, _w_b in eval_loader:
                X_b = X_b.to(config.DEVICE)
                z_b = backbone(X_b, return_sequence=False)
                logits_a, logits_b = classifier.dual_mlp(z_b, return_separate=True)
                logits_avg = (logits_a + logits_b) / 2.0
                p = torch.softmax(logits_avg, dim=1)[:, 1]
                probs_list.append(p.detach().cpu().numpy())
                labels_list.append(y_b.detach().cpu().numpy())

        classifier.train()
        if finetune_backbone:
            backbone.train()

        probs = np.concatenate(probs_list, axis=0)
        labels = np.concatenate(labels_list, axis=0).astype(int)

        if use_balanced_sampling:
            class_counts = np.bincount(labels)
            class_weights = 1.0 / np.maximum(class_counts, 1)
            base_w = class_weights[labels]
        else:
            base_w = np.ones_like(probs, dtype=np.float64)

        hard_pos = (labels == 1) & (probs < hard_mining_pos_prob_max)
        hard_neg = (labels == 0) & (probs > hard_mining_neg_prob_min)
        hard_mask = hard_pos | hard_neg

        k_ratio = float(np.clip(hard_mining_topk_ratio, 0.0, 1.0))
        k = int(max(0, round(len(probs) * k_ratio)))
        if k > 0:
            difficulty = np.where(labels == 1, 1.0 - probs, probs)
            topk_idx = np.argpartition(-difficulty, k - 1)[:k]
            hard_mask[topk_idx] = True

        weights = base_w.astype(np.float64)
        weights[hard_mask] = weights[hard_mask] * hard_mining_multiplier
        return weights, {
            'n_hard_total': int(np.sum(hard_mask)),
            'n_hard_pos': int(np.sum(hard_pos)),
            'n_hard_neg': int(np.sum(hard_neg)),
            'pos_prob_max': float(hard_mining_pos_prob_max),
            'neg_prob_min': float(hard_mining_neg_prob_min),
            'topk_ratio': float(hard_mining_topk_ratio),
            'multiplier': float(hard_mining_multiplier),
        }

    if use_balanced_sampling:
        logger.info("ğŸŒ¡ï¸  æ¸©å®¤è®­ç»ƒç­–ç•¥ï¼šå¯ç”¨åŠ æƒå¹³è¡¡é‡‡æ ·")
        logger.info("  ç›®æ ‡ï¼šæ­£å¸¸:æ¶æ„ = 1.5:1 (æ¶æ„æ ·æœ¬æƒé‡æ›´é«˜)")
        logger.info("")
        
        class_counts = np.bincount(y_train_split)
        logger.info(f"  åŸå§‹åˆ†å¸ƒ: æ­£å¸¸={class_counts[0]}, æ¶æ„={class_counts[1]}")
        logger.info(f"  åŸå§‹æ¯”ä¾‹: {class_counts[0]}:{class_counts[1]} â‰ˆ {class_counts[0]/class_counts[1]:.1f}:1")
        
        # è®¾ç½®é‡‡æ ·æƒé‡ä»¥è¾¾åˆ° æ­£å¸¸:æ¶æ„ = 1.5:1
        # å¦‚æœæ­£å¸¸ç±»æœ‰Nä¸ªæ ·æœ¬ï¼Œæ¶æ„ç±»æœ‰Mä¸ªæ ·æœ¬
        # æˆ‘ä»¬å¸Œæœ›é‡‡æ ·åçš„æœŸæœ›æ¯”ä¾‹æ˜¯ 1.5:1
        # è®¾æ­£å¸¸ç±»æƒé‡ä¸ºw0ï¼Œæ¶æ„ç±»æƒé‡ä¸ºw1
        # åˆ™ (N*w0) : (M*w1) = 1.5 : 1
        # å³ï¼šw0 = 1.5 * M / N * w1
        # è®¾w1 = 1ï¼Œåˆ™ w0 = 1.5 * M / N
        
        n_benign = class_counts[0]
        n_malicious = class_counts[1]
        
        # è®¡ç®—ç±»åˆ«æƒé‡ï¼šæ­£å¸¸ç±»æƒé‡ = 1.5 * (æ¶æ„æ•°é‡ / æ­£å¸¸æ•°é‡)
        weight_benign = 1.5 * n_malicious / n_benign
        weight_malicious = 1.0
        
        class_weights = np.array([weight_benign, weight_malicious])
        sample_sampling_weights = class_weights[y_train_split]
        
        logger.info(f"  é‡‡æ ·æƒé‡: æ­£å¸¸ç±»={weight_benign:.4f}, æ¶æ„ç±»={weight_malicious:.4f}")
        logger.info(f"  æœŸæœ›é‡‡æ ·æ¯”ä¾‹: 1.5:1 (æ­£å¸¸:æ¶æ„)")

        train_loader = _build_train_loader_with_sampler(sample_sampling_weights)
        logger.info(f"  âœ“ å¹³è¡¡é‡‡æ ·å™¨åˆ›å»ºå®Œæˆ")
        logger.info(f"  âœ“ æ¯ä¸ªbatchæœŸæœ›æ¯”ä¾‹: 1.5:1 (æ­£å¸¸:æ¶æ„)")
    else:
        logger.info("âš ï¸  ä½¿ç”¨åŸå§‹åˆ†å¸ƒè®­ç»ƒï¼ˆæœªå¯ç”¨å¹³è¡¡é‡‡æ ·ï¼‰")
        if use_hard_mining:
            train_loader = _build_train_loader_with_sampler(np.ones(len(y_train_split), dtype=np.float64))
        else:
            train_loader = DataLoader(train_dataset, batch_size=config.FINETUNE_BATCH_SIZE, shuffle=True)
    
    logger.info(f"âœ“ æ•°æ®åŠ è½½å™¨å‡†å¤‡å®Œæˆ ({len(train_loader)} ä¸ªæ‰¹æ¬¡)")
    logger.info("")
    logger.info("å¼€å§‹è®­ç»ƒ...")
    logger.info("-"*70)
    
    # Training loop
    history = {
        'train_loss': [], 'supervision': [], 'soft_f1_loss': [], 'orthogonality': [],
        'consistency': [], 'margin': [], 'lambda_orth': [],
        'lambda_con': [], 'lambda_margin': [], 'train_f1': [], 'val_f1': [], 'val_threshold': [],
        'train_f1_star': [], 'train_threshold_star': []
    }
    
    classifier.train()

    best_f1 = -1.0
    best_epoch = -1
    best_state = None
    best_threshold = float(getattr(config, 'MALICIOUS_THRESHOLD', 0.5))
    finetuned_backbone_path = None
    
    # Early stopping variables
    use_early_stopping = bool(getattr(config, 'FINETUNE_EARLY_STOPPING', False))
    es_warmup_epochs = int(getattr(config, 'FINETUNE_ES_WARMUP_EPOCHS', 30))
    es_patience = int(getattr(config, 'FINETUNE_ES_PATIENCE', 25))
    es_min_delta = float(getattr(config, 'FINETUNE_ES_MIN_DELTA', 0.002))
    no_improve_count = 0
    best_es_metric = -1.0
    allow_train_metric_es = bool(getattr(config, 'FINETUNE_ES_ALLOW_TRAIN_METRIC', False))
    if use_early_stopping and X_val_split is None and not allow_train_metric_es:
        use_early_stopping = False
        logger.info("âš ï¸  æ—©åœå·²è‡ªåŠ¨ç¦ç”¨ï¼šæœªè®¾ç½®éªŒè¯é›† (FINETUNE_VAL_SPLIT=0)ï¼Œè®­ç»ƒé›†æŒ‡æ ‡ä¸é€‚åˆä½œä¸ºæ—©åœä¾æ®")
        logger.info("")
    
    if use_early_stopping:
        logger.info(f"âœ“ æ—©åœæœºåˆ¶å·²å¯ç”¨:")
        logger.info(f"  - é¢„çƒ­è½®æ•°: {es_warmup_epochs} (å‰{es_warmup_epochs}è½®ä¸è§¦å‘æ—©åœ)")
        logger.info(f"  - è€å¿ƒå€¼: {es_patience} (è¿ç»­{es_patience}è½®ä¸æ”¹å–„åˆ™åœæ­¢)")
        logger.info(f"  - æ”¹å–„é˜ˆå€¼: {es_min_delta:.4f} (F1æ”¹å–„éœ€è¶…è¿‡{es_min_delta*100:.2f}%)")
        logger.info("")
    
    for epoch in range(config.FINETUNE_EPOCHS):
        if use_hard_mining and (epoch + 1) >= hard_mining_warmup_epochs:
            if hard_mining_freq_epochs <= 0:
                hard_mining_freq_epochs = 1
            if ((epoch + 1 - hard_mining_warmup_epochs) % hard_mining_freq_epochs) == 0:
                try:
                    new_weights, hm_stats = _compute_sampling_weights_from_model()
                    train_loader = _build_train_loader_with_sampler(new_weights)
                    logger.info(
                        f"ğŸ§² Stage 3 Hard Mining: updated sampler | "
                        f"hard_total={hm_stats['n_hard_total']} "
                        f"(pos={hm_stats['n_hard_pos']}, neg={hm_stats['n_hard_neg']}) | "
                        f"topk_ratio={hm_stats['topk_ratio']:.2f} "
                        f"mult={hm_stats['multiplier']:.2f} "
                        f"pos_prob_max={hm_stats['pos_prob_max']:.2f} "
                        f"neg_prob_min={hm_stats['neg_prob_min']:.2f}"
                    )
                except Exception as e:
                    logger.warning(f"âš ï¸  Stage 3 Hard Mining skipped due to error: {e}")

        epoch_loss = 0.0
        epoch_losses = {
            'supervision': 0.0, 'soft_f1': 0.0, 'orthogonality': 0.0, 
            'consistency': 0.0, 'margin': 0.0,
            'logit_margin_scale': 0.0
        }
        
        # æ”¶é›†è®­ç»ƒé›†é¢„æµ‹ç”¨äºè®¡ç®— F1
        all_train_probs = []
        all_train_labels = []
        
        for X_batch, y_batch, w_batch in train_loader:
            X_batch = X_batch.to(config.DEVICE)
            y_batch = y_batch.to(config.DEVICE)
            w_batch = w_batch.to(config.DEVICE)
            
            optimizer.zero_grad()
            
            # Online augmentation (training only)
            # Step 1: TrafficAugmentation (ç‰©ç†æ„ŸçŸ¥çš„å•æ ·æœ¬å¢å¼º)
            if augmentor is not None:
                X_batch, _ = augmentor(X_batch)
            
            # Step 2: ST-Mixup (ç±»å†…æ—¶ç©ºæ··åˆï¼Œæ¸è¿›å¼å¯ç”¨)
            if st_mixup is not None:
                if st_mixup_mode == 'selective':
                    # é€‰æ‹©æ€§æ¨¡å¼ï¼šéœ€è¦ä¼ å…¥æ¨¡å‹
                    X_batch, y_batch = st_mixup(
                        X_batch, y_batch, 
                        epoch=epoch, 
                        total_epochs=config.FINETUNE_EPOCHS,
                        classifier=classifier,
                        backbone=backbone
                    )
                else:
                    # ç±»å†…æ¨¡å¼ï¼šä¸éœ€è¦æ¨¡å‹
                    X_batch, y_batch = st_mixup(
                        X_batch, y_batch,
                        epoch=epoch,
                        total_epochs=config.FINETUNE_EPOCHS
                    )

            # Extract features
            if finetune_backbone:
                z = backbone(X_batch, return_sequence=False)
            else:
                with torch.no_grad():
                    z = backbone(X_batch, return_sequence=False)
            
            # Compute loss (weight-aware)
            loss, loss_dict = criterion(classifier.dual_mlp, z, y_batch, w_batch, epoch, config.FINETUNE_EPOCHS)
            
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss_dict['total']
            epoch_losses['supervision'] += loss_dict['supervision']
            epoch_losses['soft_f1'] += loss_dict.get('soft_f1', 0.0)
            epoch_losses['orthogonality'] += loss_dict['orthogonality']
            epoch_losses['consistency'] += loss_dict['consistency']
            if 'margin' in loss_dict:
                epoch_losses['margin'] = epoch_losses.get('margin', 0.0) + loss_dict['margin']
            if 'logit_margin_scale' in loss_dict:
                epoch_losses['logit_margin_scale'] = epoch_losses.get('logit_margin_scale', 0.0) + float(loss_dict['logit_margin_scale'])
            
            # æ”¶é›†é¢„æµ‹æ¦‚ç‡ç”¨äºè®¡ç®— F1
            with torch.no_grad():
                logits_a, logits_b = classifier.dual_mlp(z, return_separate=True)
                logits_avg = (logits_a + logits_b) / 2.0
                probs = torch.softmax(logits_avg, dim=1)
                all_train_probs.append(probs.cpu().numpy())
                all_train_labels.append(y_batch.cpu().numpy())
        
        n_batches = len(train_loader)
        epoch_loss /= n_batches
        for key in epoch_losses:
            epoch_losses[key] /= n_batches
        
        # è®¡ç®—è®­ç»ƒé›† Binary F1-Score (å›ºå®šé˜ˆå€¼0.5ï¼Œä»…ç”¨äºå‚è€ƒ)
        train_probs = np.concatenate(all_train_probs)
        train_labels = np.concatenate(all_train_labels)
        train_preds = (train_probs[:, 1] >= 0.5).astype(int)
        from sklearn.metrics import f1_score
        train_f1 = f1_score(train_labels, train_preds, pos_label=1, zero_division=0)

        # è®­ç»ƒé›†å•ç±»F1*(pos=1) & æœ€ä¼˜é˜ˆå€¼ï¼ˆç”¨äºæ— éªŒè¯é›†æ—¶é€‰æ‹© best checkpointï¼‰
        train_f1_star = None
        train_threshold_star = None
        if X_val_split is None:
            from MoudleCode.utils.helpers import find_optimal_threshold
            train_threshold_star, _train_metric, _ = find_optimal_threshold(train_labels, train_probs, metric='f1_binary', positive_class=1)
            train_preds_star = (train_probs[:, 1] >= float(train_threshold_star)).astype(int)
            train_f1_star = f1_score(train_labels, train_preds_star, pos_label=1, zero_division=0)

        val_f1 = None
        val_threshold = None

        if X_val_split is not None:
            classifier.eval()
            all_val_probs = []
            all_val_labels = []
            with torch.no_grad():
                val_dataset = TensorDataset(
                    torch.FloatTensor(X_val_split),
                    torch.LongTensor(y_val_split),
                    torch.FloatTensor(sample_weights_val_split)
                )
                val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
                for X_batch, y_batch, _w_batch in val_loader:
                    X_batch = X_batch.to(config.DEVICE)
                    y_batch = y_batch.to(config.DEVICE)
                    z = backbone(X_batch, return_sequence=False)
                    logits_a, logits_b = classifier.dual_mlp(z, return_separate=True)
                    logits_avg = (logits_a + logits_b) / 2.0
                    probs = torch.softmax(logits_avg, dim=1)
                    all_val_probs.append(probs.cpu().numpy())
                    all_val_labels.append(y_batch.cpu().numpy())

            val_probs = np.concatenate(all_val_probs)
            val_labels = np.concatenate(all_val_labels)

            from MoudleCode.utils.helpers import find_optimal_threshold
            val_threshold, val_metric, _ = find_optimal_threshold(val_labels, val_probs, metric='f1_binary', positive_class=1)
            val_preds = (val_probs[:, 1] >= float(val_threshold)).astype(int)
            val_f1 = f1_score(val_labels, val_preds, pos_label=1, zero_division=0)

            if val_f1 > best_f1:
                best_f1 = float(val_f1)
                best_epoch = int(epoch + 1)
                best_threshold = float(val_threshold)
                best_state = {k: v.detach().cpu().clone() for k, v in classifier.state_dict().items()}
        else:
            if train_f1_star is None:
                raise RuntimeError("Expected train_f1_star to be computed when val_split is disabled")
            if train_f1_star > best_f1:
                best_f1 = float(train_f1_star)
                best_epoch = int(epoch + 1)
                best_threshold = float(train_threshold_star)
                best_state = {k: v.detach().cpu().clone() for k, v in classifier.state_dict().items()}
        
        classifier.eval()
        classifier.train()
        
        # ä¿å­˜å†å²
        history['train_loss'].append(epoch_loss)
        history['supervision'].append(epoch_losses['supervision'])
        history['soft_f1_loss'].append(epoch_losses['soft_f1'])
        history['orthogonality'].append(epoch_losses['orthogonality'])
        history['consistency'].append(epoch_losses['consistency'])
        history['margin'].append(epoch_losses.get('margin', 0.0))
        history['lambda_orth'].append(loss_dict['lambda_orth'])
        history['lambda_con'].append(loss_dict['lambda_con'])
        history['lambda_margin'].append(loss_dict.get('lambda_margin', 0.0))
        history['train_f1'].append(train_f1)
        history['val_f1'].append(val_f1 if val_f1 is not None else np.nan)
        history['val_threshold'].append(val_threshold if val_threshold is not None else np.nan)
        history['train_f1_star'].append(train_f1_star if train_f1_star is not None else np.nan)
        history['train_threshold_star'].append(train_threshold_star if train_threshold_star is not None else np.nan)
        
        # Early stopping check
        if use_early_stopping and (epoch + 1) >= es_warmup_epochs:
            current_metric = val_f1 if val_f1 is not None else train_f1_star
            if current_metric is not None:
                current_metric = float(current_metric)
                if current_metric > (best_es_metric + es_min_delta):
                    best_es_metric = current_metric
                    no_improve_count = 0
                else:
                    no_improve_count += 1
                    
                    if no_improve_count >= es_patience:
                        logger.info("")
                        logger.info("="*70)
                        logger.info(f"ğŸ›‘ æ—©åœè§¦å‘ (Early Stopping Triggered)")
                        logger.info("="*70)
                        logger.info(f"  å½“å‰è½®æ¬¡: Epoch {epoch+1}/{config.FINETUNE_EPOCHS}")
                        logger.info(f"  æœ€ä½³F1: {best_f1:.4f} (Epoch {best_epoch})")
                        logger.info(f"  å½“å‰F1: {current_metric:.4f}")
                        logger.info(f"  è¿ç»­{no_improve_count}è½®æœªæ”¹å–„è¶…è¿‡{es_min_delta:.4f}")
                        logger.info(f"  æå‰ç»ˆæ­¢è®­ç»ƒï¼ŒèŠ‚çœ {config.FINETUNE_EPOCHS - epoch - 1} è½®")
                        logger.info("="*70)
                        logger.info("")
                        break
        
        # æ¯ä¸ªepochéƒ½è¾“å‡ºè¯¦ç»†æ—¥å¿—
        progress = (epoch + 1) / config.FINETUNE_EPOCHS * 100
        soft_f1_str = f" | F1Loss: {epoch_losses['soft_f1']:.4f}" if epoch_losses['soft_f1'] > 0 else ""
        logit_margin_scale = float(epoch_losses.get('logit_margin_scale', 0.0))
        logit_margin_str = f" | LogitM: {logit_margin_scale:.2f}" if logit_margin_scale > 0 else ""
        if val_f1 is not None:
            logger.info(f"[Stage 3] Epoch [{epoch+1}/{config.FINETUNE_EPOCHS}] ({progress:.1f}%) | "
                      f"TrLoss: {epoch_loss:.4f} | "
                      f"TrF1@0.5: {train_f1:.4f} | "
                      f"ValF1*: {val_f1:.4f} | "
                      f"ValTh*: {float(val_threshold):.4f} | "
                      f"Sup: {epoch_losses['supervision']:.4f}{soft_f1_str}{logit_margin_str}")
        else:
            train_f1_star_disp = float(train_f1_star) if train_f1_star is not None else float('nan')
            train_th_star_disp = float(train_threshold_star) if train_threshold_star is not None else float('nan')
            logger.info(f"[Stage 3] Epoch [{epoch+1}/{config.FINETUNE_EPOCHS}] ({progress:.1f}%) | "
                      f"TrLoss: {epoch_loss:.4f} | "
                      f"TrF1@0.5: {train_f1:.4f} | "
                      f"TrF1*: {train_f1_star_disp:.4f} | "
                      f"TrTh*: {train_th_star_disp:.4f} | "
                      f"Sup: {epoch_losses['supervision']:.4f}{soft_f1_str}{logit_margin_str}")
    
    logger.info("-"*70)
    logger.info("âœ“ Stage 3 å®Œæˆ: åˆ†ç±»å™¨å¾®è°ƒå®Œæˆ")
    actual_epochs = len(history['train_loss'])
    logger.info(f"  è®­ç»ƒé›† - æœ€ç»ˆæŸå¤±: {history['train_loss'][-1]:.4f}, æœ€ç»ˆF1: {history['train_f1'][-1]:.4f}")
    logger.info(f"  å®é™…è®­ç»ƒè½®æ•°: {actual_epochs}/{config.FINETUNE_EPOCHS} epochs")
    if actual_epochs < config.FINETUNE_EPOCHS:
        logger.info(f"  âœ“ æ—©åœç”Ÿæ•ˆï¼ŒèŠ‚çœäº† {config.FINETUNE_EPOCHS - actual_epochs} è½®è®­ç»ƒ")
    if best_epoch > 0:
        if X_val_split is not None:
            logger.info(f"  æœ€ä½³ValF1*(pos=1, auto-threshold): {best_f1:.4f} (epoch {best_epoch}, th={best_threshold:.4f})")
        else:
            logger.info(f"  æœ€ä½³TrF1*(pos=1, auto-threshold): {best_f1:.4f} (epoch {best_epoch}, th={best_threshold:.4f})")
    logger.info("")
    
    # ç”Ÿæˆç‰¹å¾åˆ†å¸ƒå¯è§†åŒ–ï¼ˆStage 3åˆ†ç±»å™¨è®­ç»ƒåï¼‰
    logger.info("ğŸ“Š ç”Ÿæˆç‰¹å¾åˆ†å¸ƒå¯è§†åŒ–...")
    logger.info("  æå–è®­ç»ƒé›†ç‰¹å¾ç”¨äºå¯è§†åŒ–...")
    
    classifier.eval()
    if finetune_backbone:
        backbone.eval()
    with torch.no_grad():
        # Extract features from training set for visualization
        X_train_tensor = torch.FloatTensor(X_train).to(config.DEVICE)
        features_list = []
        batch_size = 64
        for i in range(0, len(X_train_tensor), batch_size):
            X_batch = X_train_tensor[i:i+batch_size]
            z_batch = backbone(X_batch, return_sequence=False)
            features_list.append(z_batch.cpu().numpy())
        train_features = np.concatenate(features_list, axis=0)
    
    feature_dist_path = os.path.join(config.CLASSIFICATION_DIR, "figures", "feature_distribution_stage3.png")
    plot_feature_space(train_features, y_train, feature_dist_path,
                      title="Stage 3: Feature Distribution (After Classifier Training)", method='tsne')
    logger.info(f"  âœ“ ç‰¹å¾åˆ†å¸ƒå›¾ (t-SNE): {feature_dist_path}")
    logger.info("")
    
    optimal_threshold = float(best_threshold)
    
    logger.info("ğŸ“ è¾“å‡ºæ–‡ä»¶è·¯å¾„:")

    # Save finetuned backbone (never overwrite original backbone checkpoint)
    if finetune_backbone:
        finetuned_backbone_path = os.path.join(config.CLASSIFICATION_DIR, "models", "backbone_finetuned.pth")
        torch.save(backbone.state_dict(), finetuned_backbone_path)
        logger.info(f"  âœ“ å¾®è°ƒåçš„éª¨å¹²ç½‘ç»œ: {finetuned_backbone_path}")

    # Save best model by validation F1(pos=1) if validation is enabled
    if best_state is not None:
        # Save best model
        best_path = os.path.join(config.CLASSIFICATION_DIR, "models", "classifier_best_f1.pth")
        torch.save(best_state, best_path)
        logger.info(f"  âœ“ æœ€ä½³æ¨¡å‹: {best_path}")

    # Save final model
    final_path = os.path.join(config.CLASSIFICATION_DIR, "models", "classifier_final.pth")
    torch.save(classifier.dual_mlp.state_dict(), final_path)
    logger.info(f"  âœ“ æœ€ç»ˆæ¨¡å‹: {final_path}")
    
    # Save training history
    history_path = os.path.join(config.CLASSIFICATION_DIR, "models", "training_history.npz")
    np.savez(history_path, **{k: np.array(v) for k, v in history.items()})
    logger.info(f"  âœ“ è®­ç»ƒå†å²: {history_path}")
    
    # Save model metadata (including backbone path used for training)
    # This helps test.py know which backbone to load
    if finetuned_backbone_path is not None:
        backbone_path = finetuned_backbone_path
    elif backbone_path is None:
        backbone_path = os.path.join(config.FEATURE_EXTRACTION_DIR, "models", "backbone_pretrained.pth")
    
    metadata_path = os.path.join(config.CLASSIFICATION_DIR, "models", "model_metadata.json")
    metadata = {
        'backbone_path': backbone_path,  # The actual backbone path used in training
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'n_samples': len(X_train),
        'n_original': n_original if n_original is not None else len(X_train),
        'finetune_epochs': config.FINETUNE_EPOCHS,
    }
    import json
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=2)
    logger.info(f"  âœ“ æ¨¡å‹å…ƒæ•°æ®: {metadata_path}")
    logger.info(f"    (è®°å½•äº†ä½¿ç”¨çš„éª¨å¹²ç½‘ç»œ: {os.path.basename(backbone_path)})")
    
    return classifier, history, optimal_threshold

def main(args):
    """Main training function"""
    
    # Setup
    set_seed(config.SEED)
    config.create_dirs()
    logger = setup_logger(os.path.join(config.OUTPUT_ROOT, "logs"), name='train')
    
    logger.info("="*70)
    logger.info("MEDAL-Lite Training Pipeline")
    logger.info("="*70)
    
    # GPUä¿¡æ¯
    if torch.cuda.is_available():
        logger.info(f"âœ“ GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
        logger.info(f"  æ˜¾å­˜æ€»é‡: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
        logger.info(f"  CUDAç‰ˆæœ¬: {torch.version.cuda}")
        logger.info(f"  ä½¿ç”¨è®¾å¤‡: {config.DEVICE}")
    else:
        logger.warning("âš  GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUè®­ç»ƒï¼ˆé€Ÿåº¦ä¼šè¾ƒæ…¢ï¼‰")
        logger.info(f"  ä½¿ç”¨è®¾å¤‡: {config.DEVICE}")
    
    logger.info(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Get stage range first to determine what to run
    start_stage = getattr(args, 'start_stage', 1)
    end_stage = getattr(args, 'end_stage', 3)
    
    # Convert start_stage/end_stage to int if it's a string
    if isinstance(start_stage, str):
        try:
            start_stage = int(start_stage)
        except ValueError:
            logger.error(f"âŒ æ— æ•ˆçš„èµ·å§‹é˜¶æ®µ: {start_stage}")
            return

    if isinstance(end_stage, str):
        try:
            end_stage = int(end_stage)
        except ValueError:
            logger.error(f"âŒ æ— æ•ˆçš„ç»“æŸé˜¶æ®µ: {end_stage}")
            return

    if end_stage < start_stage:
        logger.error(f"âŒ æ— æ•ˆé˜¶æ®µèŒƒå›´: start_stage={start_stage} > end_stage={end_stage}")
        return
    
    # ========================
    # Load Dataset (needed for Stage 1/2, and also for Stage 3-only runs)
    # ========================
    X_train = None
    y_train_clean = None
    y_train_noisy = None
    
    if start_stage <= 3:
        # Need to load raw dataset for Stage 1/2, and for Stage 3-only (skip Stage 2)
        logger.info("\n" + "="*70)
        logger.info("ğŸ“¦ æ•°æ®é›†åŠ è½½ Dataset Loading")
        logger.info("="*70)
        logger.info(f"è®­ç»ƒé›†é…ç½®:")
        logger.info(f"  æ­£å¸¸æµé‡è·¯å¾„: {config.BENIGN_TRAIN}")
        logger.info(f"  æ¶æ„æµé‡è·¯å¾„: {config.MALICIOUS_TRAIN}")
        logger.info(f"  åºåˆ—é•¿åº¦: {config.SEQUENCE_LENGTH} ä¸ªæ•°æ®åŒ…")
        logger.info(f"  è¯´æ˜: å°†è¯»å–ä¸Šè¿°è·¯å¾„ä¸‹æ‰€æœ‰pcapæ–‡ä»¶ï¼Œæµæ•°åœ¨å¤„ç†æ—¶ç»Ÿè®¡")
        logger.info("")
        
        # ä¼˜å…ˆä½¿ç”¨é¢„å¤„ç†å¥½çš„æ•°æ®
        if PREPROCESS_AVAILABLE and check_preprocessed_exists('train'):
            logger.info("âœ“ å‘ç°é¢„å¤„ç†æ–‡ä»¶ï¼Œç›´æ¥åŠ è½½...")
            X_train, y_train_clean, train_files = load_preprocessed('train')
            logger.info(f"  ä»é¢„å¤„ç†æ–‡ä»¶åŠ è½½: {X_train.shape[0]} ä¸ªæ ·æœ¬")
        else:
            # ä»PCAPæ–‡ä»¶åŠ è½½
            logger.info("å¼€å§‹åŠ è½½è®­ç»ƒæ•°æ®é›†ï¼ˆä»PCAPæ–‡ä»¶ï¼‰...")
            logger.info("ğŸ’¡ æç¤º: è¿è¡Œ 'python preprocess.py' å¯é¢„å¤„ç†æ•°æ®ï¼ŒåŠ é€Ÿåç»­è®­ç»ƒ")
            X_train, y_train_clean, train_files = load_dataset(
                benign_dir=config.BENIGN_TRAIN,
                malicious_dir=config.MALICIOUS_TRAIN,
                sequence_length=config.SEQUENCE_LENGTH
            )
        
        if X_train is None:
            logger.error("âŒ è®­ç»ƒæ•°æ®é›†åŠ è½½å¤±è´¥!")
            return
        
        logger.info("")
        logger.info("âœ“ è®­ç»ƒæ•°æ®é›†åŠ è½½å®Œæˆ")
        logger.info(f"  æ•°æ®å½¢çŠ¶: {X_train.shape} (æ ·æœ¬æ•°Ã—åºåˆ—é•¿åº¦Ã—ç‰¹å¾ç»´åº¦)")
        logger.info(f"  æ­£å¸¸æ ·æœ¬: {(y_train_clean==0).sum()} ä¸ª")
        logger.info(f"  æ¶æ„æ ·æœ¬: {(y_train_clean==1).sum()} ä¸ª")
        logger.info("")
        
        # æ³¨æ„ï¼šStage 1 (ç‰¹å¾æå–) ä¸éœ€è¦æ ‡ç­¾ï¼Œæ˜¯æ— ç›‘ç£å­¦ä¹ 
        # åªæœ‰ Stage 2 (æ ‡ç­¾çŸ«æ­£) å’Œ Stage 3 (åˆ†ç±») æ‰éœ€è¦æ ‡ç­¾
        # å› æ­¤ï¼Œæ ‡ç­¾å™ªå£°æ³¨å…¥å»¶è¿Ÿåˆ°éœ€è¦æ—¶å†è¿›è¡Œ
        if start_stage >= 2:
            # Inject label noise (only needed for Stage 2+)
            logger.info(f"ğŸ”€ æ³¨å…¥æ ‡ç­¾å™ªå£° ({config.LABEL_NOISE_RATE*100:.0f}%)...")
            y_train_noisy, noise_mask = inject_label_noise(y_train_clean, config.LABEL_NOISE_RATE)
            logger.info(f"âœ“ å™ªå£°æ ‡ç­¾åˆ›å»ºå®Œæˆ: {noise_mask.sum()} ä¸ªæ ‡ç­¾è¢«ç¿»è½¬")
            logger.info(f"  åŸå§‹æ ‡ç­¾åˆ†å¸ƒ: æ­£å¸¸={(y_train_clean==0).sum()}, æ¶æ„={(y_train_clean==1).sum()}")
            logger.info(f"  å™ªå£°æ ‡ç­¾åˆ†å¸ƒ: æ­£å¸¸={(y_train_noisy==0).sum()}, æ¶æ„={(y_train_noisy==1).sum()}")
        else:
            # Stage 1 ä¸éœ€è¦æ ‡ç­¾
            logger.info("ğŸ’¡ Stage 1 (ç‰¹å¾æå–) æ˜¯æ— ç›‘ç£å­¦ä¹ ï¼Œä¸ä½¿ç”¨æ ‡ç­¾")
            y_train_noisy = None
            noise_mask = None
    else:
        # Starting from later stages, skip raw dataset loading
        logger.info("\n" + "="*70)
        logger.info("â­ï¸  è·³è¿‡åŸå§‹æ•°æ®é›†åŠ è½½")
        logger.info("="*70)
        logger.info("")
    
    # ========================
    # Stage 1: Pre-train Backbone (unsupervised, no labels needed)
    # ========================
    backbone = MicroBiMambaBackbone(config)
    
    if start_stage <= 1:
        # æ ¹æ®å¯¹æ¯”å­¦ä¹ æ–¹æ³•é€‰æ‹©æ‰¹æ¬¡å¤§å°
        use_instance_contrastive = getattr(config, 'USE_INSTANCE_CONTRASTIVE', False)
        contrastive_method = getattr(config, 'CONTRASTIVE_METHOD', 'infonce')
        
        if use_instance_contrastive and str(contrastive_method).lower() == 'nnclr':
            # NNCLR éœ€è¦æ›´å°çš„æ‰¹æ¬¡ä»¥é¿å…æ˜¾å­˜æº¢å‡º
            batch_size = getattr(config, 'PRETRAIN_BATCH_SIZE_NNCLR', 64)
            logger.info(f"âœ“ æ£€æµ‹åˆ° NNCLR æ–¹æ³•ï¼Œä½¿ç”¨ä¸“ç”¨æ‰¹æ¬¡å¤§å°: {batch_size}")
            logger.info(f"  (NNCLR æ˜¾å­˜å ç”¨é«˜ï¼Œéœ€è¦å‡å°æ‰¹æ¬¡)")
        else:
            # SimMTM æˆ– InfoNCE ä½¿ç”¨é»˜è®¤æ‰¹æ¬¡å¤§å°
            batch_size = config.PRETRAIN_BATCH_SIZE
        
        # SimMTM is unsupervised, so we only need X_train, not labels
        dataset = TensorDataset(torch.FloatTensor(X_train))
        train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        backbone, pretrain_history = stage1_pretrain_backbone(backbone, train_loader, config, logger)
        # Backbone is already saved in stage1_pretrain_backbone function
        if end_stage <= 1:
            logger.info("\n" + "="*70)
            logger.info("âœ… å·²å®Œæˆåˆ° Stage 1ï¼ŒæŒ‰ end_stage è®¾ç½®æå‰ç»“æŸ")
            logger.info("="*70)
            return backbone
    else:
        # Load pre-trained backbone (required for Stage 2 and 3)
        # ä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„backbone_path
        if hasattr(args, 'backbone_path') and args.backbone_path:
            backbone_path = args.backbone_path
            logger.info(f"ä½¿ç”¨æŒ‡å®šçš„éª¨å¹²ç½‘ç»œ: {backbone_path}")
        else:
            backbone_path = os.path.join(config.FEATURE_EXTRACTION_DIR, "models", "backbone_pretrained.pth")
        
        retrain_backbone = bool(getattr(args, 'retrain_backbone', False))
        if retrain_backbone:
            logger.warning("âš  --retrain_backbone å·²æŒ‡å®šï¼šå°†ä½¿ç”¨éšæœºåˆå§‹åŒ–éª¨å¹²ç½‘ç»œï¼ˆä¸åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼‰")
            backbone.freeze()
        else:
            if os.path.exists(backbone_path):
                logger.info("ğŸ“¥ è¾“å…¥æ•°æ®è·¯å¾„:")
                logger.info(f"  âœ“ éª¨å¹²ç½‘ç»œæ¨¡å‹: {backbone_path}")
                logger.info("")
                logger.info(f"âœ“ åŠ è½½å·²æœ‰éª¨å¹²ç½‘ç»œ: {backbone_path}")
                try:
                    backbone_state = torch.load(backbone_path, map_location=config.DEVICE, weights_only=True)
                except TypeError:
                    backbone_state = torch.load(backbone_path, map_location=config.DEVICE)
                load_state_dict_shape_safe(backbone, backbone_state, logger, prefix="backbone")
                logger.info(f"âœ“ åŠ è½½å·²æœ‰éª¨å¹²ç½‘ç»œ: {backbone_path}")
            else:
                logger.error(f"âŒ æ‰¾ä¸åˆ°é¢„è®­ç»ƒéª¨å¹²ç½‘ç»œ: {backbone_path}")
                logger.error("   è¯·å…ˆè¿è¡Œ Stage 1 æˆ–ä» Stage 1 å¼€å§‹è®­ç»ƒ")
                return
    
    # ========================
    # Stage 2: Label Correction & Augmentation
    # ========================
    if start_stage <= 2 and end_stage >= 2:
        # å¦‚æœä»Stage 2å¼€å§‹ï¼Œä½†è¿˜æ²¡æœ‰æ³¨å…¥å™ªå£°ï¼Œç°åœ¨æ³¨å…¥
        if y_train_noisy is None and y_train_clean is not None:
            logger.info("")
            logger.info(f"ğŸ”€ æ³¨å…¥æ ‡ç­¾å™ªå£° ({config.LABEL_NOISE_RATE*100:.0f}%)...")
            y_train_noisy, noise_mask = inject_label_noise(y_train_clean, config.LABEL_NOISE_RATE)
            logger.info(f"âœ“ å™ªå£°æ ‡ç­¾åˆ›å»ºå®Œæˆ: {noise_mask.sum()} ä¸ªæ ‡ç­¾è¢«ç¿»è½¬")
            logger.info(f"  åŸå§‹æ ‡ç­¾åˆ†å¸ƒ: æ­£å¸¸={(y_train_clean==0).sum()}, æ¶æ„={(y_train_clean==1).sum()}")
            logger.info(f"  å™ªå£°æ ‡ç­¾åˆ†å¸ƒ: æ­£å¸¸={(y_train_noisy==0).sum()}, æ¶æ„={(y_train_noisy==1).sum()}")
        
        stage2_mode = getattr(args, 'stage2_mode', 'standard')
        X_augmented, y_augmented, sample_weights, correction_stats, tabddpm, n_original = stage2_label_correction_and_augmentation(
            backbone, X_train, y_train_noisy, y_train_clean, config, logger, stage2_mode=stage2_mode
        )
        # TabDDPM is already saved in stage2_label_correction_and_augmentation function
        if end_stage <= 2:
            logger.info("\n" + "="*70)
            logger.info("âœ… å·²å®Œæˆåˆ° Stage 2ï¼ŒæŒ‰ end_stage è®¾ç½®æå‰ç»“æŸ")
            logger.info("="*70)
            return backbone
    elif end_stage >= 3:
        # è·³è¿‡Stage 2ï¼š
        # - å¦‚æœæ˜¯ Stage 3-only(start_stage==3)ï¼ŒæŒ‰å†³ç­–ç›´æ¥ä½¿ç”¨åŸå§‹å¹²å‡€æ•°æ®ï¼ˆä¸åŠ è½½ä»»ä½•ç¦»çº¿å¢å¼ºæ•°æ®ï¼‰
        # - å¦åˆ™ï¼šå…¼å®¹æ—§æµç¨‹ï¼ˆå¦‚æœå­˜åœ¨ augmented_data.npz åˆ™åŠ è½½ï¼‰
        augmented_data_path = os.path.join(config.DATA_AUGMENTATION_DIR, "models", "augmented_data.npz")
        if int(start_stage) == 3 and X_train is not None:
            logger.info("\n" + "="*70)
            logger.info("âœ… Stage 3-only: å·²è·³è¿‡ Stage 2ï¼ˆä¸ä½¿ç”¨ TabDDPM ç¦»çº¿å¢å¼ºæ•°æ®ï¼‰")
            logger.info("="*70)
            logger.info("")
            X_augmented = X_train
            y_augmented = y_train_clean
            sample_weights = np.ones(len(X_train))
            n_original = len(X_train)
            logger.info(f"âœ“ ä½¿ç”¨åŸå§‹å¹²å‡€æ•°æ®: {len(X_train)} ä¸ªæ ·æœ¬")
            if os.path.exists(augmented_data_path):
                logger.info(f"  (å·²å¿½ç•¥ç¦»çº¿å¢å¼ºæ–‡ä»¶: {augmented_data_path})")
        elif os.path.exists(augmented_data_path):
            # åŠ è½½å·²æœ‰çš„å¢å¼ºæ•°æ®ï¼ˆå…¼å®¹æ—§æµç¨‹ï¼‰
            logger.info("ğŸ“¥ è¾“å…¥æ•°æ®è·¯å¾„:")
            logger.info(f"  âœ“ å¢å¼ºæ•°æ®: {augmented_data_path}")
            backbone_path = os.path.join(config.FEATURE_EXTRACTION_DIR, "models", "backbone_pretrained.pth")
            logger.info(f"  âœ“ éª¨å¹²ç½‘ç»œæ¨¡å‹: {backbone_path}")
            logger.info("")
            logger.info(f"âœ“ åŠ è½½å·²æœ‰å¢å¼ºæ•°æ®: {augmented_data_path}")
            data = np.load(augmented_data_path)
            X_augmented = data['X_augmented']
            y_augmented = data['y_augmented']
            sample_weights = data['sample_weights'] if 'sample_weights' in data else np.ones(len(X_augmented))
            # Get n_original from saved data
            if 'n_original' in data:
                n_original = int(data['n_original'])
            else:
                # Fallback: assume all data is original if metadata not found
                n_original = len(X_augmented)
                logger.warning("âš ï¸  æœªæ‰¾åˆ°åŸå§‹æ•°æ®æ•°é‡æ ‡è®°ï¼Œå‡è®¾æ‰€æœ‰æ•°æ®å‡ä¸ºåŸå§‹æ•°æ®")
        elif X_train is not None:
            # ä½¿ç”¨åŸå§‹æ•°æ®ï¼ˆä¸åšå¢å¼ºå’ŒçŸ«æ­£ï¼‰
            logger.info("\n" + "="*70)
            logger.info("âš ï¸  è·³è¿‡ Stage 2ï¼Œä½¿ç”¨åŸå§‹æ•°æ®ï¼ˆå¸¦å™ªå£°æ ‡ç­¾ï¼‰")
            logger.info("="*70)
            logger.info("")
            
            # å¦‚æœè¿˜æ²¡æœ‰æ³¨å…¥å™ªå£°ï¼Œç°åœ¨æ³¨å…¥
            if y_train_noisy is None:
                logger.info(f"ğŸ”€ æ³¨å…¥æ ‡ç­¾å™ªå£° ({config.LABEL_NOISE_RATE*100:.0f}%)...")
                y_train_noisy, noise_mask = inject_label_noise(y_train_clean, config.LABEL_NOISE_RATE)
                logger.info(f"âœ“ å™ªå£°æ ‡ç­¾åˆ›å»ºå®Œæˆ: {noise_mask.sum()} ä¸ªæ ‡ç­¾è¢«ç¿»è½¬")
                logger.info(f"  åŸå§‹æ ‡ç­¾åˆ†å¸ƒ: æ­£å¸¸={(y_train_clean==0).sum()}, æ¶æ„={(y_train_clean==1).sum()}")
                logger.info(f"  å™ªå£°æ ‡ç­¾åˆ†å¸ƒ: æ­£å¸¸={(y_train_noisy==0).sum()}, æ¶æ„={(y_train_noisy==1).sum()}")
            
            X_augmented = X_train
            y_augmented = y_train_noisy
            sample_weights = np.ones(len(X_train))
            n_original = len(X_train)
            
            logger.info(f"âœ“ ä½¿ç”¨åŸå§‹æ•°æ®: {len(X_train)} ä¸ªæ ·æœ¬")
            logger.info("  âš ï¸  æ³¨æ„ï¼šæœªè¿›è¡Œæ ‡ç­¾çŸ«æ­£å’Œæ•°æ®å¢å¼ºï¼Œå¯èƒ½å½±å“æ€§èƒ½")
        else:
            logger.error(f"âŒ æ‰¾ä¸åˆ°å¢å¼ºæ•°æ®: {augmented_data_path}")
            logger.error("   ä¸”æœªåŠ è½½åŸå§‹æ•°æ®")
            logger.error("   è¯·å…ˆè¿è¡Œ Stage 2 æˆ–ä» Stage 1 å¼€å§‹è®­ç»ƒ")
            return
    
    # ========================
    # Stage 3: Fine-tune Classifier
    # ========================
    if end_stage >= 3 and start_stage <= 3:
        # ç¡®å®šå®é™…ä½¿ç”¨çš„backboneè·¯å¾„ï¼ˆç”¨äºè®°å½•å…ƒæ•°æ®ï¼‰
        if hasattr(args, 'backbone_path') and args.backbone_path:
            actual_backbone_path = args.backbone_path
        else:
            actual_backbone_path = os.path.join(config.FEATURE_EXTRACTION_DIR, "models", "backbone_pretrained.pth")
        
        classifier, finetune_history, optimal_threshold = stage3_finetune_classifier(
            backbone, X_augmented, y_augmented, sample_weights, config, logger, 
            n_original=n_original, backbone_path=actual_backbone_path
        )
    else:
        logger.info("\n" + "="*70)
        logger.info("â­ï¸  è·³è¿‡ Stage 3ï¼ˆæŒ‰ end_stage è®¾ç½®ï¼‰")
        logger.info("="*70)
        return backbone
    
    # Plot training history
    history_fig_path = os.path.join(config.CLASSIFICATION_DIR, "figures", "training_history.png")
    plot_training_history(finetune_history, history_fig_path)
    logger.info(f"  âœ“ è®­ç»ƒå†å²å›¾è¡¨: {history_fig_path}")
    
    logger.info("")
    logger.info("="*70)
    logger.info("ğŸ‰ è®­ç»ƒå®Œæˆ! Training Complete!")
    logger.info("="*70)
    logger.info("")
    logger.info("ğŸ“Š è®­ç»ƒæ€»ç»“:")
    logger.info(f"  Stage 1: éª¨å¹²ç½‘ç»œé¢„è®­ç»ƒ - {config.PRETRAIN_EPOCHS} epochs")
    logger.info(f"  Stage 2: æ ‡ç­¾çŸ«æ­£+æ•°æ®å¢å¼º - å®Œæˆ")
    logger.info(f"  Stage 3: åˆ†ç±»å™¨å¾®è°ƒ - {config.FINETUNE_EPOCHS} epochs")
    logger.info("")
    logger.info("ğŸ“¥ è¾“å…¥æ•°æ®è·¯å¾„:")
    logger.info(f"  âœ“ è®­ç»ƒæ•°æ®: {config.BENIGN_TRAIN} (æ­£å¸¸), {config.MALICIOUS_TRAIN} (æ¶æ„)")
    logger.info("")
    logger.info("ğŸ“ è¾“å‡ºæ–‡ä»¶è·¯å¾„:")
    logger.info(f"  âœ“ ç‰¹å¾æå–: {config.FEATURE_EXTRACTION_DIR}")
    logger.info(f"    - éª¨å¹²ç½‘ç»œ: {os.path.join(config.FEATURE_EXTRACTION_DIR, 'models', 'backbone_pretrained.pth')}")
    logger.info(f"    - è®­ç»ƒç‰¹å¾: {os.path.join(config.FEATURE_EXTRACTION_DIR, 'models', 'train_features.npy')}")
    logger.info(f"  âœ“ æ ‡ç­¾çŸ«æ­£: {config.LABEL_CORRECTION_DIR}")
    logger.info(f"    - çŸ«æ­£ç»“æœ: {os.path.join(config.LABEL_CORRECTION_DIR, 'models', 'correction_results.npz')}")
    logger.info(f"  âœ“ æ•°æ®å¢å¼º: {config.DATA_AUGMENTATION_DIR}")
    logger.info(f"    - TabDDPMæ¨¡å‹: {os.path.join(config.DATA_AUGMENTATION_DIR, 'models', 'tabddpm.pth')}")
    logger.info(f"    - å¢å¼ºæ•°æ®: {os.path.join(config.DATA_AUGMENTATION_DIR, 'models', 'augmented_data.npz')}")
    logger.info(f"  âœ“ åˆ†ç±»å™¨:   {config.CLASSIFICATION_DIR}")
    logger.info(f"    - åˆ†ç±»å™¨æ¨¡å‹: {os.path.join(config.CLASSIFICATION_DIR, 'models', 'classifier_final.pth')}")
    logger.info(f"    - è®­ç»ƒå†å²: {os.path.join(config.CLASSIFICATION_DIR, 'models', 'training_history.npz')}")
    logger.info("")
    logger.info("ğŸ’¡ ä¸‹ä¸€æ­¥: è¿è¡Œ test.py è¯„ä¼°æ¨¡å‹æ€§èƒ½")
    logger.info("="*70)
    
    return classifier


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train MEDAL-Lite model")
    parser.add_argument("--noise_rate", type=float, default=0.30, help="Label noise rate")
    parser.add_argument("--start_stage", type=int, default=1, choices=[1, 2, 3], 
                       help="Start from which stage (1=backbone pretrain, 2=label correction, 3=classifier finetune)")
    parser.add_argument("--end_stage", type=int, default=3, choices=[1, 2, 3],
                       help="End at which stage (1/2/3). Use end_stage=2 for Stage2-only run")
    parser.add_argument("--stage2_mode", type=str, default="standard", choices=["standard", "clean_augment_only"])
    parser.add_argument("--retrain_backbone", action="store_true",
                       help="Use randomly initialized backbone instead of loading pretrained weights")
    parser.add_argument("--backbone_path", type=str, default=None,
                       help="Path to specific backbone model (e.g., backbone_SimCLR_500.pth)")
    
    args = parser.parse_args()
    
    # Override config if arguments provided
    if args.noise_rate is not None:
        config.LABEL_NOISE_RATE = args.noise_rate
    
    main(args)

