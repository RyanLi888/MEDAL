"""
MEDAL-Lite æ•°æ®å¢å¼ºè®­ç»ƒ+æµ‹è¯•è„šæœ¬ (é‡æ„ç‰ˆ)
=========================================
ä½¿ç”¨çœŸå®æ ‡ç­¾ + TabDDPMæ•°æ®å¢å¼ºè®­ç»ƒåˆ†ç±»å™¨ï¼Œå¤ç”¨ä¸»æµç¨‹ä»£ç 
"""
import os
import sys
from pathlib import Path

project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

import json
import argparse
from datetime import datetime

import numpy as np
import torch
from torch.utils.data import TensorDataset, DataLoader

from MoudleCode.utils.config import config
from MoudleCode.utils.helpers import set_seed, setup_logger
from MoudleCode.utils.logging_utils import (
    log_section_header, log_data_stats, log_input_paths, log_output_paths, log_final_summary
)
from MoudleCode.preprocessing.pcap_parser import load_dataset
from MoudleCode.feature_extraction.backbone import build_backbone

try:
    from scripts.utils.preprocess import check_preprocessed_exists, load_preprocessed, normalize_burstsize_inplace
    PREPROCESS_AVAILABLE = True
except Exception:
    PREPROCESS_AVAILABLE = False

from scripts.training.train import stage1_pretrain_backbone, stage2_label_correction_and_augmentation, stage3_finetune_classifier
from scripts.testing.test import main as test_main


def _safe_makedirs(path: str) -> None:
    os.makedirs(path, exist_ok=True)


def _load_train_dataset():
    """åŠ è½½è®­ç»ƒæ•°æ®é›†"""
    if PREPROCESS_AVAILABLE and check_preprocessed_exists('train'):
        X_train, y_train, _ = load_preprocessed('train')
        X_train = normalize_burstsize_inplace(X_train)
        return X_train, y_train

    X_train, y_train, _ = load_dataset(
        benign_dir=config.BENIGN_TRAIN,
        malicious_dir=config.MALICIOUS_TRAIN,
        sequence_length=config.SEQUENCE_LENGTH,
    )
    X_train = normalize_burstsize_inplace(X_train)
    return X_train, y_train


def main():
    parser = argparse.ArgumentParser(description="æ•°æ®å¢å¼ºè®­ç»ƒ+æµ‹è¯•")
    parser.add_argument('--retrain_backbone', action='store_true', help='é‡æ–°è®­ç»ƒéª¨å¹²ç½‘ç»œ')
    parser.add_argument('--backbone_path', type=str, default='', help='éª¨å¹²ç½‘ç»œè·¯å¾„')
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--run_tag', type=str, default='')
    args = parser.parse_args()

    set_seed(args.seed)
    config.create_dirs()
    logger = setup_logger(os.path.join(config.OUTPUT_ROOT, 'logs'), name='augmented_train_test')

    # é…ç½®ï¼šä½¿ç”¨æœ€ä¼˜å‚æ•°
    config.USE_FOCAL_LOSS = True
    config.USE_BCE_LOSS = False
    config.USE_SOFT_F1_LOSS = False
    config.STAGE3_ONLINE_AUGMENTATION = False
    config.STAGE3_USE_ST_MIXUP = False
    config.FINETUNE_BACKBONE = False  # ç‰¹å¾ç©ºé—´æ•°æ®ä¸æ”¯æŒéª¨å¹²å¾®è°ƒ
    config.FINETUNE_VAL_SPLIT = 0.0
    config.FINETUNE_ES_ALLOW_TRAIN_METRIC = True

    run_tag = args.run_tag.strip() or datetime.now().strftime('%Y%m%d_%H%M%S')
    run_dir = os.path.join(config.LABEL_CORRECTION_DIR, 'analysis', 'augmented_runs', run_tag)
    _safe_makedirs(run_dir)
    
    # éš”ç¦»è¾“å‡ºç›®å½•
    original_classification_dir = config.CLASSIFICATION_DIR
    original_result_dir = config.RESULT_DIR
    original_data_aug_dir = config.DATA_AUGMENTATION_DIR
    
    config.CLASSIFICATION_DIR = os.path.join(run_dir, 'classification')
    config.RESULT_DIR = os.path.join(run_dir, 'result')
    config.DATA_AUGMENTATION_DIR = os.path.join(run_dir, 'data_augmentation')
    
    for d in [config.CLASSIFICATION_DIR, config.RESULT_DIR, config.DATA_AUGMENTATION_DIR]:
        _safe_makedirs(d)
        _safe_makedirs(os.path.join(d, 'models'))
        _safe_makedirs(os.path.join(d, 'figures'))
        _safe_makedirs(os.path.join(d, 'logs'))

    try:
        log_section_header(logger, "ğŸš€ æ•°æ®å¢å¼ºè®­ç»ƒ+æµ‹è¯•æ¨¡å¼ï¼ˆæ¶ˆèå®éªŒï¼‰")
        logger.info(f'è¿è¡Œç›®å½•: {run_dir}')
        logger.info(f'æ—¶é—´æˆ³: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}')
        logger.info('')
        logger.info('ğŸ“‹ è®­ç»ƒæ¨¡å¼è¯´æ˜:')
        logger.info('  - æ•°æ®æ¥æº: åŸå§‹æ•°æ® + TabDDPMå¢å¼ºæ•°æ®')
        logger.info('  - æ ‡ç­¾: çœŸå®æ ‡ç­¾ï¼ˆæ— å™ªå£°ï¼‰')
        logger.info('  - æ ‡ç­¾çŸ«æ­£: è·³è¿‡ï¼ˆä½¿ç”¨çœŸå®æ ‡ç­¾ï¼‰')
        logger.info('  - æ•°æ®å¢å¼º: TabDDPMï¼ˆç‰¹å¾ç©ºé—´ï¼‰')
        logger.info('  - éª¨å¹²å¾®è°ƒ: å–å†³äºæ··åˆè®­ç»ƒæ¨¡å¼')
        logger.info('')
        
        # è¾“å‡ºé…ç½®
        config.log_stage_config(logger, "Stage 2")
        config.log_stage_config(logger, "Stage 3")
        
        # åŠ è½½æ•°æ®
        X_train, y_train_true = _load_train_dataset()
        if X_train is None:
            raise RuntimeError('è®­ç»ƒæ•°æ®é›†åŠ è½½å¤±è´¥')

        y_corrected = y_train_true.astype(int)
        correction_weight = np.ones(len(y_corrected), dtype=np.float32)

        log_data_stats(logger, {
            "æ•°æ®å½¢çŠ¶": f"{X_train.shape} (æ ·æœ¬æ•°Ã—åºåˆ—é•¿åº¦Ã—ç‰¹å¾ç»´åº¦)",
            "è®­ç»ƒæ ·æœ¬æ€»æ•°": len(X_train),
            "æ­£å¸¸æ ·æœ¬": int((y_corrected == 0).sum()),
            "æ¶æ„æ ·æœ¬": int((y_corrected == 1).sum()),
            "æ•°æ®ç±»å‹": "åŸå§‹åºåˆ—ï¼ˆå°†è¿›è¡Œç‰¹å¾ç©ºé—´å¢å¼ºï¼‰"
        }, "åŸå§‹è®­ç»ƒæ•°æ®ç»Ÿè®¡")
        
        # åŠ è½½éª¨å¹²ç½‘ç»œ
        backbone = build_backbone(config, logger=logger)
        backbone_path = args.backbone_path if args.backbone_path else os.path.join(config.FEATURE_EXTRACTION_DIR, 'models', 'backbone_pretrained.pth')
        
        if args.retrain_backbone:
            logger.info('ğŸ” é‡æ–°è®­ç»ƒéª¨å¹²ç½‘ç»œï¼ˆStage 1 è‡ªç›‘ç£é¢„è®­ç»ƒï¼‰...')
            use_instance_contrastive = getattr(config, 'USE_INSTANCE_CONTRASTIVE', False)
            contrastive_method = getattr(config, 'CONTRASTIVE_METHOD', 'infonce')
            method_lower = str(contrastive_method).lower()
            if use_instance_contrastive and method_lower == 'nnclr':
                batch_size = getattr(config, 'PRETRAIN_BATCH_SIZE_NNCLR', 64)
            elif use_instance_contrastive and method_lower == 'simsiam':
                batch_size = getattr(config, 'PRETRAIN_BATCH_SIZE_SIMSIAM', config.PRETRAIN_BATCH_SIZE)
            else:
                batch_size = config.PRETRAIN_BATCH_SIZE

            dataset = TensorDataset(torch.FloatTensor(X_train))
            train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

            backbone = backbone.to(config.DEVICE)
            backbone, _pretrain_history = stage1_pretrain_backbone(backbone, train_loader, config, logger)

            default_backbone_path = os.path.join(config.FEATURE_EXTRACTION_DIR, 'models', 'backbone_pretrained.pth')
            if args.backbone_path:
                torch.save(backbone.state_dict(), args.backbone_path)
                backbone_path = args.backbone_path
                logger.info(f'âœ“ å·²ä¿å­˜æ–°éª¨å¹²ç½‘ç»œ: {backbone_path}')
            else:
                backbone_path = default_backbone_path
                logger.info(f'âœ“ å·²ä¿å­˜æ–°éª¨å¹²ç½‘ç»œ: {backbone_path}')
        elif os.path.exists(backbone_path):
            logger.info(f'âœ“ åŠ è½½éª¨å¹²ç½‘ç»œ: {backbone_path}')
            try:
                state_dict = torch.load(backbone_path, map_location=config.DEVICE, weights_only=True)
            except TypeError:
                state_dict = torch.load(backbone_path, map_location=config.DEVICE)
            backbone.load_state_dict(state_dict, strict=False)
            backbone.freeze()
        else:
            logger.warning('âš  ä½¿ç”¨éšæœºåˆå§‹åŒ–éª¨å¹²ç½‘ç»œ')
            backbone.freeze()
        
        # Stage 2: æ•°æ®å¢å¼ºï¼ˆè·³è¿‡æ ‡ç­¾çŸ«æ­£ï¼‰
        Z_aug, y_aug, w_aug, correction_stats, tabddpm, n_original = stage2_label_correction_and_augmentation(
            backbone, X_train, y_corrected, y_corrected, config, logger,
            stage2_mode='clean_augment_only'
        )

        # Stage 3: åˆ†ç±»å™¨è®­ç»ƒ
        # åŠ è½½åŸå§‹åºåˆ—ç”¨äºæ··åˆè®­ç»ƒ
        real_kept_path = os.path.join(config.DATA_AUGMENTATION_DIR, "models", "real_kept_data.npz")
        X_real = None
        use_mixed_stream = bool(getattr(config, 'STAGE3_MIXED_STREAM', True))
        
        if os.path.exists(real_kept_path) and use_mixed_stream:
            logger.info(f'âœ“ åŠ è½½åŸå§‹åºåˆ—: {real_kept_path}')
            real_data = np.load(real_kept_path)
            X_real = real_data['X_real']
            logger.info(f'  - åŸå§‹åºåˆ—å½¢çŠ¶: {X_real.shape}')
        else:
            use_mixed_stream = False
            logger.info('âš ï¸ æ··åˆè®­ç»ƒæ¨¡å¼å·²ç¦ç”¨')

        stage3_finetune_classifier(
            backbone, Z_aug, y_aug, w_aug, config, logger,
            n_original=n_original, backbone_path=backbone_path,
            X_train_real=X_real, use_mixed_stream=use_mixed_stream
        )

        # æµ‹è¯•
        log_section_header(logger, "ğŸ§ª æµ‹è¯•è¯„ä¼°")
        classifier_best = os.path.join(config.CLASSIFICATION_DIR, 'models', 'classifier_best_f1.pth')
        
        meta_path = os.path.join(config.CLASSIFICATION_DIR, 'models', 'model_metadata.json')
        backbone_path_for_test = backbone_path
        if os.path.exists(meta_path):
            with open(meta_path, 'r') as f:
                meta = json.load(f)
            bp = meta.get('backbone_path', '')
            if bp:
                backbone_path_for_test = bp

        test_args = argparse.Namespace(backbone_path=backbone_path_for_test, classifier_path=classifier_best)
        test_main(test_args)

        log_final_summary(logger, "å®Œæˆ", {}, {
            "è¿è¡Œç›®å½•": run_dir,
            "åˆ†ç±»å™¨": classifier_best,
            "æµ‹è¯•ç»“æœ": config.RESULT_DIR
        })
        
    finally:
        config.CLASSIFICATION_DIR = original_classification_dir
        config.RESULT_DIR = original_result_dir
        config.DATA_AUGMENTATION_DIR = original_data_aug_dir


if __name__ == '__main__':
    main()
