"""
MEDAL-Lite Testing Script
Evaluate trained model on test dataset
"""
import sys
import os
from pathlib import Path

# Ensure project root is on sys.path when running as a script
project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

import torch
from torch.utils.data import TensorDataset, DataLoader
import numpy as np
import random
import hashlib
import argparse
from datetime import datetime
import json
import csv

from sklearn.metrics import precision_score, recall_score, f1_score

from MoudleCode.utils.config import config
from MoudleCode.utils.helpers import (
    set_seed, setup_logger, calculate_metrics, print_metrics, find_optimal_threshold
)
from MoudleCode.utils.visualization import (
    plot_feature_space, plot_confusion_matrix, plot_roc_curve, plot_probability_distribution
)
from MoudleCode.preprocessing.pcap_parser import load_dataset
from MoudleCode.feature_extraction.backbone import MicroBiMambaBackbone, build_backbone
from MoudleCode.classification.dual_stream import MEDAL_Classifier

# å¯¼å…¥é¢„å¤„ç†æ¨¡å—
try:
    from scripts.utils.preprocess import check_preprocessed_exists, load_preprocessed, normalize_burstsize_inplace
    PREPROCESS_AVAILABLE = True
except ImportError:
    PREPROCESS_AVAILABLE = False


def _rng_fingerprint_short() -> str:
    h = hashlib.sha256()
    try:
        h.update(repr(random.getstate()).encode('utf-8'))
    except Exception:
        h.update(b'py_random_error')
    try:
        ns = np.random.get_state()
        h.update(str(ns[0]).encode('utf-8'))
        h.update(np.asarray(ns[1], dtype=np.uint32).tobytes())
        h.update(str(ns[2]).encode('utf-8'))
        h.update(str(ns[3]).encode('utf-8'))
        h.update(str(ns[4]).encode('utf-8'))
    except Exception:
        h.update(b'numpy_random_error')
    try:
        h.update(torch.get_rng_state().detach().cpu().numpy().tobytes())
    except Exception:
        h.update(b'torch_cpu_rng_error')
    try:
        if torch.cuda.is_available():
            for s in torch.cuda.get_rng_state_all():
                h.update(s.detach().cpu().numpy().tobytes())
        else:
            h.update(b'no_cuda')
    except Exception:
        h.update(b'torch_cuda_rng_error')
    return h.hexdigest()[:16]


def _seed_snapshot() -> str:
    torch_seed = None
    try:
        torch_seed = int(torch.initial_seed())
    except Exception:
        torch_seed = None
    return (
        f"config.SEED={int(getattr(config, 'SEED', -1))} | "
        f"torch.initial_seed={torch_seed}"
    )


def _load_classifier_checkpoint(classifier, state_dict, logger=None):
    """
    Load classifier checkpoint (new architecture only: 32 â†’ 16 â†’ 8 â†’ 2).
    
    Supports loading either:
    - Full classifier.state_dict (with 'backbone.' and 'dual_mlp.' prefixes)
    - Only dual_mlp.state_dict (with or without 'dual_mlp.' prefix)
    
    Raises RuntimeError if model architecture doesn't match (old models not supported).
    """
    if not isinstance(state_dict, dict):
        raise TypeError(f"Expected state_dict to be a dict, got {type(state_dict)}")

    # å°è¯•åŠ è½½å®Œæ•´æ¨¡å‹
    try:
        classifier.load_state_dict(state_dict, strict=True)
        if logger is not None:
            logger.info("âœ“ å®Œæ•´æ¨¡å‹åŠ è½½æˆåŠŸ")
        return
    except Exception as e:
        if logger is not None:
            logger.info(f"æ•´æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°è¯•ä»…åŠ è½½åˆ†ç±»å¤´(dual_mlp): {str(e)[:100]}...")

    # å¤„ç†å¸¦æœ‰ 'dual_mlp.' å‰ç¼€çš„é”®ï¼ˆå¯èƒ½åŒæ—¶åŒ…å«backboneå’Œdual_mlpçš„é”®ï¼‰
    keys = list(state_dict.keys())
    dual_mlp_keys = [k for k in keys if isinstance(k, str) and k.startswith('dual_mlp.')]
    
    if dual_mlp_keys:
        # è¿‡æ»¤å‡ºdual_mlpçš„é”®å¹¶å»é™¤å‰ç¼€
        stripped = {k[len('dual_mlp.'):]: v for k, v in state_dict.items() if k.startswith('dual_mlp.')}
        classifier.dual_mlp.load_state_dict(stripped, strict=True)
        if logger is not None:
            logger.info("âœ“ åˆ†ç±»å¤´(dual_mlp)åŠ è½½æˆåŠŸ")
        return

    # å°è¯•ç›´æ¥åŠ è½½åˆ° dual_mlpï¼ˆæ— å‰ç¼€ï¼Œä¸”æ²¡æœ‰backboneé”®ï¼‰
    # å¦‚æœstate_dictåŒ…å«backboneé”®ï¼Œè¯´æ˜æ˜¯å®Œæ•´æ¨¡å‹ï¼Œä¸åº”è¯¥èµ°è¿™é‡Œ
    has_backbone_keys = any(isinstance(k, str) and k.startswith('backbone.') for k in keys)
    if has_backbone_keys:
        raise RuntimeError(
            f"æ¨¡å‹æ–‡ä»¶åŒ…å«backboneé”®ï¼Œä½†å®Œæ•´æ¨¡å‹åŠ è½½å¤±è´¥ã€‚"
            f"è¿™é€šå¸¸æ„å‘³ç€æ¨¡å‹æ¶æ„ä¸åŒ¹é…ï¼ˆå¯èƒ½æ˜¯æ—§æ¶æ„32-32-16-2ï¼‰ã€‚"
            f"è¯·ä½¿ç”¨æ–°æ¶æ„ï¼ˆ32-16-8-2ï¼‰é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚"
        )
    
    classifier.dual_mlp.load_state_dict(state_dict, strict=True)
    if logger is not None:
        logger.info("âœ“ åˆ†ç±»å¤´(dual_mlp)åŠ è½½æˆåŠŸ")

def test_model(classifier, X_test, y_test, config, logger, save_prefix="test"):
    """
    Test the model on test dataset
    
    Args:
        classifier: Trained MEDAL classifier
        X_test: (N, L, D) test sequences
        y_test: (N,) test labels
        config: configuration object
        logger: logger
        save_prefix: prefix for saved files
        
    Returns:
        metrics: dictionary of evaluation metrics
    """
    logger.info("="*70)
    logger.info("ğŸ” æ¨¡å‹æµ‹è¯• Model Testing")
    logger.info("="*70)
    logger.info(f"æµ‹è¯•æ ·æœ¬æ•°: {len(X_test)}")
    test_batch_size = int(getattr(config, 'TEST_BATCH_SIZE', 256))
    logger.info(f"æ‰¹æ¬¡å¤§å°: {test_batch_size}")
    
    # è®°å½•é…ç½®ä¸­çš„å‚è€ƒé˜ˆå€¼ï¼ˆç”¨äºå¯¹æ¯”ä¸å¯è§†åŒ–ï¼‰
    config_threshold = getattr(config, 'MALICIOUS_THRESHOLD', 0.5)
    logger.info(f"âœ“ é…ç½®å‚è€ƒé˜ˆå€¼: {config_threshold:.2f} (ä»…ç”¨äºå‚è€ƒä¸å¯è§†åŒ–)")
    logger.info("")
    
    classifier.eval()
    classifier.to(config.DEVICE)
    
    # Create DataLoader
    logger.info(f"ğŸ”§ RNGæŒ‡çº¹(æµ‹è¯•-Dataloaderåˆ›å»ºå‰): {_rng_fingerprint_short()} ({_seed_snapshot()})")
    dataset = TensorDataset(torch.FloatTensor(X_test), torch.LongTensor(y_test))
    test_loader = DataLoader(dataset, batch_size=test_batch_size, shuffle=False)
    logger.info(f"ğŸ”§ RNGæŒ‡çº¹(æµ‹è¯•-Dataloaderåˆ›å»ºå): {_rng_fingerprint_short()} ({_seed_snapshot()})")
    total_batches = len(test_loader)
    
    # Collect probabilities / labels / featuresï¼ˆå…ˆä¸ä½¿ç”¨é˜ˆå€¼ï¼‰
    all_probs = []
    all_labels = []
    all_features = []
    
    logger.info(f"ğŸ”§ RNGæŒ‡çº¹(æµ‹è¯•-æ¨ç†å¼€å§‹å‰): {_rng_fingerprint_short()} ({_seed_snapshot()})")
    logger.info("å¼€å§‹æ¨ç†...")
    logger.info("-"*70)
    
    with torch.no_grad():
        for batch_idx, (X_batch, y_batch) in enumerate(test_loader):
            X_batch = X_batch.to(config.DEVICE)
            y_batch = y_batch.to(config.DEVICE)
            
            # ç›´æ¥å‰å‘è·å¾— logits å’Œç‰¹å¾ï¼Œä¸åœ¨è¿™é‡Œåšé˜ˆå€¼åˆ¤å†³
            logits, z = classifier(X_batch, return_features=True, return_separate=False)
            
            # Apply temperature scaling if enabled (makes predictions "softer")
            temperature = getattr(config, 'TEMPERATURE_SCALING', 1.0)
            if temperature != 1.0:
                scaled_logits = logits / temperature
                probs = torch.softmax(scaled_logits, dim=1)
            else:
                probs = torch.softmax(logits, dim=1)
            
            all_probs.append(probs.cpu().numpy())
            all_labels.append(y_batch.cpu().numpy())
            all_features.append(z.cpu().numpy())
            
            # æ¯10ä¸ªæ‰¹æ¬¡æˆ–æœ€åä¸€ä¸ªæ‰¹æ¬¡è¾“å‡ºè¿›åº¦
            if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == total_batches:
                progress = (batch_idx + 1) / total_batches * 100
                processed = min((batch_idx + 1) * test_batch_size, len(X_test))
                logger.info(f"  æ¨ç†è¿›åº¦: {batch_idx+1}/{total_batches} batches ({progress:.1f}%) | å·²å¤„ç† {processed}/{len(X_test)} ä¸ªæ ·æœ¬")
    
    logger.info(f"ğŸ”§ RNGæŒ‡çº¹(æµ‹è¯•-æ¨ç†ç»“æŸå): {_rng_fingerprint_short()} ({_seed_snapshot()})")
    # Concatenate results
    y_prob = np.concatenate(all_probs)
    y_true = np.concatenate(all_labels)
    features = np.concatenate(all_features)
    
    logger.info("-"*70)
    logger.info("âœ“ æ¨ç†å®Œæˆ")
    logger.info("")
    
    # ğŸš€ åœ¨æµ‹è¯•é›†ä¸Šæœç´¢æœ€ä¼˜é˜ˆå€¼ï¼ˆåå¤„ç†æ­¥éª¤ï¼Œæ¨¡å‹å·²å›ºå®šï¼Œä¸æ˜¯æ•°æ®æ³„éœ²ï¼‰
    logger.info("ğŸ“Š åŸºäºæµ‹è¯•é›† Binary F1-Score(pos=1) æœç´¢æœ€ä¼˜é˜ˆå€¼...")
    logger.info("   è¯´æ˜: è¿™æ˜¯åå¤„ç†æ­¥éª¤ï¼Œæ¨¡å‹å‚æ•°å·²å›ºå®šï¼Œç”¨äºä¼˜åŒ–å†³ç­–é˜ˆå€¼")
    optimal_threshold, optimal_metric, _ = find_optimal_threshold(
        y_true, y_prob, metric='f1_binary', positive_class=1
    )
    logger.info(f"âœ… æµ‹è¯•é›†æœ€ä¼˜é˜ˆå€¼: {optimal_threshold:.4f} (Binary F1 pos=1 = {optimal_metric:.4f})")
    logger.info(f"   é…ç½®é»˜è®¤é˜ˆå€¼: {config_threshold:.4f}")
    
    # é˜ˆå€¼å¯¹æ¯”ï¼ˆè§£é‡Šä¸ºä»€ä¹ˆå›¾é‡Œå¯èƒ½æ˜¯ 0.8+ è€Œä¸æ˜¯ 0.6ï¼‰
    candidate_thresholds = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, float(config_threshold)]
    # æ³¨æ„ï¼šä¸å¯¹optimal_thresholdè¿›è¡Œå››èˆäº”å…¥ï¼Œä¿æŒåŸå§‹ç²¾åº¦
    candidate_thresholds_display = sorted(set([round(t, 4) for t in candidate_thresholds]))
    # å°†optimal_thresholdæ’å…¥åˆ°æ­£ç¡®çš„ä½ç½®ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
    optimal_threshold_rounded = round(float(optimal_threshold), 4)
    if optimal_threshold_rounded not in candidate_thresholds_display:
        candidate_thresholds_display.append(optimal_threshold_rounded)
        candidate_thresholds_display.sort()
    
    logger.info("ğŸ“ é˜ˆå€¼å¯¹æ¯” (Malicious=Positive):")
    logger.info(f"  {'threshold':>10s} | {'precision':>9s} | {'recall':>7s} | {'f1':>7s}")
    logger.info("  " + "-"*44)
    for th_display in candidate_thresholds_display:
        # å¯¹äºæœ€ä¼˜é˜ˆå€¼ï¼Œä½¿ç”¨åŸå§‹é«˜ç²¾åº¦å€¼ï¼›å…¶ä»–ä½¿ç”¨æ˜¾ç¤ºå€¼
        if abs(th_display - optimal_threshold_rounded) < 0.00001:
            th_actual = float(optimal_threshold)  # ä½¿ç”¨åŸå§‹é«˜ç²¾åº¦å€¼
        else:
            th_actual = th_display
        
        y_pred_th = (y_prob[:, 1] >= th_actual).astype(int)
        p = precision_score(y_true, y_pred_th, pos_label=1, zero_division=0)
        r = recall_score(y_true, y_pred_th, pos_label=1, zero_division=0)
        f1 = f1_score(y_true, y_pred_th, pos_label=1, zero_division=0)
        marker = " â† æœ€ä¼˜" if abs(th_display - optimal_threshold_rounded) < 0.00001 else ""
        logger.info(f"  {th_display:10.4f} | {p:9.4f} | {r:7.4f} | {f1:7.4f}{marker}")
    logger.info("")
    
    # ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼ç”Ÿæˆé¢„æµ‹æ ‡ç­¾ï¼ˆè¿™æ˜¯æœ€ç»ˆä½¿ç”¨çš„é˜ˆå€¼ï¼‰
    y_pred = (y_prob[:, 1] >= optimal_threshold).astype(int)
    logger.info(f"âœ… æœ€ç»ˆè¯„ä¼°ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼: {optimal_threshold:.4f}")
    logger.info(f"   (ä¸‹æ–¹æ€§èƒ½æŒ‡æ ‡å‡åŸºäºæ­¤é˜ˆå€¼è®¡ç®—)")
    logger.info("")
    
    # Calculate metrics at optimal threshold
    logger.info("ğŸ“Š è®¡ç®—æ€§èƒ½æŒ‡æ ‡ (åŸºäºè‡ªåŠ¨é˜ˆå€¼)...")
    metrics = calculate_metrics(y_true, y_pred, y_prob)
    
    # Print metricsï¼ˆåŒ…å«è®ºæ–‡å£å¾„ï¼šæ¶æ„ç±»ä¸ºæ­£ç±»çš„å•ç±»F1ï¼‰
    print_metrics(metrics, logger)
    
    # Per-class metrics
    logger.info("\nPer-Class Analysis:")
    for class_idx, class_name in enumerate(['Benign', 'Malicious']):
        class_mask = y_true == class_idx
        class_acc = (y_pred[class_mask] == y_true[class_mask]).mean()
        logger.info(f"  {class_name:10s}: {class_mask.sum():5d} samples, Accuracy: {class_acc*100:.2f}%")
    
    # Visualizations
    logger.info("\nGenerating visualizations...")
    
    # Feature space
    feature_space_path = os.path.join(config.RESULT_DIR, "figures", f"{save_prefix}_feature_space.png")
    plot_feature_space(features, y_true, feature_space_path, 
                      title=f"Test Set Feature Space", method='tsne')
    
    # Confusion matrix
    confusion_matrix_path = os.path.join(config.RESULT_DIR, "figures", f"{save_prefix}_confusion_matrix.png")
    plot_confusion_matrix(metrics['confusion_matrix'], ['Benign', 'Malicious'], 
                         confusion_matrix_path, title=f"Test Set Confusion Matrix")
    
    # ROC curve
    roc_curve_path = os.path.join(config.RESULT_DIR, "figures", f"{save_prefix}_roc_curve.png")
    plot_roc_curve(y_true, y_prob, roc_curve_path, title=f"Test Set ROC Curve")
    
    # Probability distribution (ä½¿ç”¨è‡ªåŠ¨æœç´¢å¾—åˆ°çš„æœ€ä¼˜é˜ˆå€¼)
    prob_dist_path = os.path.join(config.RESULT_DIR, "figures", f"{save_prefix}_probability_distribution.png")
    plot_probability_distribution(y_true, y_prob, prob_dist_path, 
                                  title=f"Test Set Probability Distribution", threshold=optimal_threshold)
    
    logger.info("")
    logger.info("ğŸ“ è¾“å‡ºæ–‡ä»¶è·¯å¾„:")
    logger.info(f"  âœ“ ç‰¹å¾ç©ºé—´å¯è§†åŒ–: {feature_space_path}")
    logger.info(f"  âœ“ æ··æ·†çŸ©é˜µ: {confusion_matrix_path}")
    logger.info(f"  âœ“ ROCæ›²çº¿: {roc_curve_path}")
    logger.info(f"  âœ“ æ¦‚ç‡åˆ†å¸ƒå›¾: {prob_dist_path}")
    
    # Save predictions to result directory
    results_file = os.path.join(config.RESULT_DIR, "models", f"{save_prefix}_predictions.npz")
    np.savez(results_file,
             y_true=y_true,
             y_pred=y_pred,
             y_prob=y_prob,
             features=features)
    # Save metrics to text file
    metrics_file = os.path.join(config.RESULT_DIR, "models", f"{save_prefix}_metrics.txt")
    with open(metrics_file, 'w') as f:
        f.write("="*70 + "\n")
        f.write(f"MEDAL-Lite Test Results - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("="*70 + "\n\n")
        f.write(f"Test Samples: {len(y_true)}\n")
        f.write(f"  Benign:    {(y_true==0).sum()}\n")
        f.write(f"  Malicious: {(y_true==1).sum()}\n\n")
        f.write("Performance Metrics (Malicious=Positive):\n")
        f.write(f"  Accuracy:          {metrics['accuracy']:.4f}\n")
        f.write(f"  Precision (pos=1): {metrics['precision_pos']:.4f}\n")
        f.write(f"  Recall    (pos=1): {metrics['recall_pos']:.4f}\n")
        f.write(f"  F1 (pos=1):        {metrics['f1_pos']:.4f}\n")
        f.write("  --- reference ---\n")
        f.write(f"  F1-Macro:          {metrics['f1_macro']:.4f}\n")
        f.write(f"  F1-Weighted:       {metrics['f1_weighted']:.4f}\n")
        if 'auc' in metrics:
            f.write(f"  AUC:               {metrics['auc']:.4f}\n")
        f.write("\nConfusion Matrix:\n")
        f.write(str(metrics['confusion_matrix']) + "\n")
    
    logger.info(f"  âœ“ é¢„æµ‹ç»“æœ: {results_file}")
    logger.info(f"  âœ“ æ€§èƒ½æŒ‡æ ‡: {metrics_file}")
    
    # æ·»åŠ æ ‡å‡†åŒ–çš„é”®åä»¥ä¾¿å¯¹æ¯”
    metrics['precision'] = metrics['precision_pos']
    metrics['recall'] = metrics['recall_pos']
    metrics['f1'] = metrics['f1_pos']
    return metrics


def _infer_family_from_filename(filename: str) -> str:
    name = str(filename).lower()
    if 'dnscat' in name or 'dnscat2' in name:
        return 'dnscat2'
    if 'iodine' in name:
        return 'iodine'
    if 'dohbrw' in name or 'doh' in name:
        return 'doh'
    return 'unknown'


def _export_family_breakdown(save_prefix: str, y_true: np.ndarray, y_pred: np.ndarray, y_prob: np.ndarray, test_files, config, logger):
    if test_files is None:
        return

    if len(test_files) != len(y_true):
        logger.warning(f"âš  test_files é•¿åº¦({len(test_files)})ä¸æ ·æœ¬æ•°({len(y_true)})ä¸ä¸€è‡´ï¼Œè·³è¿‡åˆ†ç»„è¯„ä¼°")
        return

    families = np.array([_infer_family_from_filename(f) for f in test_files], dtype=object)
    unique_families = sorted(set(families.tolist()))

    rows = []
    for fam in unique_families:
        idx = families == fam
        n = int(idx.sum())
        if n <= 0:
            continue
        y_t = y_true[idx]
        y_p = y_pred[idx]
        y_pb = y_prob[idx]

        p = precision_score(y_t, y_p, pos_label=1, zero_division=0)
        r = recall_score(y_t, y_p, pos_label=1, zero_division=0)
        f1 = f1_score(y_t, y_p, pos_label=1, zero_division=0)
        acc = float((y_t == y_p).mean()) if n > 0 else 0.0

        rows.append({
            'family': fam,
            'n_samples': n,
            'n_benign': int((y_t == 0).sum()),
            'n_malicious': int((y_t == 1).sum()),
            'accuracy': float(acc),
            'precision_pos': float(p),
            'recall_pos': float(r),
            'f1_pos': float(f1),
        })

    if not rows:
        return

    models_dir = os.path.join(config.RESULT_DIR, 'models')
    os.makedirs(models_dir, exist_ok=True)

    csv_path = os.path.join(models_dir, f'{save_prefix}_family_metrics.csv')
    with open(csv_path, 'w', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=list(rows[0].keys()))
        writer.writeheader()
        writer.writerows(rows)

    json_path = os.path.join(models_dir, f'{save_prefix}_family_report.json')
    payload = {
        'save_prefix': save_prefix,
        'generated_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'family_metrics': rows,
    }
    with open(json_path, 'w') as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)

    logger.info("")
    logger.info("ğŸ“ åˆ†ç»„è¯„ä¼°è¾“å‡º:")
    logger.info(f"  âœ“ CSV:  {csv_path}")
    logger.info(f"  âœ“ JSON: {json_path}")


def main(args):
    """Main testing function"""
    
    # Setup
    rng_fp_before_seed = _rng_fingerprint_short()
    set_seed(config.SEED)
    rng_fp_after_seed = _rng_fingerprint_short()
    config.create_dirs()
    logger = setup_logger(os.path.join(config.OUTPUT_ROOT, "logs"), name='test')

    logger.info(f"ğŸ”§ RNGæŒ‡çº¹(seedå‰): {rng_fp_before_seed}")
    logger.info(f"ğŸ”§ RNGæŒ‡çº¹(seedå): {rng_fp_after_seed} ({_seed_snapshot()})")
    
    logger.info("="*70)
    logger.info("ğŸ§ª MEDAL-Lite Testing Pipeline")
    logger.info("="*70)
    
    # GPUä¿¡æ¯
    if torch.cuda.is_available():
        logger.info(f"âœ“ GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
        logger.info(f"  ä½¿ç”¨è®¾å¤‡: {config.DEVICE}")
    else:
        logger.info(f"âš  ä½¿ç”¨CPUè¿›è¡Œæ¨ç†")
        logger.info(f"  ä½¿ç”¨è®¾å¤‡: {config.DEVICE}")
    
    logger.info(f"æ—¶é—´æˆ³: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("")
    
    # ========================
    # Load Test Dataset
    # ========================
    logger.info("="*70)
    logger.info("ğŸ“¦ åŠ è½½æµ‹è¯•æ•°æ®é›† Loading Test Dataset")
    logger.info("="*70)
    logger.info(f"æµ‹è¯•é›†é…ç½®:")
    logger.info(f"  æ­£å¸¸æµé‡è·¯å¾„: {config.BENIGN_TEST}")
    logger.info(f"  æ¶æ„æµé‡è·¯å¾„: {config.MALICIOUS_TEST}")
    logger.info(f"  è¯´æ˜: å°†è¯»å–ä¸Šè¿°è·¯å¾„ä¸‹æ‰€æœ‰pcapæ–‡ä»¶ï¼Œæµæ•°åœ¨å¤„ç†æ—¶ç»Ÿè®¡")
    logger.info("")
    
    logger.info(f"ğŸ”§ RNGæŒ‡çº¹(åŠ è½½æµ‹è¯•æ•°æ®å‰): {_rng_fingerprint_short()} ({_seed_snapshot()})")
    # ä¼˜å…ˆä½¿ç”¨é¢„å¤„ç†å¥½çš„æ•°æ®
    if PREPROCESS_AVAILABLE and check_preprocessed_exists('test'):
        logger.info("âœ“ å‘ç°é¢„å¤„ç†æ–‡ä»¶ï¼Œç›´æ¥åŠ è½½...")
        X_test, y_test, test_files = load_preprocessed('test')
        X_test = normalize_burstsize_inplace(X_test)
        logger.info(f"  ä»é¢„å¤„ç†æ–‡ä»¶åŠ è½½: {X_test.shape[0]} ä¸ªæ ·æœ¬")
    else:
        # ä»PCAPæ–‡ä»¶åŠ è½½
        logger.info("å¼€å§‹åŠ è½½æµ‹è¯•æ•°æ®é›†ï¼ˆä»PCAPæ–‡ä»¶ï¼‰...")
        logger.info("ğŸ’¡ æç¤º: è¿è¡Œ 'python preprocess.py --test_only' å¯é¢„å¤„ç†æµ‹è¯•é›†ï¼ŒåŠ é€Ÿåç»­æµ‹è¯•")
        X_test, y_test, test_files = load_dataset(
            benign_dir=config.BENIGN_TEST,
            malicious_dir=config.MALICIOUS_TEST,
            sequence_length=config.SEQUENCE_LENGTH
        )
        X_test = normalize_burstsize_inplace(X_test)

    logger.info(f"ğŸ”§ RNGæŒ‡çº¹(åŠ è½½æµ‹è¯•æ•°æ®å): {_rng_fingerprint_short()} ({_seed_snapshot()})")
    
    if X_test is None:
        logger.error("âŒ æµ‹è¯•æ•°æ®é›†åŠ è½½å¤±è´¥!")
        return
    
    logger.info("âœ“ æµ‹è¯•æ•°æ®é›†åŠ è½½å®Œæˆ")
    logger.info(f"  æ•°æ®å½¢çŠ¶: {X_test.shape} (æ ·æœ¬æ•°Ã—åºåˆ—é•¿åº¦Ã—ç‰¹å¾ç»´åº¦)")
    logger.info(f"  æ­£å¸¸æ ·æœ¬: {(y_test==0).sum()} ä¸ª")
    logger.info(f"  æ¶æ„æ ·æœ¬: {(y_test==1).sum()} ä¸ª")
    logger.info("")
    
    # ========================
    # Load Model
    # ========================
    logger.info("="*70)
    logger.info("ğŸ“¥ åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ Loading Trained Model")
    logger.info("="*70)
    logger.info("ğŸ“¥ è¾“å…¥æ•°æ®è·¯å¾„:")
    logger.info(f"  âœ“ æµ‹è¯•æ•°æ®: {config.BENIGN_TEST} (æ­£å¸¸), {config.MALICIOUS_TEST} (æ¶æ„)")
    logger.info("")
    
    # Try to load model metadata to get the backbone path used during training
    metadata_path = os.path.join(config.CLASSIFICATION_DIR, "models", "model_metadata.json")
    backbone_path_from_metadata = None
    
    if os.path.exists(metadata_path):
        try:
            import json
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)
            backbone_path_from_metadata = metadata.get('backbone_path')
            input_is_features_from_metadata = metadata.get('input_is_features', False)
            feature_dim_from_metadata = metadata.get('feature_dim', None)
            
            if backbone_path_from_metadata:
                logger.info(f"âœ“ ä»æ¨¡å‹å…ƒæ•°æ®ä¸­è¯»å–åˆ°è®­ç»ƒæ—¶ä½¿ç”¨çš„éª¨å¹²ç½‘ç»œ:")
                logger.info(f"  {backbone_path_from_metadata}")
                if input_is_features_from_metadata:
                    logger.info(f"âœ“ è®­ç»ƒæ—¶è¾“å…¥ç±»å‹: ç‰¹å¾å‘é‡ (ç»´åº¦={feature_dim_from_metadata})")
                    logger.info(f"  æµ‹è¯•æ—¶å°†è‡ªåŠ¨ä»åºåˆ—æå–ç‰¹å¾")
                logger.info("")
        except Exception as e:
            logger.warning(f"âš  æ— æ³•è¯»å–æ¨¡å‹å…ƒæ•°æ®: {e}")
    
    # ç¡®å®šéª¨å¹²ç½‘ç»œè·¯å¾„
    # ä¼˜å…ˆçº§ï¼š1. å‘½ä»¤è¡Œå‚æ•° 2. å…ƒæ•°æ® 3. é»˜è®¤è·¯å¾„
    backbone_path = None
    
    # 1. æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if hasattr(args, 'backbone_path') and args.backbone_path:
        backbone_path = args.backbone_path
        logger.info(f"âœ“ ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„éª¨å¹²ç½‘ç»œ:")
        logger.info(f"  {backbone_path}")
        logger.info("")
    # 2. å°è¯•ä»å…ƒæ•°æ®è¯»å–ï¼ˆä¼˜å…ˆä½¿ç”¨è®­ç»ƒæ—¶ä¿å­˜çš„è·¯å¾„ï¼‰
    elif backbone_path_from_metadata:
        if os.path.exists(backbone_path_from_metadata):
            backbone_path = backbone_path_from_metadata
            logger.info("âœ“ ä½¿ç”¨è®­ç»ƒæ—¶çš„éª¨å¹²ç½‘ç»œï¼ˆä»å…ƒæ•°æ®ï¼‰")
            logger.info(f"  {backbone_path}")
        else:
            # å¦‚æœå…ƒæ•°æ®ä¸­æŒ‡å®šçš„è·¯å¾„ä¸å­˜åœ¨ï¼Œè¿™æ˜¯ä¸¥é‡é—®é¢˜ï¼Œä¸åº”è¯¥å›é€€åˆ°æ—§æ¨¡å‹
            logger.error(f"âŒ ä¸¥é‡é”™è¯¯ï¼šè®­ç»ƒæ—¶ä½¿ç”¨çš„éª¨å¹²ç½‘ç»œä¸å­˜åœ¨!")
            logger.error(f"  å…ƒæ•°æ®ä¸­è®°å½•çš„è·¯å¾„: {backbone_path_from_metadata}")
            logger.error(f"  è¯¥æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•ä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•!")
            logger.error("")
            logger.error("å¯èƒ½çš„åŸå› :")
            logger.error("  1. æ¨¡å‹æ–‡ä»¶è¢«æ„å¤–åˆ é™¤")
            logger.error("  2. æ¨¡å‹æ–‡ä»¶è·¯å¾„å‘ç”Ÿäº†å˜åŒ–")
            logger.error("  3. ä½¿ç”¨äº†é”™è¯¯çš„è¾“å‡ºç›®å½•")
            logger.error("")
            logger.error("è§£å†³æ–¹æ¡ˆ:")
            logger.error("  1. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
            logger.error("  2. é‡æ–°è¿è¡Œè®­ç»ƒè„šæœ¬")
            logger.error("  3. æˆ–ä½¿ç”¨ --backbone_path å‚æ•°æ‰‹åŠ¨æŒ‡å®šæ­£ç¡®çš„æ¨¡å‹è·¯å¾„")
            return
    # 3. ä½¿ç”¨é»˜è®¤è·¯å¾„ï¼ˆä»…åœ¨å…ƒæ•°æ®ä¸å­˜åœ¨æˆ–æœªæŒ‡å®šbackbone_pathæ—¶ï¼‰
    else:
        backbone_path = os.path.join(config.FEATURE_EXTRACTION_DIR, "models", "backbone_pretrained.pth")
        logger.info(f"ä½¿ç”¨é»˜è®¤éª¨å¹²ç½‘ç»œè·¯å¾„: {backbone_path}")
        logger.warning("âš  æ³¨æ„ï¼šæœªæ‰¾åˆ°æ¨¡å‹å…ƒæ•°æ®ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„å¯èƒ½ä¸æ˜¯è®­ç»ƒæ—¶ä½¿ç”¨çš„æ¨¡å‹!")
        logger.warning("  å»ºè®®ï¼šç¡®ä¿è®­ç»ƒè„šæœ¬å·²æ­£ç¡®ä¿å­˜æ¨¡å‹å…ƒæ•°æ®")
    
    if not os.path.exists(backbone_path):
        logger.error(f"âŒ éª¨å¹²ç½‘ç»œæ£€æŸ¥ç‚¹æœªæ‰¾åˆ°: {backbone_path}")
        logger.error("è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬!")
        return
    
    logger.info("æ­£åœ¨åŠ è½½éª¨å¹²ç½‘ç»œ...")
    logger.info(f"  ğŸ“¥ è¾“å…¥æ¨¡å‹: {backbone_path}")
    logger.info(f"ğŸ”§ RNGæŒ‡çº¹(åŠ è½½backboneæƒé‡å‰): {_rng_fingerprint_short()} ({_seed_snapshot()})")
    
    # ä½¿ç”¨å®‰å…¨çš„æ¨¡å‹åŠ è½½å‡½æ•°ï¼ˆè‡ªåŠ¨å¤„ç†å…¼å®¹æ€§ï¼‰
    from MoudleCode.utils.model_loader import load_backbone_safely
    backbone = load_backbone_safely(
        backbone_path=backbone_path,
        config=config,
        device=config.DEVICE,
        logger=logger
    )
    logger.info(f"ğŸ”§ RNGæŒ‡çº¹(åŠ è½½backboneæƒé‡å): {_rng_fingerprint_short()} ({_seed_snapshot()})")
    backbone.freeze()
    logger.info(f"âœ“ éª¨å¹²ç½‘ç»œåŠ è½½å®Œæˆ")
    
    # ========================
    # Load classifiers (both best and final)
    # ========================
    best_path = os.path.join(config.CLASSIFICATION_DIR, "models", "classifier_best_f1.pth")
    final_path = os.path.join(config.CLASSIFICATION_DIR, "models", "classifier_final.pth")
    minloss_candidates = []
    try:
        models_dir = os.path.join(config.CLASSIFICATION_DIR, "models")
        if os.path.isdir(models_dir):
            for fn in os.listdir(models_dir):
                if isinstance(fn, str) and fn.startswith("classifier_last10_minloss_epoch") and fn.endswith(".pth"):
                    full_path = os.path.join(models_dir, fn)
                    minloss_candidates.append(full_path)
    except Exception:
        minloss_candidates = []
    
    # æ£€æŸ¥å“ªäº›æ¨¡å‹å­˜åœ¨
    has_best = os.path.exists(best_path)
    has_final = os.path.exists(final_path)
    # æŒ‰æ–‡ä»¶ä¿®æ”¹æ—¶é—´é€‰æ‹©æœ€æ–°çš„minlossæ¨¡å‹ï¼ˆè€Œä¸æ˜¯æŒ‰å­—ç¬¦ä¸²æ’åºï¼‰
    if len(minloss_candidates) > 0:
        minloss_path = max(minloss_candidates, key=lambda p: os.path.getmtime(p))
    else:
        minloss_path = None
    has_minloss = bool(minloss_path) and os.path.exists(minloss_path)
    
    if not has_best and not has_final and not has_minloss:
        logger.error(f"âŒ æœªæ‰¾åˆ°ä»»ä½•åˆ†ç±»å™¨æ£€æŸ¥ç‚¹!")
        logger.error(f"  Bestæ¨¡å‹: {best_path}")
        logger.error(f"  Finalæ¨¡å‹: {final_path}")
        if minloss_path:
            logger.error(f"  Last10-MinLossæ¨¡å‹: {minloss_path}")
        logger.error("è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬!")
        return
    
    # å¦‚æœå‘½ä»¤è¡ŒæŒ‡å®šäº†åˆ†ç±»å™¨è·¯å¾„ï¼Œåªæµ‹è¯•æŒ‡å®šçš„æ¨¡å‹
    if hasattr(args, 'classifier_path') and args.classifier_path:
        logger.info("æ­£åœ¨åŠ è½½æŒ‡å®šçš„åˆ†ç±»å™¨...")
        logger.info(f"  ğŸ“¥ è¾“å…¥æ¨¡å‹: {args.classifier_path}")
        
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(æ„å»ºclassifierå‰): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        classifier = MEDAL_Classifier(backbone, config)
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(æ„å»ºclassifierå): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        try:
            classifier_state = torch.load(args.classifier_path, map_location=config.DEVICE, weights_only=True)
        except TypeError:
            classifier_state = torch.load(args.classifier_path, map_location=config.DEVICE)
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(åŠ è½½classifieræƒé‡å‰): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        _load_classifier_checkpoint(classifier, classifier_state, logger=logger)
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(åŠ è½½classifieræƒé‡å): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        logger.info(f"âœ“ åˆ†ç±»å™¨åŠ è½½å®Œæˆ")
        
        # Count parameters
        n_params = sum(p.numel() for p in classifier.parameters())
        n_trainable = sum(p.numel() for p in classifier.parameters() if p.requires_grad)
        logger.info("")
        logger.info("ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
        logger.info(f"  æ€»å‚æ•°é‡: {n_params:,}")
        logger.info(f"  å¯è®­ç»ƒå‚æ•°: {n_trainable:,} (éª¨å¹²ç½‘ç»œå·²å†»ç»“)")
        logger.info("")
        
        # Test single model
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(è¿›å…¥test_modelå‰): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        metrics = test_model(classifier, X_test, y_test, config, logger)
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(test_modelè¿”å›å): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        try:
            results_path = os.path.join(config.RESULT_DIR, 'models', 'test_predictions.npz')
            npz = np.load(results_path, allow_pickle=True)
            y_true = npz['y_true']
            y_pred = npz['y_pred']
            y_prob = npz['y_prob']
            _export_family_breakdown('test', y_true, y_pred, y_prob, test_files, config, logger)
        except Exception as e:
            logger.warning(f"âš  åˆ†ç»„è¯„ä¼°å¯¼å‡ºå¤±è´¥: {e}")
        
        logger.info("")
        logger.info("="*70)
        logger.info("ğŸ‰ æµ‹è¯•å®Œæˆ! Testing Complete!")
        logger.info("="*70)
        logger.info("")
        logger.info("ğŸ“ è¾“å‡ºæ–‡ä»¶è·¯å¾„:")
        logger.info(f"  âœ“ é¢„æµ‹ç»“æœ: {os.path.join(config.RESULT_DIR, 'models', 'test_predictions.npz')}")
        logger.info(f"  âœ“ æ€§èƒ½æŒ‡æ ‡: {os.path.join(config.RESULT_DIR, 'models', 'test_metrics.txt')}")
        logger.info(f"  âœ“ å¯è§†åŒ–å›¾è¡¨: {os.path.join(config.RESULT_DIR, 'figures')}")
        logger.info("")
        logger.info("="*70)
        
        return metrics
    
    # å¦åˆ™ï¼Œæµ‹è¯•æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹å¹¶å¯¹æ¯”
    logger.info("")
    logger.info("="*70)
    logger.info("ğŸ”¬ å¯¹æ¯”æµ‹è¯•ï¼šBest F1 vs Final æ¨¡å‹")
    logger.info("="*70)
    logger.info(f"  Best F1æ¨¡å‹: {'âœ“ å­˜åœ¨' if has_best else 'âœ— ä¸å­˜åœ¨'}")
    logger.info(f"  Finalæ¨¡å‹:   {'âœ“ å­˜åœ¨' if has_final else 'âœ— ä¸å­˜åœ¨'}")
    logger.info(f"  Last10-MinLossæ¨¡å‹: {'âœ“ å­˜åœ¨' if has_minloss else 'âœ— ä¸å­˜åœ¨'}")
    logger.info("")
    
    models_to_test = []
    if has_best:
        models_to_test.append(("Best F1", best_path))
    if has_final:
        models_to_test.append(("Final", final_path))
    if has_minloss:
        models_to_test.append(("Last10-MinLoss", minloss_path))
    
    all_metrics = {}
    
    for model_name, model_path in models_to_test:
        logger.info("="*70)
        logger.info(f"ğŸ“Š æµ‹è¯•æ¨¡å‹: {model_name}")
        logger.info("="*70)
        logger.info(f"æ­£åœ¨åŠ è½½åˆ†ç±»å™¨...")
        logger.info(f"  ğŸ“¥ è¾“å…¥æ¨¡å‹: {model_path}")
        
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(æ„å»ºclassifierå‰): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        classifier = MEDAL_Classifier(backbone, config)
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(æ„å»ºclassifierå): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        try:
            classifier_state = torch.load(model_path, map_location=config.DEVICE, weights_only=True)
        except TypeError:
            classifier_state = torch.load(model_path, map_location=config.DEVICE)
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(åŠ è½½classifieræƒé‡å‰): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        _load_classifier_checkpoint(classifier, classifier_state, logger=logger)
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(åŠ è½½classifieræƒé‡å): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        logger.info(f"âœ“ åˆ†ç±»å™¨åŠ è½½å®Œæˆ")
        
        if model_name == "Best F1":
            # Count parameters only once
            n_params = sum(p.numel() for p in classifier.parameters())
            n_trainable = sum(p.numel() for p in classifier.parameters() if p.requires_grad)
            logger.info("")
            logger.info("ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
            logger.info(f"  æ€»å‚æ•°é‡: {n_params:,}")
            logger.info(f"  å¯è®­ç»ƒå‚æ•°: {n_trainable:,} (éª¨å¹²ç½‘ç»œå·²å†»ç»“)")
            logger.info("")
        
        # Test model with specific save prefix
        if model_name == "Best F1":
            save_prefix = "test_best"
        elif model_name == "Final":
            save_prefix = "test_final"
        else:
            save_prefix = "test_last10_minloss"
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(è¿›å…¥test_modelå‰): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        metrics = test_model(classifier, X_test, y_test, config, logger, save_prefix=save_prefix)
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(test_modelè¿”å›å): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        try:
            results_path = os.path.join(config.RESULT_DIR, 'models', f'{save_prefix}_predictions.npz')
            npz = np.load(results_path, allow_pickle=True)
            y_true = npz['y_true']
            y_pred = npz['y_pred']
            y_prob = npz['y_prob']
            _export_family_breakdown(save_prefix, y_true, y_pred, y_prob, test_files, config, logger)
        except Exception as e:
            logger.warning(f"âš  åˆ†ç»„è¯„ä¼°å¯¼å‡ºå¤±è´¥: {e}")
        all_metrics[model_name] = metrics
        
        logger.info("")
    
    # ========================
    # Compare Results
    # ========================
    if len(all_metrics) > 1:
        logger.info("="*70)
        logger.info("ğŸ“Š æ¨¡å‹å¯¹æ¯”ç»“æœ")
        logger.info("="*70)
        logger.info("")
        
        # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼ï¼ˆæ”¯æŒä¸‰æ¨¡å‹ï¼‰
        header = f"{'Model':<16} | {'F1(pos=1)':>9} | {'Prec':>9} | {'Recall':>9} | {'AUC':>9}"
        logger.info(header)
        logger.info("-" * len(header))

        def _get(m: dict, k: str, fallback: float = float('nan')) -> float:
            try:
                return float(m.get(k, fallback))
            except Exception:
                return fallback

        def _score(m: dict) -> float:
            if not isinstance(m, dict):
                return float('-inf')
            if 'f1' in m:
                return float(m['f1'])
            if 'f1_pos' in m:
                return float(m['f1_pos'])
            return float('-inf')

        ranked = sorted(all_metrics.items(), key=lambda kv: _score(kv[1]), reverse=True)
        for name, m in ranked:
            f1v = _get(m, 'f1', _get(m, 'f1_pos'))
            pv = _get(m, 'precision', _get(m, 'precision_pos'))
            rv = _get(m, 'recall', _get(m, 'recall_pos'))
            aucv = _get(m, 'auc')
            logger.info(f"{name:<16} | {f1v:9.4f} | {pv:9.4f} | {rv:9.4f} | {aucv:9.4f}")

        logger.info("")

    chosen_name = None
    chosen_metrics = None
    try:
        def _score(m: dict) -> float:
            if not isinstance(m, dict):
                return float('-inf')
            if 'f1' in m:
                return float(m['f1'])
            if 'f1_pos' in m:
                return float(m['f1_pos'])
            return float('-inf')

        chosen_name = max(all_metrics.keys(), key=lambda k: _score(all_metrics.get(k)))
        chosen_metrics = all_metrics.get(chosen_name)
    except Exception as e:
        logger.warning(f"âš  æ— æ³•æ ¹æ®F1é€‰æ‹©æœ€ç»ˆæ¨¡å‹: {e}")

    if chosen_name is not None and chosen_metrics is not None:
        logger.info("="*70)
        logger.info("ğŸ æœ€ç»ˆæµ‹è¯•ç»“æœé€‰æ‹©")
        logger.info("="*70)
        logger.info(f"é€‰æ‹©æ¨¡å‹: {chosen_name}")
        logger.info(f"ä¾æ®æŒ‡æ ‡: F1(pos=1) = {float(chosen_metrics.get('f1', chosen_metrics.get('f1_pos', -1.0))):.4f}")
        logger.info("")
    
    logger.info("="*70)
    logger.info("ğŸ‰ æµ‹è¯•å®Œæˆ! Testing Complete!")
    logger.info("="*70)
    logger.info("")
    logger.info("ğŸ“ è¾“å‡ºæ–‡ä»¶è·¯å¾„:")
    if has_best:
        logger.info(f"  âœ“ Best F1é¢„æµ‹ç»“æœ: {os.path.join(config.RESULT_DIR, 'models', 'test_best_predictions.npz')}")
        logger.info(f"  âœ“ Best F1æ€§èƒ½æŒ‡æ ‡: {os.path.join(config.RESULT_DIR, 'models', 'test_best_metrics.txt')}")
        logger.info(f"  âœ“ Best F1å¯è§†åŒ–: {os.path.join(config.RESULT_DIR, 'figures', 'test_best_*.png')}")
    if has_final:
        logger.info(f"  âœ“ Finalé¢„æµ‹ç»“æœ: {os.path.join(config.RESULT_DIR, 'models', 'test_final_predictions.npz')}")
        logger.info(f"  âœ“ Finalæ€§èƒ½æŒ‡æ ‡: {os.path.join(config.RESULT_DIR, 'models', 'test_final_metrics.txt')}")
        logger.info(f"  âœ“ Finalå¯è§†åŒ–: {os.path.join(config.RESULT_DIR, 'figures', 'test_final_*.png')}")
    logger.info("")
    logger.info("="*70)
    
    if chosen_metrics is not None:
        return chosen_metrics
    return metrics


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Test MEDAL-Lite model")
    parser.add_argument('--backbone_path', type=str, default='', help='Path to backbone checkpoint (optional)')
    parser.add_argument('--classifier_path', type=str, default='', help='Path to classifier checkpoint (optional)')
    
    args = parser.parse_args()
    
    main(args)

