"""
Label Correction Analysis Script
Only runs label correction part and generates detailed analysis documents and feature distribution plots.

This script:
1. Loads dataset and injects label noise
2. Extracts features using pre-trained backbone
3. Runs Hybrid Court label correction (CL + AUM + KNN)
4. Generates detailed per-sample analysis document
5. Creates feature distribution plots for clean, noisy, and corrected data
"""
import sys
import os
from pathlib import Path
# Ensure project root is on sys.path so `MoudleCode.*` imports work when invoked directly
PROJECT_ROOT = Path(__file__).resolve().parents[3]
sys.path.append(str(PROJECT_ROOT))

import torch
import numpy as np
import pandas as pd
import argparse
from datetime import datetime
import json
import matplotlib.pyplot as plt
import matplotlib as mpl
from matplotlib import font_manager
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA

from MoudleCode.utils.config import config
from MoudleCode.utils.helpers import (
    set_seed, setup_logger, inject_label_noise
)
from MoudleCode.preprocessing.pcap_parser import load_dataset
from MoudleCode.feature_extraction.backbone import MicroBiMambaBackbone, build_backbone
from MoudleCode.label_correction.hybrid_court import HybridCourt

import logging

# å°è¯•å¯¼å…¥Excelå†™å…¥åº“
try:
    import openpyxl
    from openpyxl.styles import Font, PatternFill, Alignment
    EXCEL_AVAILABLE = True
except ImportError:
    EXCEL_AVAILABLE = False
    print("è­¦å‘Š: openpyxlæœªå®‰è£…ï¼Œå°†åªç”ŸæˆCSVæ–‡ä»¶ã€‚å®‰è£…å‘½ä»¤: pip install openpyxl")

# ç®€åŒ–çš„é¢„å¤„ç†æ–‡ä»¶æ£€æµ‹å’ŒåŠ è½½
def check_preprocessed_exists(split='train'):
    """æ£€æŸ¥é¢„å¤„ç†æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    preprocessed_dir = Path(config.OUTPUT_ROOT) / 'preprocessed'
    if not preprocessed_dir.exists():
        return False
    
    X_file = preprocessed_dir / f'{split}_X.npy'
    y_file = preprocessed_dir / f'{split}_y.npy'
    files_file = preprocessed_dir / f'{split}_files.npy'
    
    return X_file.exists() and y_file.exists() and files_file.exists()

def load_preprocessed(split='train'):
    """åŠ è½½é¢„å¤„ç†æ–‡ä»¶"""
    preprocessed_dir = Path(config.OUTPUT_ROOT) / 'preprocessed'
    
    X = np.load(preprocessed_dir / f'{split}_X.npy')
    y = np.load(preprocessed_dir / f'{split}_y.npy')
    files = np.load(preprocessed_dir / f'{split}_files.npy', allow_pickle=True)
    
    return X, y, files

logger = None  # Will be initialized in main()


def _configure_matplotlib_chinese_fonts():
    """Configure matplotlib to use Chinese fonts and suppress warnings if unavailable."""
    import warnings
    # Suppress font warnings globally if no Chinese font is available
    warnings.filterwarnings('ignore', category=UserWarning, 
                          message='.*Glyph.*missing from font.*')
    
    try:
        preferred = [
            'Noto Sans CJK SC',
            'Noto Sans CJK TC',
            'Noto Sans CJK JP',
            'Noto Sans SC',
            'Noto Sans TC',
            'Source Han Sans SC',
            'Source Han Sans CN',
            'WenQuanYi Zen Hei',
            'WenQuanYi Micro Hei',
            'Microsoft YaHei',
            'SimHei'
        ]
        available = {f.name for f in font_manager.fontManager.ttflist}
        chosen = None
        for name in preferred:
            if name in available:
                chosen = name
                break

        if chosen is not None:
            current = mpl.rcParams.get('font.sans-serif', [])
            if not isinstance(current, list):
                current = [current]
            mpl.rcParams['font.family'] = 'sans-serif'
            mpl.rcParams['font.sans-serif'] = [chosen] + [f for f in current if f != chosen]

        mpl.rcParams['axes.unicode_minus'] = False
    except Exception:
        # If font configuration fails, warnings are already suppressed above
        pass


_configure_matplotlib_chinese_fonts()


def extract_features_with_backbone(backbone, X_data, config, logger):
    """
    Extract features using pre-trained backbone
    
    Args:
        backbone: Pre-trained MicroBiMambaBackbone
        X_data: (N, L, D) input sequences
        config: configuration object
        logger: logger
        
    Returns:
        features: (N, d) extracted features
    """
    logger.info("Extracting features using backbone...")
    
    backbone.freeze()
    backbone.eval()
    backbone.to(config.DEVICE)
    
    features_list = []
    batch_size = 64
    X_tensor = torch.FloatTensor(X_data).to(config.DEVICE)
    
    total_batches = (len(X_tensor) + batch_size - 1) // batch_size
    
    with torch.no_grad():
        for i in range(0, len(X_tensor), batch_size):
            batch_idx = i // batch_size + 1
            X_batch = X_tensor[i:i+batch_size]
            z_batch = backbone(X_batch, return_sequence=False)
            features_list.append(z_batch.cpu().numpy())
            
            if batch_idx % 10 == 0 or batch_idx == total_batches:
                progress = batch_idx / total_batches * 100
                logger.info(f"  Feature extraction progress: {batch_idx}/{total_batches} batches ({progress:.1f}%)")
    
    features = np.concatenate(features_list, axis=0)
    logger.info(f"âœ“ Feature extraction complete: {features.shape}")
    
    return features


def run_hybrid_court_with_detailed_tracking(features, noisy_labels, config, logger, y_true=None):
    """
    Run Hybrid Court (CL+AUM+KNN Two-Phase, No-Drop) with detailed tracking
    
    Args:
        features: ç‰¹å¾çŸ©é˜µ
        noisy_labels: å™ªå£°æ ‡ç­¾
        config: é…ç½®å¯¹è±¡
        logger: æ—¥å¿—å™¨
        y_true: çœŸå®æ ‡ç­¾ (å¯é€‰ï¼Œç”¨äºè¯¦ç»†ç»Ÿè®¡)
    
    Returns:
        results: dict containing all intermediate and final results
    """
    logger.info("")
    logger.info("="*70)
    logger.info("Running Hybrid Court (CL+AUM+KNN)")
    logger.info("="*70)
    
    hybrid_court = HybridCourt(config)
    n_samples = len(noisy_labels)

    clean_labels, action_mask, confidence, correction_weight, aum_scores, neighbor_consistency, pred_probs = hybrid_court.correct_labels(
        features=features,
        noisy_labels=noisy_labels,
        device=str(config.DEVICE),
        y_true=y_true,
    )
    
    # ä»å„å­æ¨¡å—ä¸­è¯»å–ç¼“å­˜çš„ä¸­é—´ç»“æœ
    suspected_noise = getattr(hybrid_court.cl, "last_suspected_noise", None)
    pred_labels = getattr(hybrid_court.cl, "last_pred_labels", None)
    neighbor_labels = getattr(hybrid_court.knn, "last_neighbor_labels", None)
    action_names = ['Keep', 'Flip', 'Drop', 'Reweight']
    tier_info = [action_names[int(a)] if int(a) < len(action_names) else '' for a in np.asarray(action_mask).tolist()]
    
    # è·å–è¿­ä»£CLå’Œé”šç‚¹KNNç»“æœ
    iter_pred_probs = getattr(hybrid_court, "iter_pred_probs_all", None)
    anchor_votes = getattr(hybrid_court, "anchor_votes_all", None)
    anchor_consistency = getattr(hybrid_court, "anchor_consistency_all", None)

    # Phase1 çŸ«æ­£åé‡ç®—çš„ Stage2 æŒ‡æ ‡ï¼ˆä¾›äºŒé˜¶æ®µç­–ç•¥ä½¿ç”¨ï¼‰
    stage2_aum_scores = getattr(hybrid_court, "stage2_aum_scores_all", None)
    stage2_knn_neighbor_labels = getattr(hybrid_court, "stage2_knn_neighbor_labels_all", None)
    stage2_knn_neighbor_consistency = getattr(hybrid_court, "stage2_knn_neighbor_consistency_all", None)
    stage2_cl_suspected_noise = getattr(hybrid_court, "stage2_cl_suspected_noise_all", None)
    stage2_cl_pred_labels = getattr(hybrid_court, "stage2_cl_pred_labels_all", None)
    stage2_cl_pred_probs = getattr(hybrid_court, "stage2_cl_pred_probs_all", None)
    phase2_actions = getattr(hybrid_court, "phase2_action_all", None)
    
    # ç”Ÿæˆå†³ç­–åŸå› 
    decision_reasons = []
    for i in range(n_samples):
        tier = tier_info[i] if i < len(tier_info) else ''
        
        # è·å–è¿­ä»£CLå’Œé”šç‚¹KNNä¿¡æ¯
        # iter_pred_probs å¯¹åº” â€œPhase1 çŸ«æ­£åæ ‡ç­¾â€ çš„ CL æ¦‚ç‡ï¼Œå› æ­¤å½“å‰æ ‡ç­¾ç”¨ clean_labels
        iter_cl_current = float(iter_pred_probs[i, int(clean_labels[i])]) if iter_pred_probs is not None else None
        iter_cl_target = float(iter_pred_probs[i, 1 - int(clean_labels[i])]) if iter_pred_probs is not None else None
        anchor_vote = int(anchor_votes[i]) if anchor_votes is not None else None
        anchor_cons = float(anchor_consistency[i]) if anchor_consistency is not None else None
        
        reason = _generate_decision_reason(
            tier,
            int(noisy_labels[i]),
            int(clean_labels[i]),
            int(action_mask[i]),
            bool(suspected_noise[i]) if suspected_noise is not None else False,
            int(neighbor_labels[i]) if neighbor_labels is not None else -1,
            float(neighbor_consistency[i]),
            float(pred_probs[i, 0]),
            float(pred_probs[i, 1]),
            float(aum_scores[i]) if aum_scores is not None else 0.0,
            iter_cl_current,
            iter_cl_target,
            anchor_vote,
            anchor_cons
        )
        decision_reasons.append(reason)
    
    # Compile results
    results = {
        'features': features,
        'noisy_labels': noisy_labels,
        'clean_labels': clean_labels,
        'action_mask': action_mask,
        'confidence': confidence,
        'correction_weight': correction_weight,
        # CL results
        'cl_suspected_noise': suspected_noise,
        'cl_pred_labels': pred_labels,
        'cl_pred_probs': pred_probs,
        'aum_scores': aum_scores,
        # KNN results
        'knn_neighbor_labels': neighbor_labels,
        'knn_neighbor_consistency': neighbor_consistency,
        # Iterative CL results
        'iter_cl_probs': iter_pred_probs,
        # Anchor KNN results
        'anchor_votes': anchor_votes,
        'anchor_consistency': anchor_consistency,
        # Stage2 metrics (recomputed on phase1 corrected labels)
        'stage2_aum_scores': stage2_aum_scores,
        'stage2_knn_neighbor_labels': stage2_knn_neighbor_labels,
        'stage2_knn_neighbor_consistency': stage2_knn_neighbor_consistency,
        'stage2_cl_suspected_noise': stage2_cl_suspected_noise,
        'stage2_cl_pred_labels': stage2_cl_pred_labels,
        'stage2_cl_pred_probs': stage2_cl_pred_probs,
        'phase2_actions': phase2_actions,
        # Decision tracking
        'decision_reasons': decision_reasons,
        'tier_info': tier_info
    }
    
    # è¯¦ç»†çš„Tierç»Ÿè®¡
    _log_tier_statistics(results, y_true, logger)
    
    return results


def _generate_decision_reason(tier, noisy_label, clean_label, action, is_suspected,
                               knn_label, knn_cons, cl_prob_0, cl_prob_1, aum,
                               iter_cl_current=None, iter_cl_target=None,
                               anchor_vote=None, anchor_cons=None):
    """ç”Ÿæˆè¯¦ç»†çš„å†³ç­–åŸå› è¯´æ˜"""
    current_label_name = 'æ­£å¸¸' if noisy_label == 0 else 'æ¶æ„'
    target_label_name = 'æ¶æ„' if noisy_label == 0 else 'æ­£å¸¸'
    new_label_name = 'æ­£å¸¸' if clean_label == 0 else 'æ¶æ„'
    
    # åŸå§‹CLç½®ä¿¡åº¦
    orig_cl_current = cl_prob_0 if noisy_label == 0 else cl_prob_1
    orig_cl_target = cl_prob_1 if noisy_label == 0 else cl_prob_0
    
    # KNNæŠ•ç¥¨ç»“æœ
    knn_vote_name = 'æ­£å¸¸' if knn_label == 0 else 'æ¶æ„'
    knn_support = 'æ”¯æŒ' if knn_label == noisy_label else 'åå¯¹'
    
    if action == 1:
        cl_diff = orig_cl_target - orig_cl_current
        knn_vote_name = 'æ­£å¸¸' if knn_label == 0 else 'æ¶æ„'
        return (
            f"[Flip] CLå·®å€¼={cl_diff:.3f} | AUM={aum:.3f} | "
            f"KNNæŠ•ç¥¨={knn_vote_name}(cons={knn_cons:.3f}) â†’ ç¿»è½¬{current_label_name}â†’{new_label_name}"
        )
    if action == 2:
        return (
            f"[Drop] CL={orig_cl_current:.3f} | AUM={aum:.3f} | "
            f"KNN(cons={knn_cons:.3f}) â†’ ä¸¢å¼ƒ{current_label_name}"
        )
    if action == 3:
        return (
            f"[Reweight] CL={orig_cl_current:.3f} | AUM={aum:.3f} | "
            f"KNN(cons={knn_cons:.3f}) â†’ é‡åŠ æƒ{current_label_name}"
        )
    return (
        f"[Keep] CL={orig_cl_current:.3f} | AUM={aum:.3f} | "
        f"KNN(cons={knn_cons:.3f}) â†’ ä¿æŒ{current_label_name}"
    )
    
    # Tier 4: Reweight - ä¸ç¡®å®šæ ·æœ¬
    if 'Tier 4' in tier or 'Reweight' in tier:
        reasons = []
        if orig_cl_current < 0.55:
            reasons.append(f"åŸCLä½({orig_cl_current:.3f}<0.55)")
        if knn_cons < 0.55:
            reasons.append(f"KNNå¼±({knn_cons:.3f}<0.55)")
        if not is_suspected and knn_label == noisy_label:
            reasons.append("CLä¸æ€€ç–‘+KNNæ”¯æŒ")
        
        if not reasons:
            reasons.append("æœªè¾¾Core/Flip/Keepæ ‡å‡†")
        
        return f"[Phase2-Reweight] {' + '.join(reasons)} â†’ é™æƒä¿æŒ{current_label_name}(w=0.5)"
    
    # Tier 5: Rescued - Phase 3æ‹¯æ•‘
    if 'Tier 5' in tier or 'Rescued' in tier:
        if anchor_cons is not None and anchor_vote is not None:
            anchor_vote_name = 'æ­£å¸¸' if anchor_vote == 0 else 'æ¶æ„'
            if 'Keep' in tier or '5a' in tier:
                return (f"[Phase3-Rescued-Keep] iter_T={iter_cl_target:.3f}â‰¥0.60 + "
                       f"anchorå¼ºæ”¯æŒå½“å‰({anchor_vote_name},{anchor_cons:.3f}â‰¥0.60) â†’ "
                       f"æ‹¯æ•‘ä¿æŒ{current_label_name}(w=0.85)")
            else:
                return (f"[Phase3-Rescued-Flip] iter_T={iter_cl_target:.3f}â‰¥0.60 + "
                       f"anchorå¼ºåå¯¹å½“å‰({anchor_vote_name},{anchor_cons:.3f}â‰¥0.60) â†’ "
                       f"æ‹¯æ•‘ç¿»è½¬{current_label_name}â†’{new_label_name}(w=0.75)")
        else:
            return f"[Phase3-Rescued] é”šç‚¹æ‹¯æ•‘ â†’ {new_label_name}"
    
    # Dropped - ä¸¢å¼ƒ
    if 'Dropped' in tier:
        drop_reasons = []
        if iter_cl_current is not None:
            if iter_cl_current < 0.48:
                drop_reasons.append(f"iter_CLæä½({iter_cl_current:.3f}<0.48)")
            elif iter_cl_current < 0.55 and anchor_cons is not None and anchor_cons < 0.55:
                drop_reasons.append(f"iter_CLä½({iter_cl_current:.3f}<0.55) + anchorä½({anchor_cons:.3f}<0.55)")
        
        if not drop_reasons:
            drop_reasons.append("è´¨é‡è¿‡ä½")
        
        return f"[Dropped] {' æˆ– '.join(drop_reasons)} â†’ ä¸¢å¼ƒ(w=0.0)"
    
    # æœªçŸ¥Tier
    else:
        action_names = ['ä¿æŒ', 'ç¿»è½¬', 'ä¸¢å¼ƒ', 'é‡åŠ æƒ']
        action_name = action_names[action] if action < len(action_names) else 'æœªçŸ¥'
        return f"[æœªåˆ†ç±»] tier={tier}, action={action_name}, åŸCL={orig_cl_current:.3f}, KNN={knn_cons:.3f}"


def _log_tier_statistics(results, y_true, logger):
    """è¾“å‡ºè¯¦ç»†çš„Tierç»Ÿè®¡"""
    tier_info = results.get('tier_info', [])
    clean_labels = results['clean_labels']
    correction_weight = results['correction_weight']
    n_samples = len(clean_labels)
    
    # ç»Ÿè®¡å„Tier
    tier_counts = {}
    tier_correct = {}
    tier_weights = {}
    
    for i, tier in enumerate(tier_info):
        if tier not in tier_counts:
            tier_counts[tier] = 0
            tier_correct[tier] = 0
            tier_weights[tier] = correction_weight[i]
        tier_counts[tier] += 1
        if y_true is not None and clean_labels[i] == y_true[i]:
            tier_correct[tier] += 1
    
    # ä¸¤é˜¶æ®µç­–ç•¥ä¸ä½¿ç”¨Tierç»Ÿè®¡ï¼Œåˆ é™¤æ— ç”¨è¾“å‡º


def generate_sample_analysis_document(results, y_true, noise_mask, save_path, logger, noise_pct=None):
    """
    Generate detailed per-sample analysis document in strategy-grouped format
    
    Args:
        results: dict from run_hybrid_court_with_detailed_tracking
        y_true: (N,) ground truth labels
        noise_mask: (N,) boolean array indicating which labels were flipped to create noise
        save_path: path to save the document
        logger: logger
        noise_pct: noise percentage (for filename)
    """
    logger.info("")
    logger.info("="*70)
    logger.info("ç”Ÿæˆæ ·æœ¬çº§åˆ†ææ–‡æ¡£ï¼ˆç­–ç•¥åˆ†ç»„æ ¼å¼ï¼‰")
    logger.info("="*70)
    
    def _classify_error_type(is_noise, action, true_label, noisy_label, corrected_label):
        """åˆ†ç±»é”™è¯¯ç±»å‹"""
        if corrected_label == true_label:
            return 'æ­£ç¡®'
        
        if is_noise:
            # å™ªå£°æ ·æœ¬
            if action == 1:  # ç¿»è½¬
                return 'ç¿»è½¬é”™è¯¯-å™ªå£°æœªçŸ«æ­£'
            else:
                return 'ä¿æŒé”™è¯¯-å™ªå£°æœªæ£€æµ‹'
        else:
            # å¹²å‡€æ ·æœ¬
            if action == 1:  # ç¿»è½¬
                return 'ç¿»è½¬é”™è¯¯-è¯¯æ€å¹²å‡€æ ·æœ¬'
            else:
                return 'ä¿æŒé”™è¯¯-ä¸åº”å‘ç”Ÿ'
    
    n_samples = len(y_true)
    tier_info = results.get('tier_info', [''] * n_samples)
    
    # è·å–è¿­ä»£CLå’Œé”šç‚¹KNNçš„ç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    iter_cl_probs = results.get('iter_cl_probs', None)
    anchor_votes = results.get('anchor_votes', None)
    anchor_consistency = results.get('anchor_consistency', None)

    # Phase1 çŸ«æ­£åé‡ç®—çš„ Stage2 æŒ‡æ ‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    stage2_aum_scores = results.get('stage2_aum_scores', None)
    stage2_knn_neighbor_labels = results.get('stage2_knn_neighbor_labels', None)
    stage2_knn_neighbor_consistency = results.get('stage2_knn_neighbor_consistency', None)
    stage2_cl_suspected_noise = results.get('stage2_cl_suspected_noise', None)
    stage2_cl_pred_labels = results.get('stage2_cl_pred_labels', None)
    stage2_cl_pred_probs = results.get('stage2_cl_pred_probs', None)
    phase2_actions = results.get('phase2_actions', None)
    
    # Prepare data for DataFrame
    data = []
    
    for i in range(n_samples):
        # Basic info
        sample_id = i
        true_label = int(y_true[i])
        true_label_name = "æ­£å¸¸" if true_label == 0 else "æ¶æ„"
        is_noise = bool(noise_mask[i])
        noisy_label = int(results['noisy_labels'][i])
        noisy_label_name = "æ­£å¸¸" if noisy_label == 0 else "æ¶æ„"
        
        # CL (Confident Learning) results
        cl_suspected_noise = bool(results['cl_suspected_noise'][i])
        cl_pred_label = int(results['cl_pred_labels'][i])
        cl_pred_label_name = "æ­£å¸¸" if cl_pred_label == 0 else "æ¶æ„"
        cl_pred_prob_benign = float(results['cl_pred_probs'][i, 0])
        cl_pred_prob_malicious = float(results['cl_pred_probs'][i, 1])
        
        aum_scores = results.get('aum_scores', None)
        aum_score = float(aum_scores[i]) if aum_scores is not None and hasattr(aum_scores, '__getitem__') and len(aum_scores) > i else 0.0
        
        # KNN (Semantic Voting) results
        knn_neighbor_label = int(results['knn_neighbor_labels'][i])
        knn_neighbor_label_name = "æ­£å¸¸" if knn_neighbor_label == 0 else "æ¶æ„"
        knn_consistency = float(results['knn_neighbor_consistency'][i])
        
        corrected_label = int(results['clean_labels'][i])

        # Iterative CL results (if available) - computed on corrected labels
        if iter_cl_probs is not None:
            iter_cl_current = float(iter_cl_probs[i, corrected_label])
            iter_cl_target = float(iter_cl_probs[i, 1 - corrected_label])
        else:
            iter_cl_current = None
            iter_cl_target = None

        # Stage2 metrics (if available)
        if stage2_aum_scores is not None and hasattr(stage2_aum_scores, '__getitem__') and len(stage2_aum_scores) > i:
            stage2_aum = float(stage2_aum_scores[i])
        else:
            stage2_aum = None

        if stage2_knn_neighbor_labels is not None and hasattr(stage2_knn_neighbor_labels, '__getitem__') and len(stage2_knn_neighbor_labels) > i:
            stage2_knn_label = int(stage2_knn_neighbor_labels[i])
            stage2_knn_label_name = "æ­£å¸¸" if stage2_knn_label == 0 else "æ¶æ„"
        else:
            stage2_knn_label = None
            stage2_knn_label_name = None

        if stage2_knn_neighbor_consistency is not None and hasattr(stage2_knn_neighbor_consistency, '__getitem__') and len(stage2_knn_neighbor_consistency) > i:
            stage2_knn_cons = float(stage2_knn_neighbor_consistency[i])
        else:
            stage2_knn_cons = None

        if stage2_cl_suspected_noise is not None and hasattr(stage2_cl_suspected_noise, '__getitem__') and len(stage2_cl_suspected_noise) > i:
            stage2_cl_noise_flag = bool(stage2_cl_suspected_noise[i])
        else:
            stage2_cl_noise_flag = None

        if stage2_cl_pred_labels is not None and hasattr(stage2_cl_pred_labels, '__getitem__') and len(stage2_cl_pred_labels) > i:
            stage2_cl_pred_label = int(stage2_cl_pred_labels[i])
            stage2_cl_pred_label_name = "æ­£å¸¸" if stage2_cl_pred_label == 0 else "æ¶æ„"
        else:
            stage2_cl_pred_label = None
            stage2_cl_pred_label_name = None

        if stage2_cl_pred_probs is not None and hasattr(stage2_cl_pred_probs, '__getitem__') and len(stage2_cl_pred_probs) > i:
            stage2_cl_pred_prob_benign = float(stage2_cl_pred_probs[i, 0])
            stage2_cl_pred_prob_malicious = float(stage2_cl_pred_probs[i, 1])
        else:
            stage2_cl_pred_prob_benign = None
            stage2_cl_pred_prob_malicious = None

        if phase2_actions is not None and hasattr(phase2_actions, '__getitem__') and len(phase2_actions) > i:
            phase2_action = str(phase2_actions[i])
        else:
            phase2_action = ''
        
        # Anchor KNN results (if available)
        if anchor_votes is not None:
            anchor_vote = int(anchor_votes[i])
            anchor_vote_name = "æ­£å¸¸" if anchor_vote == 0 else "æ¶æ„"
            anchor_cons = float(anchor_consistency[i])
        else:
            anchor_vote = None
            anchor_vote_name = None
            anchor_cons = None
        
        # Final correction decision
        action = int(results['action_mask'][i])
        action_names = ['ä¿æŒ', 'ç¿»è½¬', 'ä¸¢å¼ƒ', 'é‡åŠ æƒ']
        action_name = action_names[action]
        corrected_label_name = "æ­£å¸¸" if corrected_label == 0 else "æ¶æ„"
        confidence = float(results['confidence'][i])
        correction_weight = float(results['correction_weight'][i])
        decision_reason = results['decision_reasons'][i]
        tier = tier_info[i] if i < len(tier_info) else ''
        
        # Correction correctness (compared to ground truth)
        if action == 2:  # Dropped
            correction_correct = "ä¸é€‚ç”¨(å·²ä¸¢å¼ƒ)"
        else:
            correction_correct = "æ­£ç¡®" if corrected_label == true_label else "é”™è¯¯"
        
        # Compile row with ALL detailed metrics
        row = {
            # åŸºæœ¬ä¿¡æ¯
            'æ ·æœ¬ID': sample_id,
            'çœŸå®æ ‡ç­¾': true_label_name,
            'çœŸå®æ ‡ç­¾å€¼': true_label,
            'æ˜¯å¦å™ªå£°': 'æ˜¯' if is_noise else 'å¦',
            'å™ªå£°æ ‡ç­¾': noisy_label_name,
            'å™ªå£°æ ‡ç­¾å€¼': noisy_label,
            
            # CL (Confident Learning) è¯¦ç»†ç»“æœ
            'CLç–‘ä¼¼å™ªå£°': 'æ˜¯' if cl_suspected_noise else 'å¦',
            'CLé¢„æµ‹æ ‡ç­¾': cl_pred_label_name,
            'CLé¢„æµ‹æ ‡ç­¾å€¼': cl_pred_label,
            'CLæ­£å¸¸æ¦‚ç‡': cl_pred_prob_benign,
            'CLæ¶æ„æ¦‚ç‡': cl_pred_prob_malicious,
            'CLå½“å‰æ ‡ç­¾ç½®ä¿¡åº¦': cl_pred_prob_benign if noisy_label == 0 else cl_pred_prob_malicious,
            'CLç›®æ ‡æ ‡ç­¾ç½®ä¿¡åº¦': cl_pred_prob_malicious if noisy_label == 0 else cl_pred_prob_benign,
            'AUMåˆ†æ•°': aum_score,
            
            # KNN (Semantic Voting) è¯¦ç»†ç»“æœ
            'KNNé‚»å±…æ ‡ç­¾': knn_neighbor_label_name,
            'KNNé‚»å±…æ ‡ç­¾å€¼': knn_neighbor_label,
            'KNNä¸€è‡´æ€§': knn_consistency,
            'KNNæ˜¯å¦æ”¯æŒå½“å‰æ ‡ç­¾': 'æ˜¯' if knn_neighbor_label == noisy_label else 'å¦',
            'KNNä¸€è‡´æ€§ç­‰çº§': 'High' if knn_consistency >= 0.7 else ('Medium' if knn_consistency >= 0.5 else 'Low'),
        }

        # Add Stage2 metrics if available
        if stage2_aum is not None:
            row['stage2_AUMåˆ†æ•°'] = stage2_aum
        if stage2_knn_label is not None:
            row['stage2_KNNé‚»å±…æ ‡ç­¾'] = stage2_knn_label_name
            row['stage2_KNNé‚»å±…æ ‡ç­¾å€¼'] = stage2_knn_label
        if stage2_knn_cons is not None:
            row['stage2_KNNä¸€è‡´æ€§'] = stage2_knn_cons
            # stage2_KNNæ˜¯å¦æ”¯æŒå½“å‰æ ‡ç­¾ï¼šåº”è¯¥åŸºäºPhase1çŸ«æ­£åçš„æ ‡ç­¾åˆ¤æ–­
            # è¿™é‡Œå…ˆç”¨corrected_labelï¼Œåç»­åœ¨é˜¶æ®µ2sheetä¸­ä¼šé‡æ–°è®¡ç®—
            row['stage2_KNNæ˜¯å¦æ”¯æŒå½“å‰æ ‡ç­¾'] = 'æ˜¯' if stage2_knn_label == corrected_label else 'å¦'

        if stage2_cl_noise_flag is not None:
            # numeric 0/1 for easier pandas operations
            row['stage2_CLç–‘ä¼¼å™ªå£°'] = 1 if stage2_cl_noise_flag else 0
        
        # Add stage2_CLé¢„æµ‹æ ‡ç­¾ if available
        if stage2_cl_pred_label is not None:
            row['stage2_CLé¢„æµ‹æ ‡ç­¾'] = stage2_cl_pred_label_name
            row['stage2_CLé¢„æµ‹æ ‡ç­¾å€¼'] = stage2_cl_pred_label
            if stage2_cl_pred_prob_benign is not None:
                # è®¡ç®—åŸºäºPhase1çŸ«æ­£åæ ‡ç­¾çš„CLç½®ä¿¡åº¦
                phase1_corrected_label = corrected_label  # è¿™é‡Œcorrected_labelå°±æ˜¯Phase1çŸ«æ­£åçš„æ ‡ç­¾
                row['stage2_CLå½“å‰æ ‡ç­¾ç½®ä¿¡åº¦'] = stage2_cl_pred_prob_benign if phase1_corrected_label == 0 else stage2_cl_pred_prob_malicious
                row['stage2_CLç›®æ ‡æ ‡ç­¾ç½®ä¿¡åº¦'] = stage2_cl_pred_prob_malicious if phase1_corrected_label == 0 else stage2_cl_pred_prob_benign
        
        # Add iterative CL columns if available
        if iter_cl_current is not None:
            row['iter_CLå½“å‰æ ‡ç­¾ç½®ä¿¡åº¦'] = iter_cl_current
            row['iter_CLç›®æ ‡æ ‡ç­¾ç½®ä¿¡åº¦'] = iter_cl_target
            row['iter_CLç½®ä¿¡åº¦å·®å€¼'] = abs(iter_cl_current - iter_cl_target)
        
        # Add anchor KNN columns if available
        if anchor_vote is not None:
            row['anchor_KNNæŠ•ç¥¨'] = anchor_vote_name
            row['anchor_KNNæŠ•ç¥¨å€¼'] = anchor_vote
            row['anchor_KNNä¸€è‡´æ€§'] = anchor_cons
            row['anchor_KNNæ˜¯å¦æ”¯æŒå½“å‰'] = 'æ˜¯' if anchor_vote == corrected_label else 'å¦'
        
        # æœ€ç»ˆå†³ç­–è¯¦ç»†ä¿¡æ¯
        row.update({
            'çŸ«æ­£åŠ¨ä½œ': action_name,
            'çŸ«æ­£åŠ¨ä½œå€¼': action,
            'çŸ«æ­£åæ ‡ç­¾': corrected_label_name,
            'çŸ«æ­£åæ ‡ç­¾å€¼': corrected_label,
            'Phase2_Action': phase2_action,
            'ç³»ç»Ÿç½®ä¿¡åº¦': confidence,
            'æ ·æœ¬æƒå€¼': correction_weight,
            'Tieråˆ†çº§': tier,
            'Tieré˜¶æ®µ': 'Dropped' if action == 2 else ('Phase 2' if action in (1, 3) else 'Phase 1'),
            'å†³ç­–ç†ç”±': decision_reason,
            
            # çŸ«æ­£ç»“æœè¯„ä¼°
            'çŸ«æ­£æ˜¯å¦æ­£ç¡®': correction_correct,
            'æ˜¯å¦ç¿»è½¬': 'æ˜¯' if action == 1 else 'å¦',
            'æ˜¯å¦ä¸¢å¼ƒ': 'æ˜¯' if action == 2 else 'å¦',
            'æ ‡ç­¾æ˜¯å¦æ”¹å˜': 'æ˜¯' if corrected_label != noisy_label else 'å¦',
            
            # é”™è¯¯ç±»å‹åˆ†æ
            'é”™è¯¯ç±»å‹': _classify_error_type(
                is_noise, action, true_label, noisy_label, corrected_label
            ) if action != 2 else 'å·²ä¸¢å¼ƒ'
        })
        
        data.append(row)
    
    # Create DataFrame
    df = pd.DataFrame(data)
    
    # Save to CSV (åŸºç¡€ç‰ˆæœ¬ï¼Œæ‰€æœ‰æ•°æ®åœ¨ä¸€ä¸ªæ–‡ä»¶)
    # æ–°ç­–ç•¥ç‰ˆæœ¬åŒ–ï¼šç¡®ä¿æ–‡ä»¶ååŒ…å«å™ªå£°ç‡ä¸ç­–ç•¥æ ‡è¯†ï¼Œé¿å…è¦†ç›–æ—§ç»“æœ
    if noise_pct is not None:
        base_name = save_path.replace('.txt', '').replace('.log', '')
        if f'_{noise_pct}pct' not in base_name:
            base_name = f"{base_name}_{noise_pct}pct"
        if '_cl_aum_knn' not in base_name:
            base_name = f"{base_name}_cl_aum_knn"
        csv_path = f"{base_name}.csv"
    else:
        csv_path = save_path.replace('.txt', '.csv').replace('.log', '.csv')
    
    df.to_csv(csv_path, index=False, encoding='utf-8-sig')
    logger.info(f"âœ“ CSVæ–‡ä»¶å·²ä¿å­˜: {csv_path}")
    
    # ========================================
    # ç”ŸæˆåŸºç¡€å¯è¯»æ–‡æœ¬æ–‡ä»¶ï¼ˆä»CSVæ•°æ®ï¼‰
    # ========================================
    try:
        readable_text_path = csv_path.replace('.csv', '_readable.txt')
        with open(readable_text_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write(f"æ ·æœ¬åˆ†ææ•°æ® - å¯è¯»æ–‡æœ¬æ ¼å¼ï¼ˆå™ªå£°ç‡: {noise_pct}%ï¼‰\n")
            f.write("=" * 80 + "\n\n")
            
            # æ€»è§ˆç»Ÿè®¡
            f.write("\n" + "=" * 80 + "\n")
            f.write("æ€»è§ˆç»Ÿè®¡\n")
            f.write("=" * 80 + "\n\n")
            
            tier_summary = []
            for tier in sorted(df['Tieråˆ†çº§'].unique()):
                tier_df = df[df['Tieråˆ†çº§'] == tier]
                valid_df = tier_df[tier_df['çŸ«æ­£æ˜¯å¦æ­£ç¡®'] != 'ä¸é€‚ç”¨(å·²ä¸¢å¼ƒ)']
                correct_count = len(tier_df[tier_df['çŸ«æ­£æ˜¯å¦æ­£ç¡®'] == 'æ­£ç¡®'])
                error_count = len(tier_df[tier_df['çŸ«æ­£æ˜¯å¦æ­£ç¡®'] == 'é”™è¯¯'])
                dropped_count = len(tier_df[tier_df['çŸ«æ­£æ˜¯å¦æ­£ç¡®'] == 'ä¸é€‚ç”¨(å·²ä¸¢å¼ƒ)'])
                
                tier_summary.append({
                    'Tier': tier,
                    'æ€»æ ·æœ¬æ•°': len(tier_df),
                    'âœ“æ­£ç¡®å¤„ç†': correct_count,
                    'âœ—é”™è¯¯å¤„ç†': error_count,
                    'âŠ—å·²ä¸¢å¼ƒ': dropped_count,
                    'å‡†ç¡®ç‡': f"{correct_count / len(valid_df) * 100:.2f}%" if len(valid_df) > 0 else 'N/A',
                    'å¹³å‡æƒå€¼': f"{tier_df['æ ·æœ¬æƒå€¼'].mean():.4f}",
                    'å™ªå£°æ ·æœ¬æ•°': len(tier_df[tier_df['æ˜¯å¦å™ªå£°'] == 'æ˜¯']),
                    'å¹²å‡€æ ·æœ¬æ•°': len(tier_df[tier_df['æ˜¯å¦å™ªå£°'] == 'å¦'])
                })
            
            summary_df = pd.DataFrame(tier_summary)
            f.write(summary_df.to_string(index=False))
            f.write("\n\n")
            
            # é”™è¯¯æ ·æœ¬æ±‡æ€»
            error_samples = df[df['çŸ«æ­£æ˜¯å¦æ­£ç¡®'] == 'é”™è¯¯']
            if len(error_samples) > 0:
                f.write("\n" + "=" * 80 + "\n")
                f.write(f"é”™è¯¯æ ·æœ¬æ±‡æ€»ï¼ˆå…±{len(error_samples)}ä¸ªï¼‰\n")
                f.write("=" * 80 + "\n\n")
                
                error_key_cols = ['æ ·æœ¬ID', 'Tieråˆ†çº§', 'æ˜¯å¦å™ªå£°', 'çœŸå®æ ‡ç­¾', 'å™ªå£°æ ‡ç­¾', 
                                 'çŸ«æ­£åæ ‡ç­¾', 'çŸ«æ­£åŠ¨ä½œ', 'é”™è¯¯ç±»å‹', 'ç³»ç»Ÿç½®ä¿¡åº¦']
                error_display_cols = [col for col in error_key_cols if col in error_samples.columns]
                if error_display_cols:
                    f.write(error_samples[error_display_cols].to_string(index=False))
                    f.write("\n\n")
            
            # è¯¯æ€æ ·æœ¬
            false_positive = df[(df['æ˜¯å¦å™ªå£°'] == 'å¦') & (df['çŸ«æ­£æ˜¯å¦æ­£ç¡®'] == 'é”™è¯¯')]
            if len(false_positive) > 0:
                f.write("\n" + "=" * 80 + "\n")
                f.write(f"è¯¯æ€å¹²å‡€æ ·æœ¬ï¼ˆå…±{len(false_positive)}ä¸ªï¼‰\n")
                f.write("=" * 80 + "\n\n")
                
                fp_key_cols = ['æ ·æœ¬ID', 'Tieråˆ†çº§', 'çœŸå®æ ‡ç­¾', 'å™ªå£°æ ‡ç­¾', 'çŸ«æ­£åæ ‡ç­¾', 
                              'çŸ«æ­£åŠ¨ä½œ', 'é”™è¯¯ç±»å‹', 'ç³»ç»Ÿç½®ä¿¡åº¦']
                fp_display_cols = [col for col in fp_key_cols if col in false_positive.columns]
                if fp_display_cols:
                    f.write(false_positive[fp_display_cols].to_string(index=False))
                    f.write("\n\n")
            
            # æ¼ç½‘å™ªå£°
            false_negative = df[(df['æ˜¯å¦å™ªå£°'] == 'æ˜¯') & (df['çŸ«æ­£æ˜¯å¦æ­£ç¡®'] == 'é”™è¯¯')]
            if len(false_negative) > 0:
                f.write("\n" + "=" * 80 + "\n")
                f.write(f"æ¼ç½‘å™ªå£°ï¼ˆå…±{len(false_negative)}ä¸ªï¼‰\n")
                f.write("=" * 80 + "\n\n")
                
                fn_key_cols = ['æ ·æœ¬ID', 'Tieråˆ†çº§', 'çœŸå®æ ‡ç­¾', 'å™ªå£°æ ‡ç­¾', 'çŸ«æ­£åæ ‡ç­¾', 
                              'çŸ«æ­£åŠ¨ä½œ', 'é”™è¯¯ç±»å‹', 'ç³»ç»Ÿç½®ä¿¡åº¦']
                fn_display_cols = [col for col in fn_key_cols if col in false_negative.columns]
                if fn_display_cols:
                    f.write(false_negative[fn_display_cols].to_string(index=False))
                    f.write("\n\n")
            
            # æŒ‰Tieråˆ†ç»„çš„æ ·æœ¬ï¼ˆå‰20ä¸ªï¼‰
            f.write("\n" + "=" * 80 + "\n")
            f.write("å„Tieræ ·æœ¬è¯¦æƒ…ï¼ˆæ¯ä¸ªTieræ˜¾ç¤ºå‰20ä¸ªæ ·æœ¬ï¼‰\n")
            f.write("=" * 80 + "\n\n")
            
            tier_order = ['P1: Keep', 'P1: Flip', 'P1: Rescue', 'P1: Reweight',
                         'P2: Keep', 'P2: Flip', 'P2: Rescue', 'P2: Reweight']
            
            for tier in tier_order:
                tier_data = df[df['Tieråˆ†çº§'] == tier]
                if len(tier_data) > 0:
                    f.write(f"\n--- {tier} (å…±{len(tier_data)}ä¸ªæ ·æœ¬) ---\n")
                    
                    key_cols = ['æ ·æœ¬ID', 'çŸ«æ­£æ˜¯å¦æ­£ç¡®', 'æ˜¯å¦å™ªå£°', 'çœŸå®æ ‡ç­¾', 'å™ªå£°æ ‡ç­¾', 
                               'çŸ«æ­£åæ ‡ç­¾', 'çŸ«æ­£åŠ¨ä½œ', 'ç³»ç»Ÿç½®ä¿¡åº¦', 'æ ·æœ¬æƒå€¼']
                    display_cols = [col for col in key_cols if col in tier_data.columns]
                    
                    display_data = tier_data[display_cols].head(20)
                    f.write(display_data.to_string(index=False))
                    if len(tier_data) > 20:
                        f.write(f"\n... (è¿˜æœ‰{len(tier_data) - 20}ä¸ªæ ·æœ¬æœªæ˜¾ç¤º)")
                    f.write("\n\n")
            
            f.write("\n" + "=" * 80 + "\n")
            f.write("æ–‡ä»¶ç»“æŸ\n")
            f.write("=" * 80 + "\n")
        
        logger.info(f"âœ“ å¯è¯»æ–‡æœ¬æ–‡ä»¶å·²ä¿å­˜: {readable_text_path}")
        logger.info(f"  è¯´æ˜: æ­¤æ–‡ä»¶åŒ…å«å…³é”®ç»Ÿè®¡å’Œæ ·æœ¬ä¿¡æ¯ï¼Œæ–¹ä¾¿AIç›´æ¥åˆ†æ")
    except Exception as e:
        logger.warning(f"âš  å¯è¯»æ–‡æœ¬æ–‡ä»¶ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        logger.warning(traceback.format_exc())
    
    # Save to Excel with multiple sheets (å¦‚æœopenpyxlå¯ç”¨)
    if EXCEL_AVAILABLE:
        # Excelæ–‡ä»¶åä»CSVæ–‡ä»¶åç”Ÿæˆï¼Œå·²åŒ…å«å™ªå£°ç‡
        excel_path = csv_path.replace('.csv', '.xlsx')
        try:
            from openpyxl.styles import PatternFill, Font
            
            with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
                # é‡æ–°æ’åˆ—åˆ—é¡ºåºï¼ŒæŠŠå…³é”®ä¿¡æ¯æ”¾å‰é¢
                key_columns = [
                    'æ ·æœ¬ID', 'çŸ«æ­£æ˜¯å¦æ­£ç¡®', 'é”™è¯¯ç±»å‹', 'Tieråˆ†çº§', 'Tieré˜¶æ®µ',
                    'æ˜¯å¦å™ªå£°', 'çœŸå®æ ‡ç­¾', 'çœŸå®æ ‡ç­¾å€¼', 'å™ªå£°æ ‡ç­¾', 'å™ªå£°æ ‡ç­¾å€¼',
                    'çŸ«æ­£åæ ‡ç­¾', 'çŸ«æ­£åæ ‡ç­¾å€¼', 'çŸ«æ­£åŠ¨ä½œ', 'å†³ç­–ç†ç”±',
                    'Phase2_Action',
                    'ç³»ç»Ÿç½®ä¿¡åº¦', 'æ ·æœ¬æƒå€¼',
                    'CLç–‘ä¼¼å™ªå£°', 'CLå½“å‰æ ‡ç­¾ç½®ä¿¡åº¦', 'CLç›®æ ‡æ ‡ç­¾ç½®ä¿¡åº¦',
                    'AUMåˆ†æ•°',
                    'KNNé‚»å±…æ ‡ç­¾', 'KNNä¸€è‡´æ€§', 'KNNæ˜¯å¦æ”¯æŒå½“å‰æ ‡ç­¾',
                    'iter_CLå½“å‰æ ‡ç­¾ç½®ä¿¡åº¦', 'iter_CLç›®æ ‡æ ‡ç­¾ç½®ä¿¡åº¦', 'iter_CLç½®ä¿¡åº¦å·®å€¼',
                    'stage2_CLç–‘ä¼¼å™ªå£°',
                    'stage2_AUMåˆ†æ•°',
                    'stage2_KNNé‚»å±…æ ‡ç­¾', 'stage2_KNNä¸€è‡´æ€§', 'stage2_KNNæ˜¯å¦æ”¯æŒå½“å‰æ ‡ç­¾',
                    'æ˜¯å¦ç¿»è½¬', 'æ˜¯å¦ä¸¢å¼ƒ', 'æ ‡ç­¾æ˜¯å¦æ”¹å˜'
                ]
                # åªé€‰æ‹©å­˜åœ¨çš„åˆ—
                existing_key_columns = [col for col in key_columns if col in df.columns]
                df_reordered = df[existing_key_columns].copy()
                
                # Sheet 1: æ€»è§ˆç»Ÿè®¡
                tier_summary = []
                for tier in sorted(df['Tieråˆ†çº§'].unique()):
                    tier_df = df[df['Tieråˆ†çº§'] == tier]
                    valid_df = tier_df[tier_df['çŸ«æ­£æ˜¯å¦æ­£ç¡®'] != 'ä¸é€‚ç”¨(å·²ä¸¢å¼ƒ)']
                    correct_count = len(tier_df[tier_df['çŸ«æ­£æ˜¯å¦æ­£ç¡®'] == 'æ­£ç¡®'])
                    error_count = len(tier_df[tier_df['çŸ«æ­£æ˜¯å¦æ­£ç¡®'] == 'é”™è¯¯'])
                    dropped_count = len(tier_df[tier_df['çŸ«æ­£æ˜¯å¦æ­£ç¡®'] == 'ä¸é€‚ç”¨(å·²ä¸¢å¼ƒ)'])
                    
                    tier_summary.append({
                        'Tier': tier,
                        'æ€»æ ·æœ¬æ•°': len(tier_df),
                        'âœ“æ­£ç¡®å¤„ç†': correct_count,
                        'âœ—é”™è¯¯å¤„ç†': error_count,
                        'âŠ—å·²ä¸¢å¼ƒ': dropped_count,
                        'å‡†ç¡®ç‡': f"{correct_count / len(valid_df) * 100:.2f}%" if len(valid_df) > 0 else 'N/A',
                        'å¹³å‡æƒå€¼': f"{tier_df['æ ·æœ¬æƒå€¼'].mean():.4f}",
                        'å™ªå£°æ ·æœ¬æ•°': len(tier_df[tier_df['æ˜¯å¦å™ªå£°'] == 'æ˜¯']),
                        'å¹²å‡€æ ·æœ¬æ•°': len(tier_df[tier_df['æ˜¯å¦å™ªå£°'] == 'å¦'])
                    })
                
                summary_df = pd.DataFrame(tier_summary)
                summary_df.to_excel(writer, sheet_name='ğŸ“Šæ€»è§ˆç»Ÿè®¡', index=False)
                
                # ä¸ºæ€»è§ˆç»Ÿè®¡æ·»åŠ æ ¼å¼
                ws = writer.sheets['ğŸ“Šæ€»è§ˆç»Ÿè®¡']
                for row in range(2, len(summary_df) + 2):
                    # æ­£ç¡®å¤„ç† - ç»¿è‰²
                    ws.cell(row=row, column=3).fill = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")
                    # é”™è¯¯å¤„ç† - çº¢è‰²
                    ws.cell(row=row, column=4).fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
                    # å·²ä¸¢å¼ƒ - ç°è‰²
                    ws.cell(row=row, column=5).fill = PatternFill(start_color="D9D9D9", end_color="D9D9D9", fill_type="solid")
                
                # ========================================
                # Sheet: æ‰€æœ‰æ ·æœ¬ç»Ÿè®¡ï¼ˆé˜¶æ®µ1å’Œé˜¶æ®µ2ï¼‰
                # ========================================
                all_samples_stats = []
                
                # è®¡ç®—Phase1çŸ«æ­£åçš„æ ‡ç­¾ï¼ˆç”¨äºé˜¶æ®µ1ç»Ÿè®¡ï¼‰
                phase1_labels = []
                for i in range(len(df_reordered)):
                    noisy_label_val = df_reordered.iloc[i]['å™ªå£°æ ‡ç­¾å€¼']
                    phase2_action = df_reordered.iloc[i].get('Phase2_Action', '')
                    final_label_val = df_reordered.iloc[i]['çŸ«æ­£åæ ‡ç­¾å€¼']
                    
                    # ä»phase2_actionåæ¨phase1çš„æ ‡ç­¾
                    if phase2_action == 'UndoFlip':
                        phase1_label = 1 - noisy_label_val
                    elif phase2_action == 'LateFlip':
                        phase1_label = noisy_label_val
                    else:
                        if final_label_val != noisy_label_val:
                            phase1_label = final_label_val
                        else:
                            phase1_label = noisy_label_val
                    phase1_labels.append(phase1_label)
                
                df_reordered['Phase1çŸ«æ­£åæ ‡ç­¾å€¼'] = phase1_labels
                df_reordered['Phase1æ˜¯å¦æ­£ç¡®'] = (df_reordered['Phase1çŸ«æ­£åæ ‡ç­¾å€¼'] == df_reordered['çœŸå®æ ‡ç­¾å€¼']).apply(lambda x: 'æ­£ç¡®' if x else 'é”™è¯¯')
                
                # é˜¶æ®µ1ç»Ÿè®¡
                total_samples = len(df_reordered)
                phase1_correct = len(df_reordered[df_reordered['Phase1æ˜¯å¦æ­£ç¡®'] == 'æ­£ç¡®'])
                phase1_error = len(df_reordered[df_reordered['Phase1æ˜¯å¦æ­£ç¡®'] == 'é”™è¯¯'])
                phase1_accuracy = (phase1_correct / total_samples * 100) if total_samples > 0 else 0
                
                # é˜¶æ®µ1åŠ¨ä½œç»Ÿè®¡
                phase1_flip = len(df_reordered[df_reordered['çŸ«æ­£åŠ¨ä½œ'] == 'Flip'])
                phase1_keep = len(df_reordered[df_reordered['çŸ«æ­£åŠ¨ä½œ'] == 'Keep'])
                
                # é˜¶æ®µ1æŒ‡æ ‡ç»Ÿè®¡
                phase1_cl_mean = df_reordered['CLå½“å‰æ ‡ç­¾ç½®ä¿¡åº¦'].mean() if 'CLå½“å‰æ ‡ç­¾ç½®ä¿¡åº¦' in df_reordered.columns else 0
                phase1_aum_mean = df_reordered['AUMåˆ†æ•°'].mean() if 'AUMåˆ†æ•°' in df_reordered.columns else 0
                phase1_knn_mean = df_reordered['KNNä¸€è‡´æ€§'].mean() if 'KNNä¸€è‡´æ€§' in df_reordered.columns else 0
                
                # é˜¶æ®µ2ç»Ÿè®¡ï¼ˆå¦‚æœæœ‰Phase2ï¼‰
                if 'Phase2_Action' in df_reordered.columns and df_reordered['Phase2_Action'].notna().any():
                    phase2_correct = len(df_reordered[df_reordered['çŸ«æ­£æ˜¯å¦æ­£ç¡®'] == 'æ­£ç¡®'])
                    phase2_error = len(df_reordered[df_reordered['çŸ«æ­£æ˜¯å¦æ­£ç¡®'] == 'é”™è¯¯'])
                    phase2_accuracy = (phase2_correct / total_samples * 100) if total_samples > 0 else 0
                    
                    # é˜¶æ®µ2åŠ¨ä½œç»Ÿè®¡
                    phase2_undo = len(df_reordered[df_reordered['Phase2_Action'] == 'UndoFlip'])
                    phase2_late = len(df_reordered[df_reordered['Phase2_Action'] == 'LateFlip'])
                    phase2_nochange = len(df_reordered[df_reordered['Phase2_Action'] == 'NoChange'])
                    
                    # é˜¶æ®µ2æŒ‡æ ‡ç»Ÿè®¡
                    phase2_cl_mean = df_reordered['stage2_CLå½“å‰æ ‡ç­¾ç½®ä¿¡åº¦'].mean() if 'stage2_CLå½“å‰æ ‡ç­¾ç½®ä¿¡åº¦' in df_reordered.columns else 0
                    phase2_aum_mean = df_reordered['stage2_AUMåˆ†æ•°'].mean() if 'stage2_AUMåˆ†æ•°' in df_reordered.columns else 0
                    phase2_knn_mean = df_reordered['stage2_KNNä¸€è‡´æ€§'].mean() if 'stage2_KNNä¸€è‡´æ€§' in df_reordered.columns else 0
                else:
                    phase2_correct = 0
                    phase2_error = 0
                    phase2_accuracy = 0
                    phase2_undo = 0
                    phase2_late = 0
                    phase2_nochange = total_samples
                    phase2_cl_mean = 0
                    phase2_aum_mean = 0
                    phase2_knn_mean = 0
                
                # åŸå§‹æ ‡ç­¾ç»Ÿè®¡
                original_correct = len(df_reordered[df_reordered['æ˜¯å¦å™ªå£°'] == 'å¦'])
                original_error = len(df_reordered[df_reordered['æ˜¯å¦å™ªå£°'] == 'æ˜¯'])
                original_accuracy = (original_correct / total_samples * 100) if total_samples > 0 else 0
                
                # æ„å»ºç»Ÿè®¡è¡¨
                all_samples_stats.append({
                    'ç»Ÿè®¡é¡¹': 'æ€»æ ·æœ¬æ•°',
                    'æ•°å€¼': total_samples,
                    'é˜¶æ®µ1': '',
                    'é˜¶æ®µ2': ''
                })
                all_samples_stats.append({
                    'ç»Ÿè®¡é¡¹': 'åŸå§‹æ ‡ç­¾å‡†ç¡®ç‡',
                    'æ•°å€¼': f"{original_accuracy:.2f}%",
                    'é˜¶æ®µ1': f"æ­£ç¡®: {original_correct}, é”™è¯¯: {original_error}",
                    'é˜¶æ®µ2': ''
                })
                all_samples_stats.append({
                    'ç»Ÿè®¡é¡¹': 'é˜¶æ®µ1å‡†ç¡®ç‡',
                    'æ•°å€¼': f"{phase1_accuracy:.2f}%",
                    'é˜¶æ®µ1': f"æ­£ç¡®: {phase1_correct}, é”™è¯¯: {phase1_error}",
                    'é˜¶æ®µ2': ''
                })
                all_samples_stats.append({
                    'ç»Ÿè®¡é¡¹': 'é˜¶æ®µ2å‡†ç¡®ç‡',
                    'æ•°å€¼': f"{phase2_accuracy:.2f}%" if phase2_accuracy > 0 else 'N/A',
                    'é˜¶æ®µ1': '',
                    'é˜¶æ®µ2': f"æ­£ç¡®: {phase2_correct}, é”™è¯¯: {phase2_error}" if phase2_accuracy > 0 else 'N/A'
                })
                all_samples_stats.append({
                    'ç»Ÿè®¡é¡¹': 'é˜¶æ®µ1åŠ¨ä½œ',
                    'æ•°å€¼': '',
                    'é˜¶æ®µ1': f"Flip: {phase1_flip}, Keep: {phase1_keep}",
                    'é˜¶æ®µ2': ''
                })
                all_samples_stats.append({
                    'ç»Ÿè®¡é¡¹': 'é˜¶æ®µ2åŠ¨ä½œ',
                    'æ•°å€¼': '',
                    'é˜¶æ®µ1': '',
                    'é˜¶æ®µ2': f"UndoFlip: {phase2_undo}, LateFlip: {phase2_late}, NoChange: {phase2_nochange}" if phase2_accuracy > 0 else 'N/A'
                })
                all_samples_stats.append({
                    'ç»Ÿè®¡é¡¹': 'é˜¶æ®µ1å¹³å‡CLç½®ä¿¡åº¦',
                    'æ•°å€¼': f"{phase1_cl_mean:.4f}",
                    'é˜¶æ®µ1': '',
                    'é˜¶æ®µ2': ''
                })
                all_samples_stats.append({
                    'ç»Ÿè®¡é¡¹': 'é˜¶æ®µ1å¹³å‡AUMåˆ†æ•°',
                    'æ•°å€¼': f"{phase1_aum_mean:.4f}",
                    'é˜¶æ®µ1': '',
                    'é˜¶æ®µ2': ''
                })
                all_samples_stats.append({
                    'ç»Ÿè®¡é¡¹': 'é˜¶æ®µ1å¹³å‡KNNä¸€è‡´æ€§',
                    'æ•°å€¼': f"{phase1_knn_mean:.4f}",
                    'é˜¶æ®µ1': '',
                    'é˜¶æ®µ2': ''
                })
                all_samples_stats.append({
                    'ç»Ÿè®¡é¡¹': 'é˜¶æ®µ2å¹³å‡CLç½®ä¿¡åº¦',
                    'æ•°å€¼': f"{phase2_cl_mean:.4f}" if phase2_cl_mean > 0 else 'N/A',
                    'é˜¶æ®µ1': '',
                    'é˜¶æ®µ2': ''
                })
                all_samples_stats.append({
                    'ç»Ÿè®¡é¡¹': 'é˜¶æ®µ2å¹³å‡AUMåˆ†æ•°',
                    'æ•°å€¼': f"{phase2_aum_mean:.4f}" if phase2_aum_mean > 0 else 'N/A',
                    'é˜¶æ®µ1': '',
                    'é˜¶æ®µ2': ''
                })
                all_samples_stats.append({
                    'ç»Ÿè®¡é¡¹': 'é˜¶æ®µ2å¹³å‡KNNä¸€è‡´æ€§',
                    'æ•°å€¼': f"{phase2_knn_mean:.4f}" if phase2_knn_mean > 0 else 'N/A',
                    'é˜¶æ®µ1': '',
                    'é˜¶æ®µ2': ''
                })
                all_samples_stats.append({
                    'ç»Ÿè®¡é¡¹': 'é˜¶æ®µ1æå‡',
                    'æ•°å€¼': f"{phase1_accuracy - original_accuracy:+.2f}%",
                    'é˜¶æ®µ1': '',
                    'é˜¶æ®µ2': ''
                })
                all_samples_stats.append({
                    'ç»Ÿè®¡é¡¹': 'é˜¶æ®µ2æå‡',
                    'æ•°å€¼': f"{phase2_accuracy - phase1_accuracy:+.2f}%" if phase2_accuracy > 0 else 'N/A',
                    'é˜¶æ®µ1': '',
                    'é˜¶æ®µ2': ''
                })
                all_samples_stats.append({
                    'ç»Ÿè®¡é¡¹': 'æ€»æå‡',
                    'æ•°å€¼': f"{phase2_accuracy - original_accuracy:+.2f}%" if phase2_accuracy > 0 else f"{phase1_accuracy - original_accuracy:+.2f}%",
                    'é˜¶æ®µ1': '',
                    'é˜¶æ®µ2': ''
                })
                
                stats_df = pd.DataFrame(all_samples_stats)
                stats_df.to_excel(writer, sheet_name='ğŸ“Šæ‰€æœ‰æ ·æœ¬ç»Ÿè®¡', index=False)
                
                # ä¸ºç»Ÿè®¡è¡¨æ·»åŠ æ ¼å¼
                ws_stats = writer.sheets['ğŸ“Šæ‰€æœ‰æ ·æœ¬ç»Ÿè®¡']
                # è®¾ç½®æ ‡é¢˜è¡Œæ ¼å¼
                for col in range(1, len(stats_df.columns) + 1):
                    ws_stats.cell(row=1, column=col).font = Font(bold=True)
                    ws_stats.cell(row=1, column=col).fill = PatternFill(start_color="4472C4", end_color="4472C4", fill_type="solid")
                    ws_stats.cell(row=1, column=col).font = Font(bold=True, color="FFFFFF")
                
                logger.info(f"  âœ“ æ‰€æœ‰æ ·æœ¬ç»Ÿè®¡: å·²ç”Ÿæˆç»Ÿè®¡è¡¨")
                
                # ========================================
                # Sheet: é˜¶æ®µ1æŒ‡æ ‡ï¼ˆæ‰€æœ‰æ ·æœ¬ï¼‰
                # ========================================
                
                # Phase1çŸ«æ­£åçš„æ ‡ç­¾å·²ç»åœ¨ç»Ÿè®¡éƒ¨åˆ†è®¡ç®—è¿‡äº†ï¼Œè¿™é‡Œç›´æ¥ä½¿ç”¨
                if 'Phase1çŸ«æ­£åæ ‡ç­¾å€¼' not in df_reordered.columns:
                    # å¦‚æœè¿˜æ²¡æœ‰è®¡ç®—ï¼Œåˆ™è®¡ç®—ä¸€æ¬¡
                    phase1_labels = []
                    for i in range(len(df_reordered)):
                        noisy_label_val = df_reordered.iloc[i]['å™ªå£°æ ‡ç­¾å€¼']
                        phase2_action = df_reordered.iloc[i].get('Phase2_Action', '')
                        final_label_val = df_reordered.iloc[i]['çŸ«æ­£åæ ‡ç­¾å€¼']
                        
                        # ä»phase2_actionåæ¨phase1çš„æ ‡ç­¾
                        if phase2_action == 'UndoFlip':
                            phase1_label = 1 - noisy_label_val
                        elif phase2_action == 'LateFlip':
                            phase1_label = noisy_label_val
                        else:
                            if final_label_val != noisy_label_val:
                                phase1_label = final_label_val
                            else:
                                phase1_label = noisy_label_val
                        phase1_labels.append(phase1_label)
                    df_reordered['Phase1çŸ«æ­£åæ ‡ç­¾å€¼'] = phase1_labels
                
                if 'Phase1çŸ«æ­£åæ ‡ç­¾' not in df_reordered.columns:
                    df_reordered['Phase1çŸ«æ­£åæ ‡ç­¾'] = df_reordered['Phase1çŸ«æ­£åæ ‡ç­¾å€¼'].apply(lambda x: 'æ­£å¸¸' if x == 0 else 'æ¶æ„')
                
                # ç¡®ä¿Phase1æ˜¯å¦æ­£ç¡®åˆ—å·²è®¡ç®—
                if 'Phase1æ˜¯å¦æ­£ç¡®' not in df_reordered.columns:
                    df_reordered['Phase1æ˜¯å¦æ­£ç¡®'] = (df_reordered['Phase1çŸ«æ­£åæ ‡ç­¾å€¼'] == df_reordered['çœŸå®æ ‡ç­¾å€¼']).apply(lambda x: 'æ­£ç¡®' if x else 'é”™è¯¯')
                
                # Sheet: é˜¶æ®µ1æŒ‡æ ‡
                phase1_columns = [
                    'æ ·æœ¬ID', 'æ˜¯å¦å™ªå£°', 'çœŸå®æ ‡ç­¾', 'çœŸå®æ ‡ç­¾å€¼', 
                    'å™ªå£°æ ‡ç­¾', 'å™ªå£°æ ‡ç­¾å€¼',
                    'Phase1çŸ«æ­£åæ ‡ç­¾', 'Phase1çŸ«æ­£åæ ‡ç­¾å€¼',
                    'CLç–‘ä¼¼å™ªå£°', 'CLå½“å‰æ ‡ç­¾ç½®ä¿¡åº¦', 'CLç›®æ ‡æ ‡ç­¾ç½®ä¿¡åº¦', 'CLé¢„æµ‹æ ‡ç­¾',
                    'AUMåˆ†æ•°',
                    'KNNé‚»å±…æ ‡ç­¾', 'KNNä¸€è‡´æ€§', 'KNNæ˜¯å¦æ”¯æŒå½“å‰æ ‡ç­¾',
                    'çŸ«æ­£åŠ¨ä½œ', 'ç³»ç»Ÿç½®ä¿¡åº¦', 'æ ·æœ¬æƒå€¼', 'å†³ç­–ç†ç”±'
                ]
                phase1_existing_cols = [col for col in phase1_columns if col in df_reordered.columns]
                phase1_df = df_reordered[phase1_existing_cols].copy()
                
                # æ·»åŠ Phase1æ˜¯å¦æ­£ç¡®åˆ—ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
                if 'Phase1æ˜¯å¦æ­£ç¡®' not in phase1_df.columns:
                    phase1_df['Phase1æ˜¯å¦æ­£ç¡®'] = (phase1_df['Phase1çŸ«æ­£åæ ‡ç­¾å€¼'] == phase1_df['çœŸå®æ ‡ç­¾å€¼']).apply(lambda x: 'æ­£ç¡®' if x else 'é”™è¯¯')
                
                # é‡æ–°æ’åˆ—åˆ—é¡ºåºï¼Œåªé€‰æ‹©å­˜åœ¨çš„åˆ—
                phase1_column_order = [
                    'æ ·æœ¬ID', 'æ˜¯å¦å™ªå£°', 'çœŸå®æ ‡ç­¾', 'çœŸå®æ ‡ç­¾å€¼', 
                    'å™ªå£°æ ‡ç­¾', 'å™ªå£°æ ‡ç­¾å€¼',
                    'Phase1çŸ«æ­£åæ ‡ç­¾', 'Phase1çŸ«æ­£åæ ‡ç­¾å€¼', 'Phase1æ˜¯å¦æ­£ç¡®',
                    'CLç–‘ä¼¼å™ªå£°', 'CLå½“å‰æ ‡ç­¾ç½®ä¿¡åº¦', 'CLç›®æ ‡æ ‡ç­¾ç½®ä¿¡åº¦', 'CLé¢„æµ‹æ ‡ç­¾',
                    'AUMåˆ†æ•°',
                    'KNNé‚»å±…æ ‡ç­¾', 'KNNä¸€è‡´æ€§', 'KNNæ˜¯å¦æ”¯æŒå½“å‰æ ‡ç­¾',
                    'çŸ«æ­£åŠ¨ä½œ', 'ç³»ç»Ÿç½®ä¿¡åº¦', 'æ ·æœ¬æƒå€¼', 'å†³ç­–ç†ç”±'
                ]
                # åªé€‰æ‹©å­˜åœ¨çš„åˆ—ï¼Œä¿æŒé¡ºåº
                phase1_final_cols = [col for col in phase1_column_order if col in phase1_df.columns]
                phase1_df = phase1_df[phase1_final_cols]
                
                phase1_df.to_excel(writer, sheet_name='ğŸ“Šé˜¶æ®µ1æŒ‡æ ‡', index=False)
                ws_phase1 = writer.sheets['ğŸ“Šé˜¶æ®µ1æŒ‡æ ‡']
                
                # ä¸ºé˜¶æ®µ1æ·»åŠ é¢œè‰²æ ‡è®°
                for row in range(2, len(phase1_df) + 2):
                    phase1_correct = phase1_df.iloc[row-2]['Phase1æ˜¯å¦æ­£ç¡®']
                    if phase1_correct == 'æ­£ç¡®':
                        for col in range(1, len(phase1_df.columns) + 1):
                            ws_phase1.cell(row=row, column=col).fill = PatternFill(start_color="E8F5E9", end_color="E8F5E9", fill_type="solid")
                    elif phase1_correct == 'é”™è¯¯':
                        for col in range(1, len(phase1_df.columns) + 1):
                            ws_phase1.cell(row=row, column=col).fill = PatternFill(start_color="FFEBEE", end_color="FFEBEE", fill_type="solid")
                
                logger.info(f"  âœ“ é˜¶æ®µ1æŒ‡æ ‡: {len(phase1_df)}ä¸ªæ ·æœ¬ï¼ˆæ‰€æœ‰æ ·æœ¬ï¼‰")
                
                # ========================================
                # Sheet: é˜¶æ®µ2æŒ‡æ ‡ï¼ˆæ‰€æœ‰æ ·æœ¬ï¼ŒåŸºäºPhase1çŸ«æ­£åæ ‡ç­¾é‡æ–°è®­ç»ƒçš„æ¨¡å‹ï¼‰
                # ========================================
                phase2_columns = [
                    'æ ·æœ¬ID', 'æ˜¯å¦å™ªå£°', 'çœŸå®æ ‡ç­¾', 'çœŸå®æ ‡ç­¾å€¼',
                    'å™ªå£°æ ‡ç­¾', 'å™ªå£°æ ‡ç­¾å€¼',  # æ·»åŠ å™ªå£°æ ‡ç­¾ï¼Œæ–¹ä¾¿å¯¹æ¯”
                    'Phase1çŸ«æ­£åæ ‡ç­¾', 'Phase1çŸ«æ­£åæ ‡ç­¾å€¼',  # é˜¶æ®µ2çš„è¾“å…¥æ ‡ç­¾
                    'çŸ«æ­£åæ ‡ç­¾', 'çŸ«æ­£åæ ‡ç­¾å€¼',  # é˜¶æ®µ2çš„è¾“å‡ºæ ‡ç­¾ï¼ˆæœ€ç»ˆæ ‡ç­¾ï¼‰
                    'Phase2_Action',
                    'iter_CLå½“å‰æ ‡ç­¾ç½®ä¿¡åº¦', 'iter_CLç›®æ ‡æ ‡ç­¾ç½®ä¿¡åº¦', 'iter_CLç½®ä¿¡åº¦å·®å€¼',
                    'stage2_CLé¢„æµ‹å™ªå£°', 'stage2_CLç–‘ä¼¼å™ªå£°', 'stage2_CLé¢„æµ‹æ ‡ç­¾', 'stage2_CLé¢„æµ‹æ ‡ç­¾å€¼',  # æ·»åŠ stage2_CLé¢„æµ‹å™ªå£°ï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰å’Œstage2_CLé¢„æµ‹æ ‡ç­¾
                    'stage2_CLå½“å‰æ ‡ç­¾ç½®ä¿¡åº¦', 'stage2_CLç›®æ ‡æ ‡ç­¾ç½®ä¿¡åº¦',  # æ·»åŠ stage2_CLç½®ä¿¡åº¦
                    'stage2_AUMåˆ†æ•°',
                    'stage2_KNNé‚»å±…æ ‡ç­¾', 'stage2_KNNé‚»å±…æ ‡ç­¾å€¼', 'stage2_KNNä¸€è‡´æ€§', 'stage2_KNNæ˜¯å¦æ”¯æŒå½“å‰æ ‡ç­¾',
                    'Phase2æ˜¯å¦æ­£ç¡®',  # æ·»åŠ Phase2æ˜¯å¦æ­£ç¡®ï¼ˆç±»ä¼¼Phase1æ˜¯å¦æ­£ç¡®ï¼‰
                    'çŸ«æ­£æ˜¯å¦æ­£ç¡®', 'ç³»ç»Ÿç½®ä¿¡åº¦', 'æ ·æœ¬æƒå€¼', 'å†³ç­–ç†ç”±'  # æ·»åŠ å†³ç­–ç†ç”±
                ]
                phase2_existing_cols = [col for col in phase2_columns if col in df_reordered.columns]
                phase2_df = df_reordered[phase2_existing_cols].copy()
                
                # æ·»åŠ Phase2æ˜¯å¦æ­£ç¡®åˆ—ï¼ˆåŸºäºæœ€ç»ˆçŸ«æ­£åæ ‡ç­¾ï¼‰
                if 'çŸ«æ­£åæ ‡ç­¾å€¼' in phase2_df.columns and 'çœŸå®æ ‡ç­¾å€¼' in phase2_df.columns:
                    phase2_df['Phase2æ˜¯å¦æ­£ç¡®'] = (phase2_df['çŸ«æ­£åæ ‡ç­¾å€¼'] == phase2_df['çœŸå®æ ‡ç­¾å€¼']).apply(lambda x: 'æ­£ç¡®' if x else 'é”™è¯¯')
                
                # ä¿®å¤é˜¶æ®µ2çš„"KNNæ˜¯å¦æ”¯æŒå½“å‰æ ‡ç­¾"ï¼šåº”è¯¥åŸºäºPhase1çŸ«æ­£åçš„æ ‡ç­¾åˆ¤æ–­
                if 'stage2_KNNé‚»å±…æ ‡ç­¾å€¼' in phase2_df.columns and 'Phase1çŸ«æ­£åæ ‡ç­¾å€¼' in phase2_df.columns:
                    phase2_df['stage2_KNNæ˜¯å¦æ”¯æŒå½“å‰æ ‡ç­¾'] = (
                        phase2_df['stage2_KNNé‚»å±…æ ‡ç­¾å€¼'] == phase2_df['Phase1çŸ«æ­£åæ ‡ç­¾å€¼']
                    ).apply(lambda x: 'æ˜¯' if x else 'å¦')
                
                # å°†stage2_CLç–‘ä¼¼å™ªå£°ä»æ•°å€¼è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ï¼ˆ'æ˜¯'/'å¦'ï¼‰ï¼Œä¸é˜¶æ®µ1ä¿æŒä¸€è‡´
                if 'stage2_CLç–‘ä¼¼å™ªå£°' in phase2_df.columns:
                    phase2_df['stage2_CLé¢„æµ‹å™ªå£°'] = phase2_df['stage2_CLç–‘ä¼¼å™ªå£°'].apply(
                        lambda x: 'æ˜¯' if (x == 1 or x == True) else ('å¦' if (x == 0 or x == False) else 'N/A')
                    )
                
                # é‡æ–°æ’åˆ—åˆ—é¡ºåºï¼Œåªé€‰æ‹©å­˜åœ¨çš„åˆ—
                phase2_column_order = [
                    'æ ·æœ¬ID', 'æ˜¯å¦å™ªå£°', 'çœŸå®æ ‡ç­¾', 'çœŸå®æ ‡ç­¾å€¼',
                    'å™ªå£°æ ‡ç­¾', 'å™ªå£°æ ‡ç­¾å€¼',
                    'Phase1çŸ«æ­£åæ ‡ç­¾', 'Phase1çŸ«æ­£åæ ‡ç­¾å€¼',
                    'çŸ«æ­£åæ ‡ç­¾', 'çŸ«æ­£åæ ‡ç­¾å€¼',
                    'Phase2_Action', 'Phase2æ˜¯å¦æ­£ç¡®',
                    'iter_CLå½“å‰æ ‡ç­¾ç½®ä¿¡åº¦', 'iter_CLç›®æ ‡æ ‡ç­¾ç½®ä¿¡åº¦', 'iter_CLç½®ä¿¡åº¦å·®å€¼',
                    'stage2_CLé¢„æµ‹å™ªå£°', 'stage2_CLç–‘ä¼¼å™ªå£°', 'stage2_CLé¢„æµ‹æ ‡ç­¾', 'stage2_CLé¢„æµ‹æ ‡ç­¾å€¼',
                    'stage2_CLå½“å‰æ ‡ç­¾ç½®ä¿¡åº¦', 'stage2_CLç›®æ ‡æ ‡ç­¾ç½®ä¿¡åº¦',
                    'stage2_AUMåˆ†æ•°',
                    'stage2_KNNé‚»å±…æ ‡ç­¾', 'stage2_KNNé‚»å±…æ ‡ç­¾å€¼', 'stage2_KNNä¸€è‡´æ€§', 'stage2_KNNæ˜¯å¦æ”¯æŒå½“å‰æ ‡ç­¾',
                    'çŸ«æ­£æ˜¯å¦æ­£ç¡®', 'ç³»ç»Ÿç½®ä¿¡åº¦', 'æ ·æœ¬æƒå€¼', 'å†³ç­–ç†ç”±'
                ]
                phase2_final_cols = [col for col in phase2_column_order if col in phase2_df.columns]
                phase2_df = phase2_df[phase2_final_cols]
                
                # æ˜¾ç¤ºæ‰€æœ‰æ ·æœ¬ï¼ˆé˜¶æ®µ2æŒ‡æ ‡åŸºäºé‡æ–°è®­ç»ƒçš„æ¨¡å‹ï¼‰
                if len(phase2_df) > 0:
                    phase2_df.to_excel(writer, sheet_name='ğŸ“Šé˜¶æ®µ2æŒ‡æ ‡', index=False)
                    ws_phase2 = writer.sheets['ğŸ“Šé˜¶æ®µ2æŒ‡æ ‡']
                    
                    # ä¸ºé˜¶æ®µ2æ·»åŠ é¢œè‰²æ ‡è®°ï¼ˆåŸºäºPhase2æ˜¯å¦æ­£ç¡®ï¼‰
                    for row in range(2, len(phase2_df) + 2):
                        phase2_correct = phase2_df.iloc[row-2].get('Phase2æ˜¯å¦æ­£ç¡®', '')
                        if phase2_correct == 'æ­£ç¡®':
                            for col in range(1, len(phase2_df.columns) + 1):
                                ws_phase2.cell(row=row, column=col).fill = PatternFill(start_color="E8F5E9", end_color="E8F5E9", fill_type="solid")
                        elif phase2_correct == 'é”™è¯¯':
                            for col in range(1, len(phase2_df.columns) + 1):
                                ws_phase2.cell(row=row, column=col).fill = PatternFill(start_color="FFEBEE", end_color="FFEBEE", fill_type="solid")
                    
                    logger.info(f"  âœ“ é˜¶æ®µ2æŒ‡æ ‡: {len(phase2_df)}ä¸ªæ ·æœ¬ï¼ˆæ‰€æœ‰æ ·æœ¬ï¼ŒåŸºäºé‡æ–°è®­ç»ƒçš„æ¨¡å‹ï¼‰")
                else:
                    logger.info(f"  âœ“ é˜¶æ®µ2æŒ‡æ ‡: 0ä¸ªæ ·æœ¬ï¼ˆæ— æ•°æ®ï¼‰")
            
            logger.info(f"âœ“ Excelæ–‡ä»¶å·²ä¿å­˜ï¼ˆå¤šsheetï¼Œå¸¦é¢œè‰²æ ‡è®°ï¼‰: {excel_path}")
            logger.info(f"  åŒ…å« {len(pd.ExcelFile(excel_path).sheet_names)} ä¸ªå·¥ä½œè¡¨")
            logger.info(f"  é¢œè‰²è¯´æ˜: ç»¿è‰²=æ­£ç¡®å¤„ç†, çº¢è‰²=é”™è¯¯å¤„ç†, ç°è‰²=å·²ä¸¢å¼ƒ")
            
            # ========================================
            # ç”ŸæˆExcelè¯¦ç»†å¯è¯»æ–‡æœ¬æ–‡ä»¶ï¼ˆä¾›AIåˆ†æä½¿ç”¨ï¼‰
            # ========================================
            try:
                excel_text_output_path = excel_path.replace('.xlsx', '_excel_readable.txt')
                with open(excel_text_output_path, 'w', encoding='utf-8') as f:
                    f.write("=" * 80 + "\n")
                    f.write(f"æ ·æœ¬åˆ†ææ•°æ® - Excelè¯¦ç»†å†…å®¹ï¼ˆå™ªå£°ç‡: {noise_pct}%ï¼‰\n")
                    f.write("=" * 80 + "\n\n")
                    
                    # è¯»å–æ‰€æœ‰sheet
                    excel_file = pd.ExcelFile(excel_path)
                    
                    for sheet_name in excel_file.sheet_names:
                        f.write("\n" + "=" * 80 + "\n")
                        f.write(f"Sheet: {sheet_name}\n")
                        f.write("=" * 80 + "\n\n")
                        
                        sheet_df = pd.read_excel(excel_path, sheet_name=sheet_name)
                        
                        # å¦‚æœæ˜¯ç»Ÿè®¡ç±»sheetï¼Œä½¿ç”¨æ›´å‹å¥½çš„æ ¼å¼
                        if 'ç»Ÿè®¡' in sheet_name or 'æ€»è§ˆ' in sheet_name:
                            f.write(sheet_df.to_string(index=False))
                            f.write("\n\n")
                        else:
                            # å¯¹äºæ•°æ®ç±»sheetï¼Œé™åˆ¶æ˜¾ç¤ºè¡Œæ•°ï¼Œé¿å…æ–‡ä»¶è¿‡å¤§
                            max_rows = 100
                            if len(sheet_df) > max_rows:
                                f.write(f"ï¼ˆä»…æ˜¾ç¤ºå‰{max_rows}è¡Œï¼Œå…±{len(sheet_df)}è¡Œï¼‰\n\n")
                                display_df = sheet_df.head(max_rows)
                            else:
                                display_df = sheet_df
                            
                            # é€‰æ‹©å…³é”®åˆ—æ˜¾ç¤º
                            key_columns = [
                                'æ ·æœ¬ID', 'çŸ«æ­£æ˜¯å¦æ­£ç¡®', 'é”™è¯¯ç±»å‹', 'Tieråˆ†çº§', 
                                'æ˜¯å¦å™ªå£°', 'çœŸå®æ ‡ç­¾', 'å™ªå£°æ ‡ç­¾', 'çŸ«æ­£åæ ‡ç­¾',
                                'çŸ«æ­£åŠ¨ä½œ', 'ç³»ç»Ÿç½®ä¿¡åº¦', 'æ ·æœ¬æƒå€¼'
                            ]
                            available_key_cols = [col for col in key_columns if col in display_df.columns]
                            other_cols = [col for col in display_df.columns if col not in available_key_cols]
                            
                            # ä¼˜å…ˆæ˜¾ç¤ºå…³é”®åˆ—
                            if available_key_cols:
                                display_df_key = display_df[available_key_cols]
                                f.write("ã€å…³é”®ä¿¡æ¯ã€‘\n")
                                f.write(display_df_key.to_string(index=False))
                                f.write("\n\n")
                            
                            # å¦‚æœæœ‰å…¶ä»–åˆ—ä¸”è¡Œæ•°ä¸å¤šï¼Œä¹Ÿæ˜¾ç¤º
                            if len(sheet_df) <= 50 and other_cols:
                                display_df_other = display_df[other_cols]
                                f.write("ã€è¯¦ç»†ä¿¡æ¯ã€‘\n")
                                f.write(display_df_other.to_string(index=False))
                                f.write("\n\n")
                    
                    f.write("\n" + "=" * 80 + "\n")
                    f.write("æ–‡ä»¶ç»“æŸ\n")
                    f.write("=" * 80 + "\n")
                
                logger.info(f"âœ“ Excelè¯¦ç»†æ–‡æœ¬æ–‡ä»¶å·²ä¿å­˜: {excel_text_output_path}")
                logger.info(f"  è¯´æ˜: æ­¤æ–‡ä»¶åŒ…å«Excelæ‰€æœ‰sheetçš„å†…å®¹ï¼Œæ–¹ä¾¿AIç›´æ¥åˆ†æ")
            except Exception as e:
                logger.warning(f"âš  Excelè¯¦ç»†æ–‡æœ¬æ–‡ä»¶ç”Ÿæˆå¤±è´¥: {e}")
                import traceback
                logger.warning(traceback.format_exc())
        except Exception as e:
            logger.warning(f"âš  Excelæ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")
            import traceback
            logger.warning(traceback.format_exc())
    else:
        logger.info("  æç¤º: å®‰è£…openpyxlå¯ç”Ÿæˆå¤šsheet Excelæ–‡ä»¶ (pip install openpyxl)")
    
    # ========================================
    # æ–°æ ¼å¼ï¼šæŒ‰ç­–ç•¥åˆ†ç»„çš„æ—¥å¿—æ€»ç»“
    # ========================================
    
    # æŒ‰Tieråˆ†ç»„æ ·æœ¬
    tier_groups = {}
    for i, row in df.iterrows():
        tier = row['Tieråˆ†çº§']
        if tier not in tier_groups:
            tier_groups[tier] = {'correct': [], 'incorrect': [], 'dropped': []}
        
        if row['çŸ«æ­£æ˜¯å¦æ­£ç¡®'] == 'æ­£ç¡®':
            tier_groups[tier]['correct'].append(row)
        elif row['çŸ«æ­£æ˜¯å¦æ­£ç¡®'] == 'é”™è¯¯':
            tier_groups[tier]['incorrect'].append(row)
        else:  # ä¸é€‚ç”¨(å·²ä¸¢å¼ƒ)
            tier_groups[tier]['dropped'].append(row)
    
    # å®šä¹‰Tieré¡ºåºå’Œè§’è‰²è¯´æ˜ï¼ˆCL+AUM+KNN æ–°ç­–ç•¥ï¼‰
    tier_order = [
        'Keep',
        'Flip',
        'Reweight',
        'Drop',
        'Dropped'
    ]
    
    role_map = {
        'Keep': 'Keepï¼šCLä¸AUMå‡æ”¯æŒï¼Œä¿æŒåŸæ ‡ç­¾ï¼ˆé«˜æƒé‡ï¼‰',
        'Flip': 'Flipï¼šç–‘ä¼¼å™ªå£°æ ·æœ¬ï¼ŒKNN å¼ºæ”¯æŒç¿»è½¬ï¼ˆé«˜æƒé‡ï¼‰',
        'Reweight': 'Reweightï¼šä¸ç¡®å®šæ ·æœ¬ï¼Œä¿æŒä½†é™æƒ',
        'Drop': 'Dropï¼šç–‘ä¼¼å™ªå£°æ ·æœ¬ä¸”ä¸æ»¡è¶³ç¿»è½¬æ¡ä»¶ï¼ˆä¸¢å¼ƒï¼‰',
        'Dropped': 'Droppedï¼šå·²ä¸¢å¼ƒæ ·æœ¬'
    }
    
    # ç­–ç•¥è¯¦ç»†è¯´æ˜
    strategy_details = {
        'Keep': {
            'phase': 'Phase 1: Keep',
            'condition': 'CLâ‰¥é˜ˆå€¼ ä¸” AUMâ‰¥é˜ˆå€¼',
            'action': 'ä¿æŒåŸæ ‡ç­¾',
            'weight': '1.0'
        },
        'Flip': {
            'phase': 'Phase 2: Flip',
            'condition': 'å€™é€‰å™ªå£°( CL<é˜ˆå€¼ æˆ– AUM<é˜ˆå€¼ ) ä¸” KNNä¸€è‡´æ€§>é˜ˆå€¼ ä¸” KNNæŠ•ç¥¨æŒ‡å‘ç›®æ ‡ç±»',
            'action': 'ç¿»è½¬æ ‡ç­¾',
            'weight': '0.9'
        },
        'Reweight': {
            'phase': 'Phase 2: Reweight',
            'condition': 'å€™é€‰å™ªå£°ä½†ä¸æ»¡è¶³ç¿»è½¬æ¡ä»¶ï¼Œä¿æŒä½†é™æƒ',
            'action': 'ä¿æŒåŸæ ‡ç­¾ä½†é™æƒ',
            'weight': '0.5'
        },
        'Drop': {
            'phase': 'Phase 3: Drop',
            'condition': 'å€™é€‰å™ªå£°ä¸”ä¸æ»¡è¶³ç¿»è½¬æ¡ä»¶ï¼ˆä»…å½“å¯ç”¨Dropï¼‰',
            'action': 'ä¸¢å¼ƒæ ·æœ¬',
            'weight': '0.0'
        },
        'Dropped': {
            'phase': 'Phase 3: Drop',
            'condition': 'å·²ä¸¢å¼ƒæ ·æœ¬',
            'action': 'ä¸¢å¼ƒæ ·æœ¬',
            'weight': '0.0'
        }
    }
    
    # å°†æ–‡ä»¶åç¼€æ”¹ä¸º.logä»¥æ”¯æŒæ›´å¥½çš„è¡¨æ ¼æ ¼å¼
    log_path = save_path.replace('.txt', '.log')
    
    # Save to formatted log file (æ–°æ ¼å¼ - è¡¨æ ¼å½¢å¼)
    with open(log_path, 'w', encoding='utf-8') as f:
        f.write("="*120 + "\n")
        f.write("MEDAL-Lite æ ‡ç­¾çŸ«æ­£åˆ†æ - ç­–ç•¥åˆ†ç»„æ—¥å¿—æŠ¥å‘Š\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("="*120 + "\n\n")
        
        # Summary statistics
        f.write("ğŸ“Š æ€»ä½“ç»Ÿè®¡\n")
        f.write("-"*120 + "\n")
        f.write(f"æ€»æ ·æœ¬æ•°: {n_samples}\n")
        f.write(f"  æ­£å¸¸æµé‡:  {(y_true == 0).sum()} ({100*(y_true == 0).sum()/n_samples:.1f}%)\n")
        f.write(f"  æ¶æ„æµé‡:  {(y_true == 1).sum()} ({100*(y_true == 1).sum()/n_samples:.1f}%)\n\n")
        
        f.write(f"å™ªå£°æ³¨å…¥ç‡: {100*noise_mask.sum()/n_samples:.1f}%\n")
        f.write(f"  å™ªå£°æ ·æœ¬æ•°: {noise_mask.sum()}\n\n")
        
        action_mask = results['action_mask']
        f.write(f"çŸ«æ­£åŠ¨ä½œç»Ÿè®¡:\n")
        f.write(f"  ä¿æŒ:     {(action_mask == 0).sum()} ({100*(action_mask == 0).sum()/n_samples:.1f}%)\n")
        f.write(f"  ç¿»è½¬:     {(action_mask == 1).sum()} ({100*(action_mask == 1).sum()/n_samples:.1f}%)\n")
        f.write(f"  ä¸¢å¼ƒ:     {(action_mask == 2).sum()} ({100*(action_mask == 2).sum()/n_samples:.1f}%)\n")
        f.write(f"  é‡åŠ æƒ:   {(action_mask == 3).sum()} ({100*(action_mask == 3).sum()/n_samples:.1f}%)\n\n")
        
        # Correction accuracy (excluding dropped samples)
        keep_mask = action_mask != 2
        corrected_labels = results['clean_labels'][keep_mask]
        true_labels_kept = y_true[keep_mask]
        correction_accuracy = (corrected_labels == true_labels_kept).mean()
        f.write(f"çŸ«æ­£å‡†ç¡®ç‡ (ä¸å«ä¸¢å¼ƒæ ·æœ¬): {correction_accuracy*100:.2f}%\n")
        f.write(f"  æ­£ç¡®: {(corrected_labels == true_labels_kept).sum()}\n")
        f.write(f"  é”™è¯¯: {(corrected_labels != true_labels_kept).sum()}\n\n")
        
        f.write("="*120 + "\n\n")
        
        # æŒ‰ç­–ç•¥åˆ†ç»„çš„è¯¦ç»†åˆ†æ
        f.write("ğŸ“‹ ç­–ç•¥åˆ†ç»„è¯¦ç»†åˆ†æï¼ˆæŒ‰é˜¶æ®µå’Œè·¯å¾„ï¼‰\n")
        f.write("="*120 + "\n\n")
        
        # æŒ‰Phaseåˆ†ç»„
        phase_groups = {
            'Phase 1: æ ¸å¿ƒä¸¥é€‰': [],
            'Phase 2: åˆ†çº§æŒ½æ•‘': [],
            'Phase 3: é”šç‚¹æ‹¯æ•‘': [],
            'å·²ä¸¢å¼ƒ': []
        }
        
        for tier in tier_order:
            if tier not in tier_groups or len(tier_groups[tier]['correct']) + len(tier_groups[tier]['incorrect']) + len(tier_groups[tier]['dropped']) == 0:
                continue
            
            if 'Tier 1' in tier:
                phase_groups['Phase 1: æ ¸å¿ƒä¸¥é€‰'].append(tier)
            elif 'Tier 2' in tier or 'Tier 3' in tier or 'Tier 4' in tier:
                phase_groups['Phase 2: åˆ†çº§æŒ½æ•‘'].append(tier)
            elif 'Tier 5' in tier:
                phase_groups['Phase 3: é”šç‚¹æ‹¯æ•‘'].append(tier)
            elif 'Dropped' in tier:
                phase_groups['å·²ä¸¢å¼ƒ'].append(tier)
        
        # æŒ‰Phaseè¾“å‡º
        for phase_name, tiers in phase_groups.items():
            if not tiers:
                continue
            
            f.write("\n" + "="*120 + "\n")
            f.write(f"ã€{phase_name}ã€‘\n")
            f.write("="*120 + "\n\n")
            
            for tier in tiers:
                if tier not in tier_groups:
                    continue
                
                correct_samples = tier_groups[tier]['correct']
                incorrect_samples = tier_groups[tier]['incorrect']
                dropped_samples = tier_groups[tier]['dropped']
                total_samples = len(correct_samples) + len(incorrect_samples) + len(dropped_samples)
                
                if total_samples == 0:
                    continue
                
                role = role_map.get(tier, '')
                
                # è®¡ç®—å‡†ç¡®ç‡ï¼ˆä¸åŒ…æ‹¬ä¸¢å¼ƒçš„æ ·æœ¬ï¼‰
                valid_samples = len(correct_samples) + len(incorrect_samples)
                accuracy = len(correct_samples) / valid_samples * 100 if valid_samples > 0 else 0
                
                # è·å–è¯¥Tierçš„æƒé‡
                if correct_samples:
                    weight = correct_samples[0]['æ ·æœ¬æƒå€¼']
                elif incorrect_samples:
                    weight = incorrect_samples[0]['æ ·æœ¬æƒå€¼']
                elif dropped_samples:
                    weight = dropped_samples[0]['æ ·æœ¬æƒå€¼']
                else:
                    weight = 'N/A'
                
                f.write(f"â”Œâ”€ ã€ç­–ç•¥ã€‘{tier}\n")
                f.write(f"â”‚  è§’è‰²å®šä½: {role}\n")
                
                # è¾“å‡ºç­–ç•¥è¯¦ç»†è¯´æ˜
                if tier in strategy_details:
                    details = strategy_details[tier]
                    f.write(f"â”‚  å†³ç­–æ¡ä»¶: {details['condition']}\n")
                    f.write(f"â”‚  æ‰§è¡ŒåŠ¨ä½œ: {details['action']}\n")
                    f.write(f"â”‚  æ ·æœ¬æƒé‡: {details['weight']}\n")
                
                f.write(f"â”‚  æ ·æœ¬ç»Ÿè®¡: æ€»æ•°={total_samples} | å®é™…æƒé‡={weight} | å‡†ç¡®ç‡={accuracy:.1f}%\n")
                f.write(f"â”‚             æ­£ç¡®={len(correct_samples)} | é”™è¯¯={len(incorrect_samples)} | ä¸¢å¼ƒ={len(dropped_samples)}\n")
                f.write(f"â””â”€" + "-"*116 + "\n\n")
                
                # æ­£ç¡®å¤„ç†çš„æ ·æœ¬ - è¡¨æ ¼å½¢å¼
                if correct_samples:
                    f.write(f"  âœ“ æ­£ç¡®å¤„ç†çš„æ ·æœ¬ ({len(correct_samples)}ä¸ª):\n")
                    f.write("  " + "-"*150 + "\n")
                    # è¡¨å¤´ - æ·»åŠ æ›´å¤šå…³é”®æŒ‡æ ‡
                    f.write(f"  {'æ ·æœ¬ID':>8s} | {'çœŸå®':>4s} | {'å™ªå£°':>4s} | {'çŸ«æ­£å':>6s} | "
                           f"{'åŸCL':>6s} | {'iter_CL':>7s} | {'iter_T':>7s} | "
                           f"{'anchor':>7s} | {'orig_KNN':>8s} | å†³ç­–ç†ç”±\n")
                    f.write("  " + "-"*150 + "\n")
                    
                    # æ˜¾ç¤ºå‰10ä¸ªæ ·æœ¬
                    for row in correct_samples[:10]:
                        # åŸå§‹CLç½®ä¿¡åº¦
                        orig_cl = row['CLæ­£å¸¸æ¦‚ç‡'] if row['å™ªå£°æ ‡ç­¾']=='æ­£å¸¸' else row['CLæ¶æ„æ¦‚ç‡']
                        # è¿­ä»£CL
                        iter_cl_current = row.get('iter_CLå½“å‰æ ‡ç­¾ç½®ä¿¡åº¦', 'N/A')
                        iter_cl_target = row.get('iter_CLç›®æ ‡æ ‡ç­¾ç½®ä¿¡åº¦', 'N/A')
                        # é”šç‚¹KNN
                        anchor_cons = row.get('anchor_KNNä¸€è‡´æ€§', 'N/A')
                        # åŸå§‹KNN
                        orig_knn = row.get('KNNä¸€è‡´æ€§', 'N/A')
                        
                        reason_short = row['å†³ç­–ç†ç”±'][:40] + '...' if len(row['å†³ç­–ç†ç”±']) > 40 else row['å†³ç­–ç†ç”±']
                        f.write(f"  {row['æ ·æœ¬ID']:>8d} | {str(row['çœŸå®æ ‡ç­¾']):>4} | {str(row['å™ªå£°æ ‡ç­¾']):>4} | "
                               f"{str(row['çŸ«æ­£åæ ‡ç­¾']):>6} | "
                               f"{str(orig_cl)[:6]:>6} | {str(iter_cl_current)[:7]:>7} | {str(iter_cl_target)[:7]:>7} | "
                               f"{str(anchor_cons)[:7]:>7} | {str(orig_knn)[:8]:>8} | {reason_short}\n")
                    
                    if len(correct_samples) > 10:
                        f.write(f"  ... è¿˜æœ‰ {len(correct_samples) - 10} ä¸ªæ­£ç¡®æ ·æœ¬ï¼ˆè¯¦è§Excelæ–‡ä»¶ï¼‰\n")
                    f.write("\n")
                
                # é”™è¯¯å¤„ç†çš„æ ·æœ¬ - è¡¨æ ¼å½¢å¼ï¼ˆæ˜¾ç¤ºæ‰€æœ‰ï¼‰
                if incorrect_samples:
                    f.write(f"  âœ— é”™è¯¯å¤„ç†çš„æ ·æœ¬ ({len(incorrect_samples)}ä¸ª):\n")
                    f.write("  " + "-"*150 + "\n")
                    # è¡¨å¤´
                    f.write(f"  {'æ ·æœ¬ID':>8s} | {'çœŸå®':>4s} | {'å™ªå£°':>4s} | {'çŸ«æ­£å':>6s} | "
                           f"{'åŸCL':>6s} | {'iter_CL':>7s} | {'iter_T':>7s} | "
                           f"{'anchor':>7s} | {'orig_KNN':>8s} | é”™è¯¯åŸå› \n")
                    f.write("  " + "-"*150 + "\n")
                    
                    for row in incorrect_samples:
                        # åŸå§‹CLç½®ä¿¡åº¦
                        orig_cl = row['CLæ­£å¸¸æ¦‚ç‡'] if row['å™ªå£°æ ‡ç­¾']=='æ­£å¸¸' else row['CLæ¶æ„æ¦‚ç‡']
                        # è¿­ä»£CL
                        iter_cl_current = row.get('iter_CLå½“å‰æ ‡ç­¾ç½®ä¿¡åº¦', 'N/A')
                        iter_cl_target = row.get('iter_CLç›®æ ‡æ ‡ç­¾ç½®ä¿¡åº¦', 'N/A')
                        # é”šç‚¹KNN
                        anchor_cons = row.get('anchor_KNNä¸€è‡´æ€§', 'N/A')
                        # åŸå§‹KNN
                        orig_knn = row.get('KNNä¸€è‡´æ€§', 'N/A')
                        
                        error_reason = f"çœŸå®={row['çœŸå®æ ‡ç­¾']}, çŸ«æ­£ä¸º{row['çŸ«æ­£åæ ‡ç­¾']}"
                        f.write(f"  {row['æ ·æœ¬ID']:>8d} | {str(row['çœŸå®æ ‡ç­¾']):>4} | {str(row['å™ªå£°æ ‡ç­¾']):>4} | "
                               f"{str(row['çŸ«æ­£åæ ‡ç­¾']):>6} | "
                               f"{str(orig_cl)[:6]:>6} | {str(iter_cl_current)[:7]:>7} | {str(iter_cl_target)[:7]:>7} | "
                               f"{str(anchor_cons)[:7]:>7} | {str(orig_knn)[:8]:>8} | {error_reason}\n")
                    f.write("\n")
                
                # ä¸¢å¼ƒçš„æ ·æœ¬ - è¡¨æ ¼å½¢å¼
                if dropped_samples:
                    f.write(f"  ğŸ—‘ ä¸¢å¼ƒçš„æ ·æœ¬ ({len(dropped_samples)}ä¸ª):\n")
                    f.write("  " + "-"*150 + "\n")
                    # è¡¨å¤´
                    f.write(f"  {'æ ·æœ¬ID':>8s} | {'çœŸå®':>4s} | {'å™ªå£°':>4s} | {'æ˜¯å¦å™ªå£°':>8s} | "
                           f"{'åŸCL':>6s} | {'iter_CL':>7s} | {'iter_T':>7s} | "
                           f"{'anchor':>7s} | {'orig_KNN':>8s} | ä¸¢å¼ƒåŸå› \n")
                    f.write("  " + "-"*150 + "\n")
                    
                    # æ˜¾ç¤ºå‰10ä¸ªä¸¢å¼ƒæ ·æœ¬
                    for row in dropped_samples[:10]:
                        # åŸå§‹CLç½®ä¿¡åº¦
                        orig_cl = row['CLæ­£å¸¸æ¦‚ç‡'] if row['å™ªå£°æ ‡ç­¾']=='æ­£å¸¸' else row['CLæ¶æ„æ¦‚ç‡']
                        # è¿­ä»£CL
                        iter_cl_current = row.get('iter_CLå½“å‰æ ‡ç­¾ç½®ä¿¡åº¦', 'N/A')
                        iter_cl_target = row.get('iter_CLç›®æ ‡æ ‡ç­¾ç½®ä¿¡åº¦', 'N/A')
                        # é”šç‚¹KNN
                        anchor_cons = row.get('anchor_KNNä¸€è‡´æ€§', 'N/A')
                        # åŸå§‹KNN
                        orig_knn = row.get('KNNä¸€è‡´æ€§', 'N/A')
                        
                        drop_reason = "CLæä½" if 'CL' in row['å†³ç­–ç†ç”±'] else "æ— æ³•æŒ½æ•‘"
                        f.write(f"  {row['æ ·æœ¬ID']:>8d} | {str(row['çœŸå®æ ‡ç­¾']):>4} | {str(row['å™ªå£°æ ‡ç­¾']):>4} | "
                               f"{str(row['æ˜¯å¦å™ªå£°']):>8} | "
                               f"{str(orig_cl)[:6]:>6} | {str(iter_cl_current)[:7]:>7} | {str(iter_cl_target)[:7]:>7} | "
                               f"{str(anchor_cons)[:7]:>7} | {str(orig_knn)[:8]:>8} | {drop_reason}\n")
                    
                    if len(dropped_samples) > 10:
                        f.write(f"  ... è¿˜æœ‰ {len(dropped_samples) - 10} ä¸ªä¸¢å¼ƒæ ·æœ¬ï¼ˆè¯¦è§Excelæ–‡ä»¶ï¼‰\n")
                    f.write("\n")
                
                f.write("\n")
        
        # æ·»åŠ è¯¯æ€åˆ†æï¼ˆè¢«é”™è¯¯ä¸¢å¼ƒçš„å¹²å‡€æ ·æœ¬ï¼‰
        f.write("="*120 + "\n")
        f.write("âš ï¸  è¯¯æ€åˆ†æ - è¢«é”™è¯¯ä¸¢å¼ƒçš„å¹²å‡€æ ·æœ¬\n")
        f.write("="*120 + "\n\n")
        
        # æ‰¾å‡ºæ‰€æœ‰è¢«ä¸¢å¼ƒä½†å®é™…æ˜¯å¹²å‡€çš„æ ·æœ¬
        false_drops = []
        for i, row in df.iterrows():
            if row['çŸ«æ­£åŠ¨ä½œ'] == 'ä¸¢å¼ƒ' and row['æ˜¯å¦å™ªå£°'] == 'å¦':
                false_drops.append(row)
        
        if false_drops:
            f.write(f"ğŸ“Š è¢«è¯¯æ€çš„å¹²å‡€æ ·æœ¬è¯¦æƒ… (å…±{len(false_drops)}ä¸ª):\n")
            f.write("-"*120 + "\n")
            f.write(f"  {'æ ·æœ¬ID':>8s} | {'æ ‡ç­¾':>4s} | {'iter_CL':>7s} | {'iter_CL_T':>9s} | {'anchor':>7s} | {'orig_KNN':>8s} | åŸå› \n")
            f.write("-"*120 + "\n")
            
            for row in false_drops:
                # ä½¿ç”¨iter_CLå½“å‰æ ‡ç­¾ç½®ä¿¡åº¦ï¼ˆè¿™æ‰æ˜¯Dropåˆ¤æ–­çš„ä¾æ®ï¼‰
                iter_cl_current = row.get('iter_CLå½“å‰æ ‡ç­¾ç½®ä¿¡åº¦', 'N/A')
                iter_cl_target = row.get('iter_CLç›®æ ‡æ ‡ç­¾ç½®ä¿¡åº¦', 'N/A')
                anchor_cons = row.get('anchor_KNNä¸€è‡´æ€§', 'N/A')
                orig_knn_cons = row.get('KNNä¸€è‡´æ€§', 'N/A')
                
                f.write(f"  {row['æ ·æœ¬ID']:>8d} | {str(row['çœŸå®æ ‡ç­¾']):>4} | {str(iter_cl_current):>7} | "
                       f"{str(iter_cl_target):>9} | {str(anchor_cons):>7} | {str(orig_knn_cons):>8} | CLæä½\n")
        else:
            f.write("âœ“ æ²¡æœ‰è¯¯æ€å¹²å‡€æ ·æœ¬\n")
        
        f.write("\n")
        
        # æ·»åŠ æ¼ç½‘ä¹‹é±¼åˆ†æï¼ˆæœªè¢«æ£€æµ‹åˆ°çš„å™ªå£°æ ·æœ¬ï¼‰
        f.write("="*120 + "\n")
        f.write("ğŸŸ æ¼ç½‘ä¹‹é±¼åˆ†æ - æœªè¢«çŸ«æ­£çš„å™ªå£°æ ·æœ¬\n")
        f.write("="*120 + "\n\n")
        
        # æ‰¾å‡ºæ‰€æœ‰æ˜¯å™ªå£°ä½†æœªè¢«ç¿»è½¬çš„æ ·æœ¬
        missed_noise = []
        for i, row in df.iterrows():
            if row['æ˜¯å¦å™ªå£°'] == 'æ˜¯' and row['çŸ«æ­£åŠ¨ä½œ'] != 'ç¿»è½¬':
                missed_noise.append(row)
        
        if missed_noise:
            f.write(f"ğŸ“Š æ¼ç½‘çš„å™ªå£°æ ·æœ¬è¯¦æƒ… (å…±{len(missed_noise)}ä¸ª):\n")
            f.write("-"*120 + "\n")
            f.write(f"  {'æ ·æœ¬ID':>8s} | {'çœŸå®':>4s} | {'å™ªå£°':>4s} | {'åŠ¨ä½œ':>4s} | "
                   f"{'CLç½®ä¿¡':>7s} | {'AUM':>7s} | {'KNNä¸€è‡´':>7s} | åŸå› \n")
            f.write("-"*120 + "\n")
            
            for row in missed_noise[:20]:  # æ˜¾ç¤ºå‰20ä¸ª
                cl_conf = row['CLæ­£å¸¸æ¦‚ç‡'] if row['å™ªå£°æ ‡ç­¾']=='æ­£å¸¸' else row['CLæ¶æ„æ¦‚ç‡']
                reason = "æœªè¢«CLæ£€æµ‹" if row['CLç–‘ä¼¼å™ªå£°'] == 'å¦' else "KNNæ”¯æŒé”™è¯¯æ ‡ç­¾"
                aum_val = row.get('AUMåˆ†æ•°', 'N/A')
                f.write(f"  {row['æ ·æœ¬ID']:>8d} | {str(row['çœŸå®æ ‡ç­¾']):>4} | {str(row['å™ªå£°æ ‡ç­¾']):>4} | "
                       f"{str(row['çŸ«æ­£åŠ¨ä½œ']):>4} | {str(cl_conf):>7} | {str(aum_val)[:7]:>7} | "
                       f"{str(row['KNNä¸€è‡´æ€§']):>7} | {reason}\n")
            
            if len(missed_noise) > 20:
                f.write(f"  ... è¿˜æœ‰ {len(missed_noise) - 20} ä¸ªæ¼ç½‘æ ·æœ¬ï¼ˆè¯¦è§CSVæ–‡ä»¶ï¼‰\n")
        else:
            f.write("âœ“ æ‰€æœ‰å™ªå£°æ ·æœ¬éƒ½è¢«æˆåŠŸæ£€æµ‹\n")
        
        f.write("\n")
        f.write("="*120 + "\n")
        f.write("æŠ¥å‘Šç»“æŸ\n")
        f.write("="*120 + "\n")
    
    logger.info(f"âœ“ ç­–ç•¥åˆ†ç»„æ—¥å¿—å·²ä¿å­˜: {log_path}")
    logger.info(f"  å·²åˆ†ææ ·æœ¬æ€»æ•°: {n_samples}")
    logger.info(f"  ç­–ç•¥åˆ†ç»„æ•°: {len([t for t in tier_order if t in tier_groups])}")
    
    return df


def plot_feature_distributions(results, y_true, noise_mask, save_dir, logger):
    """
    Generate feature distribution plots for clean, noisy, and corrected data
    
    Args:
        results: dict from run_hybrid_court_with_detailed_tracking
        y_true: (N,) ground truth labels
        noise_mask: (N,) boolean array indicating noise
        save_dir: directory to save plots
        logger: logger
    """
    logger.info("")
    logger.info("="*70)
    logger.info("Generating Feature Distribution Plots")
    logger.info("="*70)
    
    features = results['features']
    noisy_labels = results['noisy_labels']
    clean_labels = results['clean_labels']
    action_mask = results['action_mask']
    
    os.makedirs(save_dir, exist_ok=True)
    
    # Use t-SNE for dimensionality reduction
    logger.info("Running t-SNE for dimensionality reduction...")
    tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)
    features_2d = tsne.fit_transform(features)
    logger.info("âœ“ t-SNE complete")
    
    # Define colors
    color_benign = '#2E86AB'  # Blue
    color_malicious = '#A23B72'  # Red
    
    # 1. Ground Truth Labels (Clean Data)
    logger.info("ç»˜åˆ¶ 1/4: çœŸå®æ ‡ç­¾åˆ†å¸ƒ...")
    fig, ax = plt.subplots(figsize=(10, 8))
    
    benign_mask = y_true == 0
    malicious_mask = y_true == 1
    
    ax.scatter(features_2d[benign_mask, 0], features_2d[benign_mask, 1],
               c=color_benign, label=f'æ­£å¸¸æµé‡ (n={benign_mask.sum()})', 
               alpha=0.6, s=30, edgecolors='k', linewidths=0.3)
    ax.scatter(features_2d[malicious_mask, 0], features_2d[malicious_mask, 1],
               c=color_malicious, label=f'æ¶æ„æµé‡ (n={malicious_mask.sum()})', 
               alpha=0.6, s=30, edgecolors='k', linewidths=0.3)
    
    ax.set_xlabel('t-SNE ç¬¬ä¸€ä¸»æˆåˆ†', fontsize=12)
    ax.set_ylabel('t-SNE ç¬¬äºŒä¸»æˆåˆ†', fontsize=12)
    ax.set_title('ç‰¹å¾åˆ†å¸ƒ: çœŸå®æ ‡ç­¾ (å¹²å‡€æ•°æ®)', 
                fontsize=14, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    
    save_path = os.path.join(save_dir, '1_çœŸå®æ ‡ç­¾åˆ†å¸ƒ.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    logger.info(f"  âœ“ å·²ä¿å­˜: {save_path}")
    
    # 2. Noisy Labels (with noise highlighted)
    logger.info("ç»˜åˆ¶ 2/4: å™ªå£°æ ‡ç­¾åˆ†å¸ƒ...")
    fig, ax = plt.subplots(figsize=(10, 8))
    
    # Plot clean samples first
    clean_samples = ~noise_mask
    benign_clean = clean_samples & (noisy_labels == 0)
    malicious_clean = clean_samples & (noisy_labels == 1)
    
    ax.scatter(features_2d[benign_clean, 0], features_2d[benign_clean, 1],
               c=color_benign, label=f'æ­£å¸¸æµé‡ (å¹²å‡€, n={benign_clean.sum()})', 
               alpha=0.4, s=25, edgecolors='k', linewidths=0.3)
    ax.scatter(features_2d[malicious_clean, 0], features_2d[malicious_clean, 1],
               c=color_malicious, label=f'æ¶æ„æµé‡ (å¹²å‡€, n={malicious_clean.sum()})', 
               alpha=0.4, s=25, edgecolors='k', linewidths=0.3)
    
    # Plot noisy samples with special marker
    noisy_samples = noise_mask
    benign_noisy = noisy_samples & (noisy_labels == 0)
    malicious_noisy = noisy_samples & (noisy_labels == 1)
    
    ax.scatter(features_2d[benign_noisy, 0], features_2d[benign_noisy, 1],
               c=color_benign, label=f'æ­£å¸¸æµé‡ (å™ªå£°, n={benign_noisy.sum()})', 
               alpha=0.9, s=80, edgecolors='yellow', linewidths=2, marker='s')
    ax.scatter(features_2d[malicious_noisy, 0], features_2d[malicious_noisy, 1],
               c=color_malicious, label=f'æ¶æ„æµé‡ (å™ªå£°, n={malicious_noisy.sum()})', 
               alpha=0.9, s=80, edgecolors='yellow', linewidths=2, marker='s')
    
    ax.set_xlabel('t-SNE ç¬¬ä¸€ä¸»æˆåˆ†', fontsize=12)
    ax.set_ylabel('t-SNE ç¬¬äºŒä¸»æˆåˆ†', fontsize=12)
    ax.set_title(f'ç‰¹å¾åˆ†å¸ƒ: å™ªå£°æ ‡ç­¾ (å™ªå£°ç‡: {100*noise_mask.sum()/len(noise_mask):.1f}%)', 
                fontsize=14, fontweight='bold')
    ax.legend(fontsize=9, loc='best')
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    
    save_path = os.path.join(save_dir, '2_å™ªå£°æ ‡ç­¾åˆ†å¸ƒ.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    logger.info(f"  âœ“ å·²ä¿å­˜: {save_path}")
    
    # 3. Corrected Labels (excluding dropped samples)
    logger.info("ç»˜åˆ¶ 3/4: çŸ«æ­£åæ ‡ç­¾åˆ†å¸ƒ...")
    fig, ax = plt.subplots(figsize=(10, 8))
    
    kept_samples = action_mask != 2  # Exclude dropped samples
    
    benign_corrected = kept_samples & (clean_labels == 0)
    malicious_corrected = kept_samples & (clean_labels == 1)
    dropped_samples = action_mask == 2
    
    ax.scatter(features_2d[benign_corrected, 0], features_2d[benign_corrected, 1],
               c=color_benign, label=f'æ­£å¸¸æµé‡ (n={benign_corrected.sum()})', 
               alpha=0.6, s=30, edgecolors='k', linewidths=0.3)
    ax.scatter(features_2d[malicious_corrected, 0], features_2d[malicious_corrected, 1],
               c=color_malicious, label=f'æ¶æ„æµé‡ (n={malicious_corrected.sum()})', 
               alpha=0.6, s=30, edgecolors='k', linewidths=0.3)
    
    # Show dropped samples
    if dropped_samples.sum() > 0:
        ax.scatter(features_2d[dropped_samples, 0], features_2d[dropped_samples, 1],
                   c='gray', label=f'å·²ä¸¢å¼ƒ (n={dropped_samples.sum()})', 
                   alpha=0.5, s=50, marker='x', linewidths=1.5)
    
    ax.set_xlabel('t-SNE ç¬¬ä¸€ä¸»æˆåˆ†', fontsize=12)
    ax.set_ylabel('t-SNE ç¬¬äºŒä¸»æˆåˆ†', fontsize=12)
    ax.set_title('ç‰¹å¾åˆ†å¸ƒ: çŸ«æ­£åæ ‡ç­¾ (Hybrid CourtçŸ«æ­£å)', 
                fontsize=14, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    
    save_path = os.path.join(save_dir, '3_çŸ«æ­£åæ ‡ç­¾åˆ†å¸ƒ.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    logger.info(f"  âœ“ å·²ä¿å­˜: {save_path}")
    
    # 4. Action Visualization (Keep, Flip, Drop, Reweight)
    logger.info("ç»˜åˆ¶ 4/4: çŸ«æ­£åŠ¨ä½œåˆ†å¸ƒ...")
    fig, ax = plt.subplots(figsize=(12, 9))
    
    keep_mask = action_mask == 0
    flip_mask = action_mask == 1
    drop_mask = action_mask == 2
    reweight_mask = action_mask == 3
    
    # Plot in reverse order so important actions are on top
    if reweight_mask.sum() > 0:
        ax.scatter(features_2d[reweight_mask, 0], features_2d[reweight_mask, 1],
                   c='#F77F00', label=f'é‡åŠ æƒ (é•¿å°¾æ ·æœ¬, n={reweight_mask.sum()})', 
                   alpha=0.7, s=60, edgecolors='k', linewidths=0.5, marker='v')
    
    if keep_mask.sum() > 0:
        ax.scatter(features_2d[keep_mask, 0], features_2d[keep_mask, 1],
                   c='#06A77D', label=f'ä¿æŒ (æ ¸å¿ƒæ ·æœ¬, n={keep_mask.sum()})', 
                   alpha=0.5, s=25, edgecolors='k', linewidths=0.3)
    
    if flip_mask.sum() > 0:
        ax.scatter(features_2d[flip_mask, 0], features_2d[flip_mask, 1],
                   c='#D62828', label=f'ç¿»è½¬ (å·²çŸ«æ­£, n={flip_mask.sum()})', 
                   alpha=0.8, s=70, edgecolors='k', linewidths=0.8, marker='*')
    
    if drop_mask.sum() > 0:
        ax.scatter(features_2d[drop_mask, 0], features_2d[drop_mask, 1],
                   c='#6A4C93', label=f'ä¸¢å¼ƒ (ç³»ç»Ÿå™ªå£°, n={drop_mask.sum()})', 
                   alpha=0.8, s=80, edgecolors='k', linewidths=1, marker='X')
    
    ax.set_xlabel('t-SNE ç¬¬ä¸€ä¸»æˆåˆ†', fontsize=12)
    ax.set_ylabel('t-SNE ç¬¬äºŒä¸»æˆåˆ†', fontsize=12)
    ax.set_title('ç‰¹å¾åˆ†å¸ƒ: Hybrid CourtçŸ«æ­£åŠ¨ä½œ', 
                fontsize=14, fontweight='bold')
    ax.legend(fontsize=11, loc='best', framealpha=0.9)
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    
    save_path = os.path.join(save_dir, '4_çŸ«æ­£åŠ¨ä½œåˆ†å¸ƒ.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    logger.info(f"  âœ“ å·²ä¿å­˜: {save_path}")
    
    # 5. Comparison Plot (Before vs After)
    logger.info("ç»˜åˆ¶ 5/5: çŸ«æ­£å‰åå¯¹æ¯”...")
    fig, axes = plt.subplots(1, 2, figsize=(18, 7))
    
    # Before: Noisy Labels
    ax = axes[0]
    benign_noisy_all = noisy_labels == 0
    malicious_noisy_all = noisy_labels == 1
    
    ax.scatter(features_2d[benign_noisy_all, 0], features_2d[benign_noisy_all, 1],
               c=color_benign, label=f'æ­£å¸¸æµé‡ (n={benign_noisy_all.sum()})', 
               alpha=0.5, s=30, edgecolors='k', linewidths=0.3)
    ax.scatter(features_2d[malicious_noisy_all, 0], features_2d[malicious_noisy_all, 1],
               c=color_malicious, label=f'æ¶æ„æµé‡ (n={malicious_noisy_all.sum()})', 
               alpha=0.5, s=30, edgecolors='k', linewidths=0.3)
    
    # Highlight noise
    ax.scatter(features_2d[noise_mask, 0], features_2d[noise_mask, 1],
               facecolors='none', edgecolors='yellow', 
               label=f'å™ªå£°æ ·æœ¬ (n={noise_mask.sum()})', 
               s=100, linewidths=2, marker='o')
    
    ax.set_xlabel('t-SNE ç¬¬ä¸€ä¸»æˆåˆ†', fontsize=12)
    ax.set_ylabel('t-SNE ç¬¬äºŒä¸»æˆåˆ†', fontsize=12)
    ax.set_title(f'çŸ«æ­£å‰: å™ªå£°æ ‡ç­¾\n(å™ªå£°ç‡: {100*noise_mask.sum()/len(noise_mask):.1f}%)', 
                fontsize=13, fontweight='bold')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)
    
    # After: Corrected Labels
    ax = axes[1]
    benign_corrected_all = kept_samples & (clean_labels == 0)
    malicious_corrected_all = kept_samples & (clean_labels == 1)
    
    ax.scatter(features_2d[benign_corrected_all, 0], features_2d[benign_corrected_all, 1],
               c=color_benign, label=f'æ­£å¸¸æµé‡ (n={benign_corrected_all.sum()})', 
               alpha=0.5, s=30, edgecolors='k', linewidths=0.3)
    ax.scatter(features_2d[malicious_corrected_all, 0], features_2d[malicious_corrected_all, 1],
               c=color_malicious, label=f'æ¶æ„æµé‡ (n={malicious_corrected_all.sum()})', 
               alpha=0.5, s=30, edgecolors='k', linewidths=0.3)
    
    # Show corrections
    ax.scatter(features_2d[flip_mask, 0], features_2d[flip_mask, 1],
               facecolors='none', edgecolors='lime', 
               label=f'å·²ç¿»è½¬ (n={flip_mask.sum()})', 
               s=100, linewidths=2, marker='o')
    
    if drop_mask.sum() > 0:
        ax.scatter(features_2d[drop_mask, 0], features_2d[drop_mask, 1],
                   c='gray', label=f'å·²ä¸¢å¼ƒ (n={drop_mask.sum()})', 
                   alpha=0.6, s=50, marker='x', linewidths=1.5)
    
    # Calculate correction accuracy
    correction_accuracy = (clean_labels[kept_samples] == y_true[kept_samples]).mean()
    
    ax.set_xlabel('t-SNE ç¬¬ä¸€ä¸»æˆåˆ†', fontsize=12)
    ax.set_ylabel('t-SNE ç¬¬äºŒä¸»æˆåˆ†', fontsize=12)
    ax.set_title(f'çŸ«æ­£å: Hybrid Court\n(å‡†ç¡®ç‡: {correction_accuracy*100:.2f}%)', 
                fontsize=13, fontweight='bold')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    save_path = os.path.join(save_dir, '5_çŸ«æ­£å‰åå¯¹æ¯”.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    logger.info(f"  âœ“ å·²ä¿å­˜: {save_path}")
    
    logger.info("="*70)
    logger.info("âœ“ æ‰€æœ‰ç‰¹å¾åˆ†å¸ƒå›¾å·²ç”Ÿæˆ")


def main(args):
    """Main function"""
    global logger
    
    # Setup
    set_seed(config.SEED)
    config.create_dirs()
    
    # å™ªå£°ç‡ç™¾åˆ†æ¯”
    noise_pct = int(args.noise_rate * 100)
    
    # Create analysis output directory (åŒ…å«å™ªå£°ç‡)
    analysis_dir = os.path.join(config.LABEL_CORRECTION_DIR, "analysis", f"noise_{noise_pct}pct")
    os.makedirs(analysis_dir, exist_ok=True)
    os.makedirs(os.path.join(analysis_dir, "figures"), exist_ok=True)
    os.makedirs(os.path.join(analysis_dir, "documents"), exist_ok=True)
    
    # åˆ›å»ºå¸¦å™ªå£°ç‡çš„æ—¥å¿—æ–‡ä»¶
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_filename = f"noise_{noise_pct}pct_analysis_{timestamp}.log"
    log_dir = os.path.join(config.OUTPUT_ROOT, "logs")
    os.makedirs(log_dir, exist_ok=True)
    log_path = os.path.join(log_dir, log_filename)
    
    # åˆ›å»ºå…±äº«çš„handlers
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%H:%M:%S')
    
    fh = logging.FileHandler(log_path, encoding='utf-8')
    fh.setLevel(logging.INFO)
    fh.setFormatter(formatter)
    
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch.setFormatter(formatter)
    
    # é…ç½®root logger (æ•è·æ‰€æœ‰æ¨¡å—çš„æ—¥å¿—ï¼ŒåŒ…æ‹¬HybridCourt)
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    root_logger.handlers = []  # æ¸…é™¤å·²æœ‰handlers
    root_logger.addHandler(fh)
    root_logger.addHandler(ch)
    
    # è·å–æœ¬æ¨¡å—çš„logger (ä½¿ç”¨root loggerï¼Œä¸éœ€è¦å•ç‹¬é…ç½®)
    logger = logging.getLogger('label_correction_analysis')
    logger.setLevel(logging.INFO)
    logger.handlers = []  # æ¸…é™¤handlersï¼Œä½¿ç”¨rootçš„
    logger.propagate = True  # ä¼ æ’­åˆ°root logger
    
    # ç®€æ´çš„æ ‡é¢˜
    logger.info("")
    logger.info("â•”" + "â•"*68 + "â•—")
    logger.info("â•‘" + f"  MEDAL-Lite æ ‡ç­¾çŸ«æ­£åˆ†æ - å™ªå£°ç‡ {noise_pct}%".center(68) + "â•‘")
    logger.info("â•š" + "â•"*68 + "â•")
    logger.info(f"  æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  |  è®¾å¤‡: {config.DEVICE}")
    logger.info("")
    
    # Override config with arguments
    if args.noise_rate is not None:
        config.LABEL_NOISE_RATE = args.noise_rate
    
    # ========================
    # 1. Load Dataset
    # ========================
    logger.info("â”Œâ”€ Step 1: åŠ è½½æ•°æ®é›†")
    logger.info(f"â”‚  æ•°æ®è·¯å¾„: {config.BENIGN_TRAIN} | {config.MALICIOUS_TRAIN}")
    logger.info(f"â”‚  åºåˆ—é•¿åº¦: {config.SEQUENCE_LENGTH}")
    
    # ä¼˜å…ˆä½¿ç”¨é¢„å¤„ç†å¥½çš„æ•°æ®
    if check_preprocessed_exists('train'):
        logger.info("â”‚  âœ“ ä½¿ç”¨é¢„å¤„ç†æ–‡ä»¶")
        X_train, y_train_clean, train_files = load_preprocessed('train')
    else:
        # ä»PCAPæ–‡ä»¶åŠ è½½
        logger.info("â”‚  âš  ä»PCAPæ–‡ä»¶åŠ è½½ï¼ˆå»ºè®®å…ˆé¢„å¤„ç†ä»¥åŠ é€Ÿï¼‰")
        X_train, y_train_clean, train_files = load_dataset(
            benign_dir=config.BENIGN_TRAIN,
            malicious_dir=config.MALICIOUS_TRAIN,
            sequence_length=config.SEQUENCE_LENGTH
        )
    
    if X_train is None:
        logger.error("â”‚  âŒ æ•°æ®é›†åŠ è½½å¤±è´¥!")
        return
    
    logger.info(f"â””â”€ âœ“ å®Œæˆ: {X_train.shape[0]} ä¸ªæ ·æœ¬ | æ­£å¸¸={(y_train_clean==0).sum()} | æ¶æ„={(y_train_clean==1).sum()}")
    logger.info("")
    
    # ========================
    # 2. Inject Label Noise
    # ========================
    logger.info("â”Œâ”€ Step 2: æ³¨å…¥æ ‡ç­¾å™ªå£°")
    logger.info(f"â”‚  å™ªå£°ç‡: {config.LABEL_NOISE_RATE*100:.0f}%")
    
    # å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿ç›¸åŒå™ªå£°ç‡çš„ç»“æœå¯å¤ç°
    set_seed(config.SEED)
    y_train_noisy, noise_mask = inject_label_noise(y_train_clean, config.LABEL_NOISE_RATE)
    
    logger.info(f"â””â”€ âœ“ å®Œæˆ: {noise_mask.sum()} ä¸ªæ ‡ç­¾è¢«ç¿»è½¬ | åŸå§‹çº¯åº¦: {100*(y_train_clean==y_train_noisy).mean():.1f}%")
    logger.info("")
    
    # ========================
    # 3. Extract Features
    # ========================
    logger.info("â”Œâ”€ Step 3: æå–ç‰¹å¾")
    
    # ç¡®å®šbackboneè·¯å¾„ï¼šä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
    if args.backbone_path:
        backbone_path = args.backbone_path
    else:
        backbone_path = os.path.join(config.FEATURE_EXTRACTION_DIR, "models", "backbone_pretrained.pth")
    
    # Try to load pre-trained backbone
    if os.path.exists(backbone_path) and not args.retrain_backbone:
        logger.info(f"â”‚  åŠ è½½é¢„è®­ç»ƒbackbone: {os.path.basename(backbone_path)}")
        # ä½¿ç”¨å®‰å…¨çš„æ¨¡å‹åŠ è½½å‡½æ•°ï¼ˆè‡ªåŠ¨å¤„ç†å…¼å®¹æ€§ï¼‰
        from MoudleCode.utils.model_loader import load_backbone_safely
        backbone = load_backbone_safely(
            backbone_path=backbone_path,
            config=config,
            device=config.DEVICE,
            logger=logger
        )
        logger.info("â”‚  âœ“ BackboneåŠ è½½å®Œæˆ")
    else:
        # æ„å»ºæ–°æ¨¡å‹
        backbone = build_backbone(config)
        if args.retrain_backbone:
            logger.info("â”‚  âš  ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„backbone")
        else:
            logger.info(f"â”‚  âš  æœªæ‰¾åˆ°é¢„è®­ç»ƒbackboneï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
        logger.info("â”‚  âœ“ Backboneåˆå§‹åŒ–å®Œæˆ")
    
    features = extract_features_with_backbone(backbone, X_train, config, logger)
    
    # Save features
    features_path = os.path.join(analysis_dir, "extracted_features.npy")
    np.save(features_path, features)
    logger.info(f"â””â”€ âœ“ å®Œæˆ: ç‰¹å¾ç»´åº¦ {features.shape} | å·²ä¿å­˜")
    logger.info("")
    
    # ========================
    # 4. Run Hybrid Court Label Correction
    # ========================
    logger.info("â”Œâ”€ Step 4: è¿è¡Œ Hybrid Court æ ‡ç­¾çŸ«æ­£")
    
    results = run_hybrid_court_with_detailed_tracking(features, y_train_noisy, config, logger, y_true=y_train_clean)
    
    # Save results
    results_path = os.path.join(analysis_dir, "correction_results.npz")
    np.savez(results_path,
             y_clean=y_train_clean,
             y_noisy=y_train_noisy,
             y_corrected=results['clean_labels'],
             action_mask=results['action_mask'],
             confidence=results['confidence'],
             correction_weight=results['correction_weight'],
             noise_mask=noise_mask,
             features=features,
             cl_suspected_noise=results['cl_suspected_noise'],
             cl_pred_labels=results['cl_pred_labels'],
             cl_pred_probs=results['cl_pred_probs'],
             aum_scores=results['aum_scores'],
             knn_neighbor_labels=results['knn_neighbor_labels'],
             knn_neighbor_consistency=results['knn_neighbor_consistency'],
             tier_info=np.array(results.get('tier_info', []), dtype=object))
    
    # è®¡ç®—å…³é”®æŒ‡æ ‡
    action_mask = results['action_mask']
    keep_mask = action_mask != 2
    correction_accuracy = (results['clean_labels'][keep_mask] == y_train_clean[keep_mask]).mean()
    original_purity = (y_train_clean == y_train_noisy).mean()
    final_purity = (results['clean_labels'][keep_mask] == y_train_clean[keep_mask]).mean()
    improvement = final_purity - original_purity
    
    logger.info(f"â””â”€ âœ“ å®Œæˆ: åŸå§‹çº¯åº¦ {original_purity*100:.1f}% â†’ æœ€ç»ˆçº¯åº¦ {final_purity*100:.1f}% (æå‡ +{improvement*100:.1f}%)")
    logger.info("")
    
    # ========================
    # 5. Generate Analysis Document
    # ========================
    logger.info("â”Œâ”€ Step 5: ç”Ÿæˆæ ·æœ¬åˆ†ææ–‡æ¡£")
    doc_path = os.path.join(analysis_dir, "documents", f"sample_analysis_{noise_pct}pct_cl_aum_knn.log")
    df_analysis = generate_sample_analysis_document(
        results, y_train_clean, noise_mask, doc_path, logger, noise_pct=noise_pct
    )
    logger.info(f"â””â”€ âœ“ å®Œæˆ: CSV/Excel/LOG å·²ç”Ÿæˆ")
    logger.info("")
    
    # ========================
    # 6. Generate Feature Distribution Plots
    # ========================
    logger.info("â”Œâ”€ Step 6: ç”Ÿæˆç‰¹å¾åˆ†å¸ƒå›¾")
    plot_dir = os.path.join(analysis_dir, "figures")
    plot_feature_distributions(results, y_train_clean, noise_mask, plot_dir, logger)
    logger.info(f"â””â”€ âœ“ å®Œæˆ: 5 å¼ å›¾è¡¨å·²ç”Ÿæˆ")
    logger.info("")
    
    # ========================
    # Summary
    # ========================
    logger.info("â•”" + "â•"*68 + "â•—")
    logger.info("â•‘" + f"  âœ“ å™ªå£°ç‡ {noise_pct}% åˆ†æå®Œæˆ".center(68) + "â•‘")
    logger.info("â•š" + "â•"*68 + "â•")
    logger.info("")
    
    # å…³é”®ç»Ÿè®¡æ‘˜è¦ï¼ˆç´§å‡‘æ ¼å¼ï¼‰
    n_samples = len(y_train_clean)
    action_mask = results['action_mask']
    keep_mask = action_mask != 2
    correction_accuracy = (results['clean_labels'][keep_mask] == y_train_clean[keep_mask]).mean()
    original_purity = (y_train_clean == y_train_noisy).mean()
    final_purity = correction_accuracy
    improvement = final_purity - original_purity
    
    logger.info("ğŸ“Š å…³é”®æŒ‡æ ‡:")
    logger.info(f"   åŸå§‹çº¯åº¦: {original_purity*100:.1f}%  â†’  æœ€ç»ˆçº¯åº¦: {final_purity*100:.1f}%  (æå‡: +{improvement*100:.1f}%)")
    logger.info(f"   åŠ¨ä½œåˆ†å¸ƒ: æœªç¿»è½¬ {(action_mask==0).sum()} ({100*(action_mask==0).sum()/n_samples:.1f}%) | "
                f"ç¿»è½¬ {(action_mask==1).sum()} ({100*(action_mask==1).sum()/n_samples:.1f}%) | "
                f"ä¸¢å¼ƒ {(action_mask==2).sum()} ({100*(action_mask==2).sum()/n_samples:.1f}%)")
    logger.info("")
    
    # è¾“å‡ºå¯è§£æçš„æ‘˜è¦è¡Œï¼ˆä¾›Shellè„šæœ¬æå–ï¼‰
    logger.info(f"SUMMARY: noise_rate={noise_pct}% | original_purity={original_purity*100:.2f}% | "
                f"final_purity={final_purity*100:.2f}% | improvement={improvement*100:.2f}% | "
                f"flip_count={(action_mask==1).sum()} | keep_count={(action_mask==0).sum()}")
    logger.info("")
    
    # æ¸…ç†root loggerçš„handlers
    for handler in root_logger.handlers[:]:
        handler.close()
        root_logger.removeHandler(handler)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Label Correction Analysis for MEDAL-Lite"
    )
    parser.add_argument(
        "--noise_rate", 
        type=float, 
        default=0.30, 
        help="Label noise rate (default: 0.30)"
    )
    parser.add_argument(
        "--retrain_backbone",
        action='store_true',
        help="Use randomly initialized backbone instead of pre-trained"
    )
    parser.add_argument(
        "--backbone_path",
        type=str,
        default='',
        help="Path to backbone model (optional, default: backbone_pretrained.pth)"
    )
    
    args = parser.parse_args()
    
    main(args)

