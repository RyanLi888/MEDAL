"""
MEDAL-Lite Training Script
Implements the complete 3-stage training pipeline
"""
import sys
import os
from pathlib import Path

# Ensure project root is on sys.path when running as a script
project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

import torch
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import numpy as np
import argparse
from datetime import datetime

from MoudleCode.utils.config import config
from MoudleCode.utils.helpers import (
    set_seed, setup_logger, inject_label_noise,
    calculate_metrics, print_metrics, save_checkpoint, find_optimal_threshold
)
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, f1_score, silhouette_score
from MoudleCode.utils.visualization import (
    plot_feature_space, plot_noise_correction_comparison, plot_training_history
)
from MoudleCode.preprocessing.pcap_parser import load_dataset
from MoudleCode.feature_extraction.backbone import (
    MicroBiMambaBackbone, SimMTMLoss, build_backbone
)
from MoudleCode.feature_extraction.traffic_augmentation import DualViewAugmentation
from MoudleCode.feature_extraction.instance_contrastive import (
    InstanceContrastiveLearning, HybridPretrainingLoss
)
from MoudleCode.utils.checkpoint import load_state_dict_shape_safe

# å¯¼å…¥é¢„å¤„ç†æ¨¡å—
try:
    from scripts.utils.preprocess import check_preprocessed_exists, load_preprocessed, preprocess_train
    from scripts.utils.preprocess import normalize_burstsize_inplace
    PREPROCESS_AVAILABLE = True
except ImportError:
    PREPROCESS_AVAILABLE = False
from MoudleCode.label_correction.hybrid_court import HybridCourt
from MoudleCode.data_augmentation.tabddpm import TabDDPM
from MoudleCode.classification.dual_stream import MEDAL_Classifier, DualStreamLoss

def stage1_pretrain_backbone(backbone, train_loader, config, logger):
    """
    Stage 1: Pre-train backbone with SimMTM only (unsupervised)
    
    Args:
        backbone: MicroBiMambaBackbone model
        train_loader: DataLoader with X only (no labels needed for SimMTM)
        config: configuration object
        logger: logger
        
    Returns:
        backbone: pre-trained backbone
    """
    # Clear GPU cache before starting
    import torch
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        logger.info("âœ“ GPU ç¼“å­˜å·²æ¸…ç†")
    
    logger.info("="*70)
    logger.info("STAGE 1: Pre-training Backbone")
    logger.info("="*70)
    logger.info(f"ç›®æ ‡: è®­ç»ƒMicro-Bi-Mambaéª¨å¹²ç½‘ç»œï¼Œå­¦ä¹ æµé‡ç‰¹å¾è¡¨ç¤º")
    logger.info(f"è®­ç»ƒè½®æ•°: {config.PRETRAIN_EPOCHS} epochs")
    
    # æ˜¾ç¤ºå®é™…ä½¿ç”¨çš„æ‰¹æ¬¡å¤§å°
    actual_batch_size = train_loader.batch_size
    logger.info(f"æ‰¹æ¬¡å¤§å°: {actual_batch_size}")
    
    logger.info(f"å­¦ä¹ ç‡: {config.PRETRAIN_LR}")
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨å®ä¾‹å¯¹æ¯”å­¦ä¹ 
    use_instance_contrastive = getattr(config, 'USE_INSTANCE_CONTRASTIVE', False)
    contrastive_method = getattr(config, 'CONTRASTIVE_METHOD', 'infonce')
    if use_instance_contrastive:
        method = contrastive_method
        logger.info(f"ä¼˜åŒ–ç›®æ ‡: SimMTM + {str(method).upper()} (å®ä¾‹å¯¹æ¯”å­¦ä¹ )")
        if str(method).lower() in ['infonce', 'nnclr']:
            logger.info(f"  - æ¸©åº¦: {config.INFONCE_TEMPERATURE}")
        logger.info(f"  - æƒé‡: {config.INFONCE_LAMBDA}")
        
        # NNCLR é»˜è®¤å¯ç”¨æ¢¯åº¦ç´¯ç§¯ï¼ˆç”± PRETRAIN_GRADIENT_ACCUMULATION_STEPS æ§åˆ¶ï¼‰
        if str(method).lower() == 'nnclr':
            gradient_accumulation_steps = int(getattr(config, 'PRETRAIN_GRADIENT_ACCUMULATION_STEPS', 1))
            if gradient_accumulation_steps > 1:
                effective_batch_size = actual_batch_size * gradient_accumulation_steps
                logger.info(f"  - æ¢¯åº¦ç´¯ç§¯: {gradient_accumulation_steps} æ­¥")
                logger.info(f"  - æœ‰æ•ˆæ‰¹æ¬¡: {actual_batch_size} Ã— {gradient_accumulation_steps} = {effective_batch_size}")
    else:
        logger.info(f"ä¼˜åŒ–ç›®æ ‡: SimMTM (æ©ç é‡æ„)")
    logger.info("")
    logger.info("ğŸ“¥ è¾“å…¥æ•°æ®è·¯å¾„:")
    logger.info(f"  âœ“ è®­ç»ƒæ•°æ®: {config.BENIGN_TRAIN} (æ­£å¸¸), {config.MALICIOUS_TRAIN} (æ¶æ„)")
    logger.info("")
    
    backbone.train()
    backbone.to(config.DEVICE)
    
    # åˆ›å»ºå¢å¼ºå™¨å’ŒæŸå¤±å‡½æ•°
    if use_instance_contrastive:
        # å®ä¾‹å¯¹æ¯”å­¦ä¹ æ¨¡å¼
        augmentation = DualViewAugmentation(config)
        instance_contrastive = InstanceContrastiveLearning(backbone, config).to(config.DEVICE)
        simmtm_loss_fn = SimMTMLoss(
            config,
            mask_rate=config.SIMMTM_MASK_RATE,
            noise_std=getattr(config, 'PRETRAIN_NOISE_STD', 0.05)
        )
        hybrid_loss_fn = HybridPretrainingLoss(
            simmtm_loss=simmtm_loss_fn,
            instance_contrastive=instance_contrastive,
            lambda_infonce=config.INFONCE_LAMBDA
        )
        logger.info(f"âœ“ æ··åˆæŸå¤±å‡½æ•°åˆå§‹åŒ–å®Œæˆ (SimMTM + InfoNCE)")
        
        # ä¼˜åŒ–å™¨åŒ…æ‹¬projection head
        optimizer = optim.AdamW(
            list(backbone.parameters()) + list(instance_contrastive.projection_head.parameters()),
            lr=config.PRETRAIN_LR,
            weight_decay=config.PRETRAIN_WEIGHT_DECAY
        )
    else:
        # åŸå§‹SimMTMæ¨¡å¼
        simmtm_loss_fn = SimMTMLoss(
            config,
            mask_rate=config.SIMMTM_MASK_RATE,
            noise_std=getattr(config, 'PRETRAIN_NOISE_STD', 0.05)
        )
        logger.info(f"âœ“ æŸå¤±å‡½æ•°åˆå§‹åŒ–å®Œæˆ (SimMTMæ©ç ç‡: {config.SIMMTM_MASK_RATE})")
        
        optimizer = optim.AdamW(
            backbone.parameters(),
            lr=config.PRETRAIN_LR,
            weight_decay=config.PRETRAIN_WEIGHT_DECAY
        )
    
    pretrain_min_lr = float(getattr(config, 'PRETRAIN_MIN_LR', 1e-5))
    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=config.PRETRAIN_EPOCHS, eta_min=pretrain_min_lr
    )
    logger.info(f"âœ“ ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨åˆå§‹åŒ–å®Œæˆ")
    logger.info("")
    logger.info("å¼€å§‹è®­ç»ƒ...")
    logger.info("-"*70)
    
    # Training loop
    if use_instance_contrastive:
        history = {'loss': [], 'simmtm': [], 'infonce': []}
    else:
        history = {'loss': [], 'simmtm': []}
    
    use_early_stopping = bool(getattr(config, 'PRETRAIN_EARLY_STOPPING', True))
    es_warmup_epochs = int(getattr(config, 'PRETRAIN_ES_WARMUP_EPOCHS', 50))
    es_patience = int(getattr(config, 'PRETRAIN_ES_PATIENCE', 20))
    es_min_delta = float(getattr(config, 'PRETRAIN_ES_MIN_DELTA', 0.01))

    best_loss = float('inf')
    best_epoch = -1
    best_state = None
    no_improve = 0
    
    # Gradient accumulation (ç”¨äº NNCLRï¼Œä¿æŒæœ‰æ•ˆæ‰¹æ¬¡ä¸å˜)
    use_gradient_accumulation = (
        use_instance_contrastive and
        str(contrastive_method).lower() == 'nnclr' and
        int(getattr(config, 'PRETRAIN_GRADIENT_ACCUMULATION_STEPS', 1)) > 1
    )

    if use_gradient_accumulation:
        gradient_accumulation_steps = int(getattr(config, 'PRETRAIN_GRADIENT_ACCUMULATION_STEPS', 2))
    else:
        gradient_accumulation_steps = 1  # ç¦ç”¨æ¢¯åº¦ç´¯ç§¯

    if use_instance_contrastive:
        method_lower = str(contrastive_method).lower()
        temperature = float(getattr(config, 'INFONCE_TEMPERATURE', getattr(config, 'SUPCON_TEMPERATURE', 0.1)))
        lambda_infonce = float(getattr(config, 'INFONCE_LAMBDA', 1.0))
        effective_batch_size = int(actual_batch_size) * int(gradient_accumulation_steps)
        logger.info("[Stage 1] Contrastive config:")
        logger.info(f"  - method: {str(contrastive_method).upper()}")
        logger.info(f"  - temperature: {temperature}")
        logger.info(f"  - lambda_infonce: {lambda_infonce}")
        logger.info(f"  - per-step batch_size: {int(actual_batch_size)}")
        logger.info(f"  - gradient_accumulation_steps: {int(gradient_accumulation_steps)}")
        logger.info(f"  - effective_batch_size: {effective_batch_size}")
        if method_lower in ['infonce', 'simclr', 'nnclr']:
            approx_negatives = max(2 * int(actual_batch_size) - 2, 0)
            logger.info(f"  - negatives per anchor (within step): ~{approx_negatives}")
        if method_lower == 'nnclr':
            nnclr_queue_size = int(getattr(config, 'NNCLR_QUEUE_SIZE', 4096))
            logger.info(f"  - nnclr queue size: {nnclr_queue_size}")

    for epoch in range(config.PRETRAIN_EPOCHS):
        epoch_loss = 0.0
        epoch_simmtm = 0.0
        epoch_infonce = 0.0
        epoch_stream_consistency = 0.0
        
        for batch_idx, batch_data in enumerate(train_loader):
            # TensorDataset with single tensor returns the tensor directly
            # TensorDataset with multiple tensors returns a tuple
            if isinstance(batch_data, (list, tuple)):
                X_batch = batch_data[0]  # Get first element (X)
            else:
                X_batch = batch_data  # Already a single tensor
            X_batch = X_batch.to(config.DEVICE)
            
            # Only zero grad at the start of accumulation
            if batch_idx % gradient_accumulation_steps == 0:
                optimizer.zero_grad()
            
            if use_instance_contrastive:
                # å¢å¼ºè§†å›¾
                x_view1, x_view2 = augmentation(X_batch)
                
                # æ··åˆæŸå¤±
                loss, loss_dict = hybrid_loss_fn(
                    backbone=backbone,
                    x_original=X_batch,
                    x_view1=x_view1,
                    x_view2=x_view2,
                    epoch=epoch
                )
                
                epoch_simmtm += loss_dict['simmtm']
                epoch_infonce += loss_dict['infonce']
            else:
                # åŸå§‹SimMTMæ¨¡å¼
                loss = simmtm_loss_fn(backbone, X_batch)
                epoch_simmtm += loss.item()
            
            # Scale loss by accumulation steps
            loss = loss / gradient_accumulation_steps
            loss.backward()
            
            # Only step optimizer after accumulation
            if (batch_idx + 1) % gradient_accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):
                optimizer.step()
            
            epoch_loss += loss.item() * gradient_accumulation_steps  # Restore original scale for logging
        
        scheduler.step()
        
        # Average losses
        n_batches = len(train_loader)
        epoch_loss /= n_batches
        epoch_simmtm /= n_batches
        
        history['loss'].append(epoch_loss)
        history['simmtm'].append(epoch_simmtm)
        
        if use_instance_contrastive:
            epoch_infonce /= n_batches
            history['infonce'].append(epoch_infonce)
            try:
                if epoch in (0, 1) and hasattr(simmtm_loss_fn, 'last_loss_dict') and isinstance(simmtm_loss_fn.last_loss_dict, dict):
                    ld = simmtm_loss_fn.last_loss_dict
                    logger.info(
                        f"[Stage 1] SimMTM components | "
                        f"len={float(ld.get('length', 0.0)):.4f} | "
                        f"burst={float(ld.get('burst', 0.0)):.4f} | "
                        f"dir={float(ld.get('direction', 0.0)):.4f} | "
                        f"vm={float(ld.get('validmask', 0.0)):.4f}"
                    )
            except Exception:
                pass
        
        # æ¯ä¸ªepochéƒ½è¾“å‡ºæ—¥å¿—ï¼ˆä¾¿äºç›‘æ§ï¼‰
        progress = (epoch + 1) / config.PRETRAIN_EPOCHS * 100
        if use_instance_contrastive:
            method_name = str(contrastive_method).upper()
            logger.info(f"[Stage 1] Epoch [{epoch+1}/{config.PRETRAIN_EPOCHS}] ({progress:.1f}%) | "
                       f"Loss: {epoch_loss:.4f} | "
                       f"SimMTM: {epoch_simmtm:.4f} | "
                       f"{method_name}: {epoch_infonce:.4f} | "
                       f"LR: {scheduler.get_last_lr()[0]:.6f}")
        else:
            logger.info(f"[Stage 1] Epoch [{epoch+1}/{config.PRETRAIN_EPOCHS}] ({progress:.1f}%) | "
                       f"Loss: {epoch_loss:.4f} | "
                       f"SimMTM: {epoch_simmtm:.4f} | "
                       f"LR: {scheduler.get_last_lr()[0]:.6f}")

        improved = (best_loss - epoch_loss) > es_min_delta
        if improved:
            best_loss = float(epoch_loss)
            best_epoch = int(epoch + 1)
            best_state = {k: v.detach().cpu().clone() for k, v in backbone.state_dict().items()}
            no_improve = 0
        else:
            no_improve += 1

        if use_early_stopping and (epoch + 1) >= es_warmup_epochs and no_improve >= es_patience:
            logger.info(
                f"[Stage 1] EarlyStopping triggered at epoch {epoch+1}: "
                f"best_loss={best_loss:.4f} (epoch {best_epoch}), "
                f"no_improve={no_improve}, min_delta={es_min_delta}"
            )
            break

    if best_state is not None:
        load_state_dict_shape_safe(backbone, best_state, logger, prefix="backbone(best)")
        backbone.to(config.DEVICE)
    
    logger.info("-"*70)
    logger.info("âœ“ Stage 1 å®Œæˆ: éª¨å¹²ç½‘ç»œé¢„è®­ç»ƒå®Œæˆ")
    logger.info(f"  æœ€ç»ˆæŸå¤±: {history['loss'][-1]:.4f}")
    if best_epoch > 0:
        logger.info(f"  æœ€ä½³æŸå¤±: {best_loss:.4f} (epoch {best_epoch})")
    logger.info(f"  è®­ç»ƒäº† {len(history['loss'])} ä¸ªepoch")
    logger.info("")
    
    # ç”Ÿæˆbackboneæ–‡ä»¶åï¼šbackbone_{method}_{dim}d_{n_samples}.pth
    # method: SimMTM æˆ– SimCLR (å¦‚æœå¯ç”¨å®ä¾‹å¯¹æ¯”å­¦ä¹ )
    # dim: æ¨¡å‹ç»´åº¦ (MODEL_DIM)
    n_samples = len(train_loader.dataset)
    model_dim = config.MODEL_DIM
    if use_instance_contrastive:
        method = getattr(config, 'CONTRASTIVE_METHOD', 'infonce')
        method_name = f"SimMTM_{str(method).upper()}"
    else:
        method_name = "SimMTM"
    
    backbone_filename = f"backbone_{method_name}_{model_dim}d_{n_samples}.pth"
    
    logger.info("ğŸ“ è¾“å‡ºæ–‡ä»¶è·¯å¾„:")
    # Save backbone to feature_extraction module directory
    backbone_path = os.path.join(config.FEATURE_EXTRACTION_DIR, "models", backbone_filename)
    torch.save(backbone.state_dict(), backbone_path)
    logger.info(f"  âœ“ éª¨å¹²ç½‘ç»œæ¨¡å‹: {backbone_path}")
    logger.info(f"    (å‘½åæ ¼å¼: backbone_{{æ–¹æ³•}}_{{ç»´åº¦}}d_{{æ ·æœ¬æ•°}}.pth)")
    
    # åŒæ—¶ä¿å­˜ä¸€ä¸ªé»˜è®¤åç§°çš„å‰¯æœ¬ï¼ˆå‘åå…¼å®¹ï¼‰
    default_backbone_path = os.path.join(config.FEATURE_EXTRACTION_DIR, "models", "backbone_pretrained.pth")
    torch.save(backbone.state_dict(), default_backbone_path)
    logger.info(f"  âœ“ é»˜è®¤å‰¯æœ¬: {default_backbone_path}")
    logger.info(f"    (ç”¨äºå‘åå…¼å®¹)")
    
    return backbone, history


def stage2_label_correction_and_augmentation(backbone, X_train, y_train_noisy, y_train_clean, config, logger, stage2_mode='standard'):
    """
    Stage 2: Label correction and data augmentation
    
    Args:
        backbone: Pre-trained frozen backbone
        X_train: (N, L, D) training sequences
        y_train_noisy: (N,) noisy labels
        y_train_clean: (N,) clean labels (for evaluation only)
        config: configuration object
        logger: logger
        
    Returns:
        X_augmented: augmented dataset
        y_augmented: augmented labels
        correction_stats: statistics about correction
    """
    logger.info("")
    logger.info("="*70)
    logger.info("STAGE 2: Label Correction & Data Augmentation")
    logger.info("="*70)
    logger.info(f"ç›®æ ‡: çŸ«æ­£æ ‡ç­¾å™ªå£°å¹¶ç”Ÿæˆå¢å¼ºæ ·æœ¬")
    logger.info(f"æ­¥éª¤: 1) ç‰¹å¾æå– 2) Hybrid Courtæ ‡ç­¾çŸ«æ­£ 3) TabDDPMæ•°æ®å¢å¼º")
    logger.info("")
    logger.info("ğŸ“¥ è¾“å…¥æ•°æ®è·¯å¾„:")
    logger.info(f"  âœ“ è®­ç»ƒæ•°æ®: {config.BENIGN_TRAIN} (æ­£å¸¸), {config.MALICIOUS_TRAIN} (æ¶æ„)")
    backbone_path = os.path.join(config.FEATURE_EXTRACTION_DIR, "models", "backbone_pretrained.pth")
    logger.info(f"  âœ“ éª¨å¹²ç½‘ç»œæ¨¡å‹: {backbone_path}")
    logger.info("")
    
    # Freeze backbone and extract features
    backbone.to(config.DEVICE)  # ç¡®ä¿ backbone åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
    backbone.freeze()
    backbone.eval()
    logger.info("âœ“ éª¨å¹²ç½‘ç»œå·²å†»ç»“ï¼Œå¼€å§‹ç‰¹å¾æå–...")
    
    logger.info(f"æ­£åœ¨ä» {len(X_train)} ä¸ªè®­ç»ƒæ ·æœ¬ä¸­æå–ç‰¹å¾...")
    with torch.no_grad():
        X_tensor = torch.FloatTensor(X_train).to(config.DEVICE)
        
        # Extract features in batches
        features_list = []
        batch_size = 64
        total_batches = (len(X_tensor) + batch_size - 1) // batch_size
        
        for i in range(0, len(X_tensor), batch_size):
            batch_idx = i // batch_size + 1
            X_batch = X_tensor[i:i+batch_size]
            z_batch = backbone(X_batch, return_sequence=False)
            features_list.append(z_batch.cpu().numpy())
            
            if batch_idx % 10 == 0 or batch_idx == total_batches:
                progress = batch_idx / total_batches * 100
                logger.info(f"  ç‰¹å¾æå–è¿›åº¦: {batch_idx}/{total_batches} batches ({progress:.1f}%)")
        
        features = np.concatenate(features_list, axis=0)
    
    logger.info(f"âœ“ ç‰¹å¾æå–å®Œæˆ: {features.shape} (æ ·æœ¬æ•°Ã—ç‰¹å¾ç»´åº¦)")
    logger.info("")
    
    # ç”Ÿæˆç‰¹å¾åˆ†å¸ƒå¯è§†åŒ–ï¼ˆStage 2ç‰¹å¾æå–åï¼‰
    logger.info("ğŸ“Š ç”Ÿæˆç‰¹å¾åˆ†å¸ƒå¯è§†åŒ–...")
    feature_dist_path = os.path.join(config.LABEL_CORRECTION_DIR, "figures", "feature_distribution_stage2.png")
    plot_feature_space(features, y_train_clean, feature_dist_path,
                      title="Stage 2: Feature Distribution (After Backbone Extraction)", method='tsne')
    logger.info(f"  âœ“ ç‰¹å¾åˆ†å¸ƒå›¾ (t-SNE): {feature_dist_path}")
    logger.info("")

    logger.info("="*70)
    logger.info("ğŸ“ ç‰¹å¾å¯åˆ†æ€§è¯„ä¼° (Feature Separability)")
    logger.info("="*70)
    try:
        X_tr, X_te, y_tr, y_te = train_test_split(
            features, y_train_clean,
            test_size=0.2,
            stratify=y_train_clean,
            random_state=config.SEED
        )
        sep_clf = LogisticRegression(max_iter=1000, class_weight='balanced')
        sep_clf.fit(X_tr, y_tr)
        te_proba = sep_clf.predict_proba(X_te)[:, 1]
        te_auc = roc_auc_score(y_te, te_proba)
        te_pred = (te_proba >= 0.5).astype(int)
        te_f1 = f1_score(y_te, te_pred, pos_label=1, zero_division=0)
        sil = silhouette_score(features, y_train_clean) if len(np.unique(y_train_clean)) > 1 else np.nan
        logger.info(f"  LogisticRegression ROC-AUC: {te_auc:.4f}")
        logger.info(f"  LogisticRegression F1@0.5:  {te_f1:.4f}")
        logger.info(f"  Silhouette Score:          {sil:.4f}")
    except Exception as e:
        logger.warning(f"âš  ç‰¹å¾å¯åˆ†æ€§è¯„ä¼°å¤±è´¥: {e}")
    logger.info("")
    logger.info("ğŸ“ è¾“å‡ºæ–‡ä»¶è·¯å¾„:")
    # Save extracted features
    features_path = os.path.join(config.FEATURE_EXTRACTION_DIR, "models", "train_features.npy")
    np.save(features_path, features)
    logger.info(f"  âœ“ æå–çš„ç‰¹å¾: {features_path}")
    
    # Visualize feature space before correction
    save_path = os.path.join(config.FEATURE_EXTRACTION_DIR, "figures", "feature_space_before_correction.png")
    plot_feature_space(features, y_train_clean, save_path, 
                      title="Feature Space (Ground Truth Labels)", method='tsne')
    logger.info(f"  âœ“ ç‰¹å¾ç©ºé—´å¯è§†åŒ–: {save_path}")
    
    # Label correction with Hybrid Court
    logger.info("")
    logger.info("æ­¥éª¤ 2.1: Hybrid Court æ ‡ç­¾å™ªå£°çŸ«æ­£")
    logger.info(f"  è¾“å…¥: {len(y_train_noisy)} ä¸ªæ ·æœ¬ï¼Œå™ªå£°ç‡: {config.LABEL_NOISE_RATE*100:.0f}%")
    logger.info("  æ–¹æ³•: CL (ç½®ä¿¡å­¦ä¹ ) + MADE (å¯†åº¦ä¼°è®¡) + KNN (è¯­ä¹‰æŠ•ç¥¨)")
    logger.info("  å¼€å§‹çŸ«æ­£...")
    
    hybrid_court = HybridCourt(config)

    if stage2_mode == 'clean_augment_only':
        suspected_noise, pred_labels, pred_probs = hybrid_court.cl.fit_predict(features, y_train_clean)
        hybrid_court.made.fit(features, device=config.DEVICE)
        is_dense, density_scores = hybrid_court.made.predict_density(features, device=config.DEVICE)
        hybrid_court.knn.fit(features)
        neighbor_labels, neighbor_consistency = hybrid_court.knn.predict_semantic_label(features, y_train_clean)
        y_corrected = y_train_clean.copy()
        action_mask = np.zeros(len(y_train_clean), dtype=int)
        confidence = pred_probs.max(axis=1)
        correction_weight = np.ones(len(y_train_clean), dtype=np.float32)
        cl_confidence = pred_probs.max(axis=1)
    else:
        y_corrected, action_mask, confidence, correction_weight, density_scores, neighbor_consistency, pred_probs = hybrid_court.correct_labels(
            features, y_train_noisy, device=config.DEVICE
        )
        cl_confidence = pred_probs.max(axis=1)
    
    logger.info("âœ“ æ ‡ç­¾çŸ«æ­£å®Œæˆ")
    
    # Save correction results
    correction_results_path = os.path.join(config.LABEL_CORRECTION_DIR, "models", "correction_results.npz")
    np.savez(correction_results_path,
             y_noisy=y_train_noisy if stage2_mode != 'clean_augment_only' else y_train_clean,
             y_corrected=y_corrected,
             action_mask=action_mask,
             confidence=confidence,
             correction_weight=correction_weight,
             density_scores=density_scores,
             neighbor_consistency=neighbor_consistency,
             pred_probs=pred_probs)
    logger.info("")
    logger.info("ğŸ“ è¾“å‡ºæ–‡ä»¶è·¯å¾„:")
    logger.info(f"  âœ“ æ ‡ç­¾çŸ«æ­£ç»“æœ: {correction_results_path}")
    
    # Visualize correction results
    save_path = os.path.join(config.LABEL_CORRECTION_DIR, "figures", "noise_correction_comparison.png")
    plot_noise_correction_comparison(y_train_clean, y_train_noisy, y_corrected, action_mask, save_path)
    logger.info(f"  âœ“ æ ‡ç­¾çŸ«æ­£å¯¹æ¯”å›¾: {save_path}")
    
    # Calculate correction accuracy
    keep_mask = action_mask != 2  # Exclude dropped samples
    correction_accuracy = (y_corrected[keep_mask] == y_train_clean[keep_mask]).mean()
    logger.info(f"\nLabel Correction Accuracy: {correction_accuracy*100:.2f}%")
    
    correction_stats = {
        'accuracy': correction_accuracy,
        'n_keep': (action_mask == 0).sum(),
        'n_flip': (action_mask == 1).sum(),
        'n_drop': (action_mask == 2).sum(),
        'n_reweight': (action_mask == 3).sum()
    }
    
    # ========================
    # ç”Ÿæˆè¯¦ç»†çš„æ ‡ç­¾çŸ«æ­£åˆ†ææŠ¥å‘Š
    # ========================
    logger.info("")
    logger.info("="*70)
    logger.info("ğŸ“Š ç”Ÿæˆæ ‡ç­¾çŸ«æ­£åˆ†ææŠ¥å‘Š")
    logger.info("="*70)
    
    from datetime import datetime
    
    # å‡†å¤‡åˆ†ææ•°æ®
    n_total = len(y_train_noisy if stage2_mode != 'clean_augment_only' else y_train_clean)
    n_keep = correction_stats['n_keep']
    n_flip = correction_stats['n_flip']
    n_drop = correction_stats['n_drop']
    n_reweight = correction_stats['n_reweight']
    
    # è®¡ç®—å„ç±»åˆ«çš„çŸ«æ­£æƒ…å†µ
    y_noisy_input = y_train_noisy if stage2_mode != 'clean_augment_only' else y_train_clean
    
    # çœŸå®å™ªå£°æ ·æœ¬ï¼ˆground truthï¼‰
    true_noise_mask = (y_noisy_input != y_train_clean)
    n_true_noise = true_noise_mask.sum()
    
    # è¢«è¯†åˆ«ä¸ºå™ªå£°çš„æ ·æœ¬ï¼ˆé¢„æµ‹ï¼‰
    predicted_noise_mask = (action_mask == 1) | (action_mask == 2)
    n_predicted_noise = predicted_noise_mask.sum()
    
    # æ­£ç¡®è¯†åˆ«çš„å™ªå£°ï¼ˆTrue Positiveï¼‰
    tp_noise = (true_noise_mask & predicted_noise_mask).sum()
    # é”™è¯¯è¯†åˆ«çš„å™ªå£°ï¼ˆFalse Positiveï¼‰
    fp_noise = (~true_noise_mask & predicted_noise_mask).sum()
    # æ¼æ£€çš„å™ªå£°ï¼ˆFalse Negativeï¼‰
    fn_noise = (true_noise_mask & ~predicted_noise_mask).sum()
    # æ­£ç¡®ä¿ç•™çš„å¹²å‡€æ ·æœ¬ï¼ˆTrue Negativeï¼‰
    tn_noise = (~true_noise_mask & ~predicted_noise_mask).sum()
    
    # è®¡ç®—å™ªå£°æ£€æµ‹æŒ‡æ ‡
    noise_precision = tp_noise / n_predicted_noise if n_predicted_noise > 0 else 0
    noise_recall = tp_noise / n_true_noise if n_true_noise > 0 else 0
    noise_f1 = 2 * noise_precision * noise_recall / (noise_precision + noise_recall) if (noise_precision + noise_recall) > 0 else 0
    
    # çŸ«æ­£åçš„å‡†ç¡®ç‡
    final_accuracy = (y_corrected[keep_mask] == y_train_clean[keep_mask]).mean()
    
    # å„åŠ¨ä½œçš„å‡†ç¡®ç‡
    keep_samples = (action_mask == 0)
    flip_samples = (action_mask == 1)
    reweight_samples = (action_mask == 3)
    
    keep_accuracy = (y_corrected[keep_samples] == y_train_clean[keep_samples]).mean() if keep_samples.sum() > 0 else 0
    flip_accuracy = (y_corrected[flip_samples] == y_train_clean[flip_samples]).mean() if flip_samples.sum() > 0 else 0
    reweight_accuracy = (y_corrected[reweight_samples] == y_train_clean[reweight_samples]).mean() if reweight_samples.sum() > 0 else 0
    
    # ç”ŸæˆMarkdownæŠ¥å‘Š
    report_path = os.path.join(config.LABEL_CORRECTION_DIR, "models", "correction_analysis_report.md")
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# æ ‡ç­¾å™ªå£°çŸ«æ­£åˆ†ææŠ¥å‘Š\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"**å™ªå£°ç‡**: {config.LABEL_NOISE_RATE*100:.1f}%\n\n")
        f.write(f"**çŸ«æ­£æ–¹æ³•**: Hybrid Court (CL + MADE + KNN)\n\n")
        
        f.write("---\n\n")
        f.write("## 1. æ•°æ®é›†æ¦‚è§ˆ\n\n")
        f.write(f"- **æ€»æ ·æœ¬æ•°**: {n_total}\n")
        f.write(f"- **çœŸå®å™ªå£°æ ·æœ¬æ•°**: {n_true_noise} ({n_true_noise/n_total*100:.2f}%)\n")
        f.write(f"- **å¹²å‡€æ ·æœ¬æ•°**: {n_total - n_true_noise} ({(n_total-n_true_noise)/n_total*100:.2f}%)\n\n")
        
        f.write("### ç±»åˆ«åˆ†å¸ƒ\n\n")
        f.write("| æ ‡ç­¾ç±»å‹ | æ­£å¸¸æ ·æœ¬ | æ¶æ„æ ·æœ¬ | æ€»è®¡ |\n")
        f.write("|---------|---------|---------|------|\n")
        f.write(f"| çœŸå®æ ‡ç­¾ | {(y_train_clean==0).sum()} | {(y_train_clean==1).sum()} | {len(y_train_clean)} |\n")
        f.write(f"| å™ªå£°æ ‡ç­¾ | {(y_noisy_input==0).sum()} | {(y_noisy_input==1).sum()} | {len(y_noisy_input)} |\n")
        f.write(f"| çŸ«æ­£åæ ‡ç­¾ | {(y_corrected==0).sum()} | {(y_corrected==1).sum()} | {len(y_corrected)} |\n\n")
        
        f.write("---\n\n")
        f.write("## 2. çŸ«æ­£åŠ¨ä½œç»Ÿè®¡\n\n")
        f.write("| åŠ¨ä½œ | æ ·æœ¬æ•° | å æ¯” | å‡†ç¡®ç‡ | è¯´æ˜ |\n")
        f.write("|------|--------|------|--------|------|\n")
        f.write(f"| **ä¿æŒ (Keep)** | {n_keep} | {n_keep/n_total*100:.2f}% | {keep_accuracy*100:.2f}% | æ ‡ç­¾å¯ä¿¡ï¼Œä¿æŒä¸å˜ |\n")
        f.write(f"| **ç¿»è½¬ (Flip)** | {n_flip} | {n_flip/n_total*100:.2f}% | {flip_accuracy*100:.2f}% | æ ‡ç­¾é”™è¯¯ï¼Œç¿»è½¬æ ‡ç­¾ |\n")
        f.write(f"| **ä¸¢å¼ƒ (Drop)** | {n_drop} | {n_drop/n_total*100:.2f}% | - | ä¸ç¡®å®šæ€§é«˜ï¼Œä¸¢å¼ƒæ ·æœ¬ |\n")
        f.write(f"| **é‡åŠ æƒ (Reweight)** | {n_reweight} | {n_reweight/n_total*100:.2f}% | {reweight_accuracy*100:.2f}% | å¯ç–‘ä½†ä¿ç•™ï¼Œé™ä½æƒé‡ |\n")
        f.write(f"| **æ€»è®¡** | {n_total} | 100.00% | - | - |\n\n")
        
        f.write("### åŠ¨ä½œåˆ†å¸ƒå¯è§†åŒ–\n\n")
        f.write("```\n")
        f.write(f"ä¿æŒ:   {'â–ˆ' * int(n_keep/n_total*50)} {n_keep/n_total*100:.1f}%\n")
        f.write(f"ç¿»è½¬:   {'â–ˆ' * int(n_flip/n_total*50)} {n_flip/n_total*100:.1f}%\n")
        f.write(f"ä¸¢å¼ƒ:   {'â–ˆ' * int(n_drop/n_total*50)} {n_drop/n_total*100:.1f}%\n")
        f.write(f"é‡åŠ æƒ: {'â–ˆ' * int(n_reweight/n_total*50)} {n_reweight/n_total*100:.1f}%\n")
        f.write("```\n\n")
        
        f.write("---\n\n")
        f.write("## 3. å™ªå£°æ£€æµ‹æ€§èƒ½\n\n")
        f.write("### æ··æ·†çŸ©é˜µï¼ˆå™ªå£°æ£€æµ‹ï¼‰\n\n")
        f.write("| | é¢„æµ‹ä¸ºå¹²å‡€ | é¢„æµ‹ä¸ºå™ªå£° |\n")
        f.write("|---|-----------|----------|\n")
        f.write(f"| **çœŸå®å¹²å‡€** | {tn_noise} (TN) | {fp_noise} (FP) |\n")
        f.write(f"| **çœŸå®å™ªå£°** | {fn_noise} (FN) | {tp_noise} (TP) |\n\n")
        
        f.write("### å™ªå£°æ£€æµ‹æŒ‡æ ‡\n\n")
        f.write(f"- **Precision (ç²¾ç¡®ç‡)**: {noise_precision*100:.2f}% - è¯†åˆ«ä¸ºå™ªå£°çš„æ ·æœ¬ä¸­ï¼ŒçœŸæ­£æ˜¯å™ªå£°çš„æ¯”ä¾‹\n")
        f.write(f"- **Recall (å¬å›ç‡)**: {noise_recall*100:.2f}% - æ‰€æœ‰çœŸå®å™ªå£°ä¸­ï¼Œè¢«æ­£ç¡®è¯†åˆ«çš„æ¯”ä¾‹\n")
        f.write(f"- **F1-Score**: {noise_f1*100:.2f}% - ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡\n\n")
        
        f.write("### æ€§èƒ½è¯„ä¼°\n\n")
        if noise_f1 >= 0.8:
            f.write("âœ… **ä¼˜ç§€** - å™ªå£°æ£€æµ‹æ€§èƒ½å¾ˆå¥½ï¼Œå¤§éƒ¨åˆ†å™ªå£°è¢«æ­£ç¡®è¯†åˆ«\n\n")
        elif noise_f1 >= 0.6:
            f.write("âš ï¸ **è‰¯å¥½** - å™ªå£°æ£€æµ‹æ€§èƒ½å°šå¯ï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´\n\n")
        else:
            f.write("âŒ **éœ€æ”¹è¿›** - å™ªå£°æ£€æµ‹æ€§èƒ½è¾ƒå·®ï¼Œå»ºè®®è°ƒæ•´å‚æ•°æˆ–æ–¹æ³•\n\n")
        
        f.write("---\n\n")
        f.write("## 4. çŸ«æ­£æ•ˆæœè¯„ä¼°\n\n")
        f.write(f"- **çŸ«æ­£å‰å‡†ç¡®ç‡**: {(y_noisy_input == y_train_clean).mean()*100:.2f}%\n")
        f.write(f"- **çŸ«æ­£åå‡†ç¡®ç‡**: {final_accuracy*100:.2f}%\n")
        f.write(f"- **å‡†ç¡®ç‡æå‡**: {(final_accuracy - (y_noisy_input == y_train_clean).mean())*100:.2f}%\n\n")
        
        improvement = final_accuracy - (y_noisy_input == y_train_clean).mean()
        if improvement > 0.1:
            f.write("âœ… **æ˜¾è‘—æ”¹å–„** - æ ‡ç­¾çŸ«æ­£æ˜¾è‘—æå‡äº†æ•°æ®è´¨é‡\n\n")
        elif improvement > 0.05:
            f.write("âœ… **æœ‰æ•ˆæ”¹å–„** - æ ‡ç­¾çŸ«æ­£æœ‰æ•ˆæå‡äº†æ•°æ®è´¨é‡\n\n")
        elif improvement > 0:
            f.write("âš ï¸ **è½»å¾®æ”¹å–„** - æ ‡ç­¾çŸ«æ­£ç•¥å¾®æå‡äº†æ•°æ®è´¨é‡\n\n")
        else:
            f.write("âŒ **æ— æ”¹å–„** - æ ‡ç­¾çŸ«æ­£æœªèƒ½æå‡æ•°æ®è´¨é‡ï¼Œå»ºè®®æ£€æŸ¥å‚æ•°\n\n")
        
        f.write("---\n\n")
        f.write("## 5. å„ç»„ä»¶è´¡çŒ®åˆ†æ\n\n")
        
        # CLç»„ä»¶åˆ†æ
        cl_suspected = (pred_probs.max(axis=1) < 0.5).sum()
        f.write(f"### CL (ç½®ä¿¡å­¦ä¹ )\n\n")
        f.write(f"- **è¯†åˆ«å¯ç–‘æ ·æœ¬æ•°**: {cl_suspected}\n")
        f.write(f"- **å¹³å‡ç½®ä¿¡åº¦**: {pred_probs.max(axis=1).mean():.4f}\n")
        f.write(f"- **ä½ç½®ä¿¡åº¦æ ·æœ¬ (<0.5)**: {cl_suspected} ({cl_suspected/n_total*100:.2f}%)\n\n")
        
        # MADEç»„ä»¶åˆ†æ
        dense_samples = (density_scores > 0.5).sum()
        f.write(f"### MADE (å¯†åº¦ä¼°è®¡)\n\n")
        f.write(f"- **é«˜å¯†åº¦æ ·æœ¬æ•°**: {dense_samples} ({dense_samples/n_total*100:.2f}%)\n")
        f.write(f"- **å¹³å‡å¯†åº¦åˆ†æ•°**: {density_scores.mean():.4f}\n")
        f.write(f"- **ä½å¯†åº¦æ ·æœ¬ (<0.3)**: {(density_scores < 0.3).sum()} ({(density_scores < 0.3).sum()/n_total*100:.2f}%)\n\n")
        
        # KNNç»„ä»¶åˆ†æ
        high_consistency = (neighbor_consistency > 0.7).sum()
        f.write(f"### KNN (è¯­ä¹‰æŠ•ç¥¨)\n\n")
        f.write(f"- **é«˜ä¸€è‡´æ€§æ ·æœ¬æ•°**: {high_consistency} ({high_consistency/n_total*100:.2f}%)\n")
        f.write(f"- **å¹³å‡é‚»å±…ä¸€è‡´æ€§**: {neighbor_consistency.mean():.4f}\n")
        f.write(f"- **ä½ä¸€è‡´æ€§æ ·æœ¬ (<0.5)**: {(neighbor_consistency < 0.5).sum()} ({(neighbor_consistency < 0.5).sum()/n_total*100:.2f}%)\n\n")
        
        f.write("---\n\n")
        f.write("## 6. æ ·æœ¬æƒé‡åˆ†å¸ƒ\n\n")
        f.write(f"- **å¹³å‡æƒé‡**: {correction_weight.mean():.4f}\n")
        f.write(f"- **æƒé‡æ ‡å‡†å·®**: {correction_weight.std():.4f}\n")
        f.write(f"- **æœ€å°æƒé‡**: {correction_weight.min():.4f}\n")
        f.write(f"- **æœ€å¤§æƒé‡**: {correction_weight.max():.4f}\n\n")
        
        f.write("### æƒé‡åˆ†å¸ƒç»Ÿè®¡\n\n")
        f.write("| æƒé‡èŒƒå›´ | æ ·æœ¬æ•° | å æ¯” |\n")
        f.write("|---------|--------|------|\n")
        f.write(f"| [0.0, 0.2) | {((correction_weight >= 0.0) & (correction_weight < 0.2)).sum()} | {((correction_weight >= 0.0) & (correction_weight < 0.2)).sum()/n_total*100:.2f}% |\n")
        f.write(f"| [0.2, 0.4) | {((correction_weight >= 0.2) & (correction_weight < 0.4)).sum()} | {((correction_weight >= 0.2) & (correction_weight < 0.4)).sum()/n_total*100:.2f}% |\n")
        f.write(f"| [0.4, 0.6) | {((correction_weight >= 0.4) & (correction_weight < 0.6)).sum()} | {((correction_weight >= 0.4) & (correction_weight < 0.6)).sum()/n_total*100:.2f}% |\n")
        f.write(f"| [0.6, 0.8) | {((correction_weight >= 0.6) & (correction_weight < 0.8)).sum()} | {((correction_weight >= 0.6) & (correction_weight < 0.8)).sum()/n_total*100:.2f}% |\n")
        f.write(f"| [0.8, 1.0] | {((correction_weight >= 0.8) & (correction_weight <= 1.0)).sum()} | {((correction_weight >= 0.8) & (correction_weight <= 1.0)).sum()/n_total*100:.2f}% |\n\n")
        
        f.write("---\n\n")
        f.write("## 7. å»ºè®®ä¸æ€»ç»“\n\n")
        
        if final_accuracy >= 0.95:
            f.write("### âœ… çŸ«æ­£æ•ˆæœä¼˜ç§€\n\n")
            f.write("- æ ‡ç­¾çŸ«æ­£æ•ˆæœå¾ˆå¥½ï¼Œæ•°æ®è´¨é‡é«˜\n")
            f.write("- å¯ä»¥ç›´æ¥ç”¨äºåç»­è®­ç»ƒ\n\n")
        elif final_accuracy >= 0.85:
            f.write("### âœ… çŸ«æ­£æ•ˆæœè‰¯å¥½\n\n")
            f.write("- æ ‡ç­¾çŸ«æ­£æ•ˆæœè¾ƒå¥½ï¼Œå¤§éƒ¨åˆ†å™ªå£°è¢«å¤„ç†\n")
            f.write("- å»ºè®®å…³æ³¨ä½ç½®ä¿¡åº¦æ ·æœ¬\n\n")
        else:
            f.write("### âš ï¸ çŸ«æ­£æ•ˆæœä¸€èˆ¬\n\n")
            f.write("- æ ‡ç­¾çŸ«æ­£æ•ˆæœæœ‰é™ï¼Œä»æœ‰è¾ƒå¤šå™ªå£°\n")
            f.write("- å»ºè®®è°ƒæ•´ä»¥ä¸‹å‚æ•°ï¼š\n")
            f.write("  - å¢åŠ CLçš„ç½®ä¿¡åº¦é˜ˆå€¼\n")
            f.write("  - è°ƒæ•´MADEçš„å¯†åº¦é˜ˆå€¼\n")
            f.write("  - å¢åŠ KNNçš„é‚»å±…æ•°é‡\n\n")
        
        f.write("### å…³é”®å‘ç°\n\n")
        if noise_precision > 0.8 and noise_recall < 0.6:
            f.write("- **é«˜ç²¾ç¡®ç‡ï¼Œä½å¬å›ç‡**: çŸ«æ­£ç­–ç•¥ä¿å®ˆï¼Œæ¼æ£€äº†éƒ¨åˆ†å™ªå£°\n")
            f.write("  - å»ºè®®: é™ä½ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œå¢åŠ å™ªå£°æ£€æµ‹çš„æ•æ„Ÿåº¦\n\n")
        elif noise_precision < 0.6 and noise_recall > 0.8:
            f.write("- **ä½ç²¾ç¡®ç‡ï¼Œé«˜å¬å›ç‡**: çŸ«æ­£ç­–ç•¥æ¿€è¿›ï¼Œè¯¯åˆ¤äº†éƒ¨åˆ†å¹²å‡€æ ·æœ¬\n")
            f.write("  - å»ºè®®: æé«˜ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œå‡å°‘è¯¯åˆ¤\n\n")
        elif noise_precision > 0.7 and noise_recall > 0.7:
            f.write("- **å¹³è¡¡çš„æ£€æµ‹æ€§èƒ½**: ç²¾ç¡®ç‡å’Œå¬å›ç‡éƒ½è¾ƒé«˜ï¼ŒçŸ«æ­£ç­–ç•¥åˆç†\n\n")
        
        if n_drop > n_total * 0.2:
            f.write("- **ä¸¢å¼ƒæ ·æœ¬è¿‡å¤š**: è¶…è¿‡20%çš„æ ·æœ¬è¢«ä¸¢å¼ƒ\n")
            f.write("  - å»ºè®®: æ£€æŸ¥æ•°æ®è´¨é‡æˆ–è°ƒæ•´ä¸¢å¼ƒé˜ˆå€¼\n\n")
        
        f.write("---\n\n")
        f.write("## 8. è¾“å‡ºæ–‡ä»¶\n\n")
        f.write(f"- **çŸ«æ­£ç»“æœ**: `{correction_results_path}`\n")
        f.write(f"- **å¯¹æ¯”å›¾**: `{os.path.join(config.LABEL_CORRECTION_DIR, 'figures', 'noise_correction_comparison.png')}`\n")
        f.write(f"- **ç‰¹å¾ç©ºé—´å›¾**: `{os.path.join(config.FEATURE_EXTRACTION_DIR, 'figures', 'feature_space_before_correction.png')}`\n")
        f.write(f"- **æœ¬æŠ¥å‘Š**: `{report_path}`\n\n")
        
        f.write("---\n\n")
        f.write("*æŠ¥å‘Šç”±MEDAL-Liteè‡ªåŠ¨ç”Ÿæˆ*\n")
    
    logger.info(f"  âœ“ æ ‡ç­¾çŸ«æ­£åˆ†ææŠ¥å‘Š: {report_path}")
    logger.info("")
    
    # ä½¿ç”¨å…¨éƒ¨å¹²å‡€æ ·æœ¬è¿›è¡Œè®­ç»ƒä¸å¢å¼ºï¼ˆå–æ¶ˆéªŒè¯é›†ï¼‰
    X_clean = X_train[keep_mask]
    y_clean = y_corrected[keep_mask]
    weights_clean = correction_weight[keep_mask]
    action_clean = action_mask[keep_mask]
    density_clean = density_scores[keep_mask]
    cl_conf_clean = cl_confidence[keep_mask]
    knn_conf_clean = neighbor_consistency[keep_mask]

    # Save kept real sequences for Stage 3 mixed-stream training (feature-mode)
    try:
        real_kept_path = os.path.join(config.DATA_AUGMENTATION_DIR, "models", "real_kept_data.npz")
        np.savez(
            real_kept_path,
            X_real=X_clean,
            y_real=y_clean,
            sample_weights_real=weights_clean
        )
        logger.info(f"  âœ“ å·²ä¿å­˜ Stage3 åŒæµè®­ç»ƒæ‰€éœ€çœŸå®åºåˆ—: {real_kept_path}")
    except Exception as e:
        logger.warning(f"âš  æ— æ³•ä¿å­˜ real_kept_data.npzï¼ˆStage3 åŒæµè®­ç»ƒå¯èƒ½ä¸å¯ç”¨ï¼‰: {e}")
    
    stage2_use_tabddpm = bool(getattr(config, 'STAGE2_USE_TABDDPM', True))

    Z_clean = features[keep_mask]

    if not stage2_use_tabddpm:
        logger.info("")
        logger.info("æ­¥éª¤ 2.2: TabDDPM æ•°æ®å¢å¼ºï¼ˆå·²è·³è¿‡ï¼‰")
        logger.info("  è¯´æ˜: å½“å‰é…ç½®å·²ç¦ç”¨ Stage2 TabDDPMï¼ˆæœ¬å·¥ç¨‹ä»…æ”¯æŒ feature-space æµç¨‹ï¼‰")
        logger.info("  å°†ç›´æ¥ä½¿ç”¨éª¨å¹²ç½‘ç»œç‰¹å¾è¿›å…¥ Stage 3")

        Z_augmented = Z_clean
        y_augmented = y_clean
        sample_weights = weights_clean
        n_train_original = int(Z_clean.shape[0])
        return Z_augmented, y_augmented, sample_weights, correction_stats, None, n_train_original
    else:
        logger.info("")
        logger.info("æ­¥éª¤ 2.2: TabDDPM æ•°æ®å¢å¼º (Feature Space)")
        logger.info("  ç›®æ ‡: åœ¨éª¨å¹²ç½‘ç»œç‰¹å¾ç©ºé—´ä¸­è®­ç»ƒ/ç”Ÿæˆï¼Œé¿å…ç ´åæ—¶åºç»“æ„")

        tabddpm = TabDDPM(
            config,
            input_dim=Z_clean.shape[1],
            cond_indices=[],
            dep_indices=list(range(Z_clean.shape[1])),
            enable_protocol_constraints=False
        ).to(config.DEVICE)

        tabddpm.fit_scaler(Z_clean)

        optimizer_ddpm = optim.AdamW(tabddpm.parameters(), lr=1e-4)
        n_epochs_ddpm = int(getattr(config, 'DDPM_EPOCHS', 100))
        ddpm_use_early_stopping = bool(getattr(config, 'DDPM_EARLY_STOPPING', True))
        ddpm_es_warmup_epochs = int(getattr(config, 'DDPM_ES_WARMUP_EPOCHS', 20))
        ddpm_es_patience = int(getattr(config, 'DDPM_ES_PATIENCE', 30))
        ddpm_es_min_delta = float(getattr(config, 'DDPM_ES_MIN_DELTA', 0.001))

        dataset_ddpm = TensorDataset(
            torch.FloatTensor(Z_clean),
            torch.LongTensor(y_clean)
        )
        loader_ddpm = DataLoader(dataset_ddpm, batch_size=2048, shuffle=True)

        ddpm_best_loss = float('inf')
        ddpm_best_epoch = -1
        ddpm_best_state = None
        ddpm_no_improve = 0

        tabddpm.train()
        for epoch in range(n_epochs_ddpm):
            epoch_loss = 0.0
            for Z_batch, y_batch in loader_ddpm:
                z_0 = Z_batch.to(config.DEVICE)
                y_batch = y_batch.to(config.DEVICE)
                optimizer_ddpm.zero_grad()
                loss = tabddpm.compute_loss(
                    z_0, y_batch,
                    mask_prob=float(getattr(config, 'MASK_PROBABILITY', 0.5)),
                    mask_lambda=float(getattr(config, 'MASK_LAMBDA', 0.1)),
                    p_uncond=0.2
                )
                loss.backward()
                optimizer_ddpm.step()
                epoch_loss += float(loss.item())

            avg_loss = epoch_loss / max(len(loader_ddpm), 1)
            logger.info(f"[TabDDPM-Feature] Epoch [{epoch+1}/{n_epochs_ddpm}] | Loss: {avg_loss:.4f}")

            improved = (ddpm_best_loss - float(avg_loss)) > ddpm_es_min_delta
            if improved:
                ddpm_best_loss = float(avg_loss)
                ddpm_best_epoch = int(epoch + 1)
                ddpm_best_state = {k: v.detach().cpu().clone() for k, v in tabddpm.state_dict().items()}
                ddpm_no_improve = 0
            else:
                ddpm_no_improve += 1

            if ddpm_use_early_stopping and (epoch + 1) >= ddpm_es_warmup_epochs and ddpm_no_improve >= ddpm_es_patience:
                logger.info(
                    f"[TabDDPM-Feature] EarlyStopping at epoch {epoch+1}: best_loss={ddpm_best_loss:.4f} (epoch {ddpm_best_epoch})"
                )
                break

        if ddpm_best_state is not None:
            tabddpm.load_state_dict(ddpm_best_state)
            tabddpm.to(config.DEVICE)

        logger.info("æ­¥éª¤ 2.3: ç”Ÿæˆå¢å¼ºç‰¹å¾")
        Z_augmented, y_augmented, sample_weights = tabddpm.augment_feature_dataset(
            Z_clean, y_clean, weights_clean
        )

        n_train_original = int(Z_clean.shape[0])
        n_synthetic = int(Z_augmented.shape[0] - n_train_original)
        logger.info(f"âœ“ ç‰¹å¾å¢å¼ºå®Œæˆ: ä» {n_train_original} å¢å¼ºåˆ° {len(Z_augmented)} (åˆæˆ={n_synthetic})")

        # Save TabDDPM model
        tabddpm_path = os.path.join(config.DATA_AUGMENTATION_DIR, "models", "tabddpm_feature.pth")
        torch.save(tabddpm.state_dict(), tabddpm_path)
        logger.info(f"  âœ“ TabDDPM(Feature)æ¨¡å‹: {tabddpm_path}")

        # Save augmented features
        augmented_data_path = os.path.join(config.DATA_AUGMENTATION_DIR, "models", "augmented_features.npz")
        is_original_mask = np.zeros(len(Z_augmented), dtype=bool)
        is_original_mask[:n_train_original] = True
        np.savez(
            augmented_data_path,
            Z_augmented=Z_augmented,
            y_augmented=y_augmented,
            is_original=is_original_mask,
            n_original=n_train_original,
            sample_weights=sample_weights
        )
        logger.info(f"  âœ“ å¢å¼ºç‰¹å¾: {augmented_data_path}")

        # Feature space quality visualization
        from MoudleCode.utils.visualization import plot_feature_space
        feature_cmp_path = os.path.join(config.DATA_AUGMENTATION_DIR, "figures", "real_vs_synthetic_features_tsne.png")
        plot_feature_space(
            np.concatenate([Z_augmented[:n_train_original], Z_augmented[n_train_original:]], axis=0),
            np.concatenate([y_augmented[:n_train_original], y_augmented[n_train_original:]], axis=0),
            feature_cmp_path,
            title="TabDDPM Feature Generation: Real + Synthetic",
            method='tsne'
        )
        logger.info(f"  âœ“ t-SNEå¯¹æ¯”å›¾(Feature): {feature_cmp_path}")

        return Z_augmented, y_augmented, sample_weights, correction_stats, tabddpm, n_train_original



def stage3_finetune_classifier(backbone, X_train, y_train, sample_weights, config, logger, n_original=None, backbone_path=None):
    """
    Stage 3: Fine-tune dual-stream classifier
    
    Args:
        backbone: Frozen pre-trained backbone
        X_train: (N, L, D) training sequences (augmented, includes original + synthetic)
        y_train: (N,) training labels
        sample_weights: (N,) lifecycle weights from Stage 2 (original + synthetic)
        config: configuration object
        logger: logger
        n_original: int, number of original samples (first n_original in X_train)
                    If None, assumes all samples are original
        backbone_path: str, path to the backbone model used (for metadata recording)
        
    Returns:
        classifier: trained MEDAL classifier
        history: training history
        optimal_threshold: optimal decision threshold
    """
    logger.info("")
    logger.info("="*70)
    logger.info("STAGE 3: Fine-tuning Dual-Stream Classifier")
    logger.info("="*70)
    logger.info(f"ç›®æ ‡: è®­ç»ƒåŒæµMLPåˆ†ç±»å™¨è¿›è¡Œæœ€ç»ˆå¨èƒæ£€æµ‹")
    logger.info(f"è®­ç»ƒè½®æ•°: {config.FINETUNE_EPOCHS} epochs")
    logger.info(f"æ‰¹æ¬¡å¤§å°: {config.FINETUNE_BATCH_SIZE}")
    logger.info(f"å­¦ä¹ ç‡: {config.FINETUNE_LR}")
    logger.info(f"è®­ç»ƒæ ·æœ¬æ•°: {len(X_train)} (åŒ…å«å¢å¼ºæ ·æœ¬)")
    logger.info(f"æŸå¤±ç»„ä»¶: åŠ æƒç›‘ç£")
    logger.info("")
    logger.info("ğŸ“¥ è¾“å…¥æ•°æ®è·¯å¾„:")
    augmented_data_path = os.path.join(config.DATA_AUGMENTATION_DIR, "models", "augmented_features.npz")
    logger.info(f"  âœ“ å¢å¼ºç‰¹å¾: {augmented_data_path}")

    input_is_features = bool(getattr(config, 'CLASSIFIER_INPUT_IS_FEATURES', False))
    try:
        if hasattr(X_train, 'ndim') and int(X_train.ndim) == 2:
            input_is_features = True
    except Exception:
        pass

    finetune_backbone_requested = bool(getattr(config, 'FINETUNE_BACKBONE', False))
    if input_is_features:
        finetune_backbone_requested = False
    finetune_scope = str(getattr(config, 'FINETUNE_BACKBONE_SCOPE', 'projection')).lower()
    finetune_backbone_lr = float(getattr(config, 'FINETUNE_BACKBONE_LR', 1e-5))
    finetune_backbone_warmup_epochs = int(getattr(config, 'FINETUNE_BACKBONE_WARMUP_EPOCHS', 0))
    
    # ä½¿ç”¨ä¼ å…¥çš„backbone_pathå‚æ•°ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
    if backbone_path is None:
        backbone_path_display = os.path.join(config.FEATURE_EXTRACTION_DIR, "models", "backbone_pretrained.pth")
    else:
        backbone_path_display = backbone_path
    logger.info(f"  âœ“ éª¨å¹²ç½‘ç»œæ¨¡å‹: {backbone_path_display}")
    logger.info("")

    if input_is_features:
        config.CLASSIFIER_INPUT_IS_FEATURES = True
        logger.info("ğŸ§© Stage 3: æ£€æµ‹åˆ°è¾“å…¥ä¸ºç‰¹å¾å‘é‡ (B, D)ï¼Œå°†è·³è¿‡éª¨å¹²ç½‘ç»œå¹¶ä»…è®­ç»ƒåˆ†ç±»å¤´")
        logger.info("  - FINETUNE_BACKBONE å°†è¢«å¼ºåˆ¶å…³é—­ï¼ˆåˆæˆç‰¹å¾æ— æ³•åä¼ åˆ°éª¨å¹²ï¼‰")
        logger.info("  - Stage3 åœ¨çº¿å¢å¼º / ST-Mixup å°†è¢«å…³é—­ï¼ˆä»…å¯¹åŸå§‹åºåˆ—æœ‰æ•ˆï¼‰")

    # Create classifier
    classifier = MEDAL_Classifier(backbone, config).to(config.DEVICE)
    logger.info("âœ“ åŒæµåˆ†ç±»å™¨åˆ›å»ºå®Œæˆ (MLP_A + MLP_B)")
    
    # Loss function
    criterion = DualStreamLoss(config)
    logger.info("âœ“ æŸå¤±å‡½æ•°åˆå§‹åŒ–å®Œæˆ")

    logger.info(
        f"ğŸ”§ FocalLoss: alpha={float(getattr(config, 'FOCAL_ALPHA', 0.5))} "
        f"gamma={float(getattr(config, 'FOCAL_GAMMA', 2.0))}"
    )

    augmentor = None
    st_mixup = None
    
    # -------------------- Backbone finetuning policy --------------------
    backbone_param_candidates = []
    if finetune_backbone_requested:
        if finetune_scope == 'all':
            backbone_param_candidates = list(backbone.parameters())
        elif finetune_scope == 'projection':
            if hasattr(backbone, 'projection'):
                backbone_param_candidates = list(backbone.projection.parameters())
            else:
                logger.warning("âš ï¸  FINETUNE_BACKBONE_SCOPE='projection' but backbone has no attribute 'projection'. Falling back to frozen backbone.")
                finetune_backbone_requested = False
        else:
            logger.warning(f"âš ï¸  Unknown FINETUNE_BACKBONE_SCOPE='{finetune_scope}'. Falling back to frozen backbone.")
            finetune_backbone_requested = False

    if finetune_backbone_warmup_epochs < 0:
        finetune_backbone_warmup_epochs = 0

    backbone_finetune_active = False
    backbone_finetune_started = False

    def _set_backbone_trainable(enable: bool) -> None:
        nonlocal backbone_finetune_active, backbone_finetune_started, finetune_backbone_requested
        backbone.freeze()
        if not enable or not finetune_backbone_requested:
            backbone.eval()
            backbone_finetune_active = False
            return

        if finetune_scope == 'all':
            backbone.unfreeze()
        elif finetune_scope == 'projection':
            if hasattr(backbone, 'projection'):
                for p in backbone.projection.parameters():
                    p.requires_grad = True
            else:
                logger.warning("âš ï¸  FINETUNE_BACKBONE_SCOPE='projection' but backbone has no attribute 'projection'. Falling back to frozen backbone.")
                finetune_backbone_requested = False
                backbone.eval()
                backbone_finetune_active = False
                return
        else:
            logger.warning(f"âš ï¸  Unknown FINETUNE_BACKBONE_SCOPE='{finetune_scope}'. Falling back to frozen backbone.")
            finetune_backbone_requested = False
            backbone.eval()
            backbone_finetune_active = False
            return

        backbone.train()
        backbone_finetune_active = True
        backbone_finetune_started = True

    if finetune_backbone_requested and finetune_backbone_warmup_epochs > 0:
        _set_backbone_trainable(False)
        logger.info(
            f"ğŸ§Š Stage 3: Backbone frozen for first {finetune_backbone_warmup_epochs} epochs, "
            f"then finetune (scope={finetune_scope}, lr={finetune_backbone_lr})"
        )
    elif finetune_backbone_requested:
        _set_backbone_trainable(True)
        logger.info(f"ğŸ”¥ Stage 3: Backbone finetuning enabled (scope={finetune_scope})")
    else:
        _set_backbone_trainable(False)

    # -------------------- Optimizer (classifier + optional backbone) --------------------
    param_groups = [
        {'params': classifier.dual_mlp.parameters(), 'lr': float(config.FINETUNE_LR)}
    ]
    if finetune_backbone_requested:
        if len(backbone_param_candidates) == 0:
            logger.warning("âš ï¸  FINETUNE_BACKBONE=True but no trainable backbone params found; backbone will remain frozen.")
            finetune_backbone_requested = False
            _set_backbone_trainable(False)
        else:
            param_groups.append({'params': backbone_param_candidates, 'lr': finetune_backbone_lr})
            logger.info(f"âœ“ ä¼˜åŒ–å™¨åŒ…å«éª¨å¹²ç½‘ç»œå¯è®­ç»ƒå‚æ•°: {len(backbone_param_candidates)} | backbone_lr={finetune_backbone_lr}")

    optimizer = optim.AdamW(
        param_groups,
        weight_decay=config.PRETRAIN_WEIGHT_DECAY
    )
    if finetune_backbone_requested:
        if finetune_backbone_warmup_epochs > 0:
            logger.info("âœ“ ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ (åˆ†ç±»å™¨ + éª¨å¹²ç½‘ç»œè®¡åˆ’å¾®è°ƒ)")
        else:
            logger.info("âœ“ ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ (åˆ†ç±»å™¨ + éª¨å¹²ç½‘ç»œå¾®è°ƒ)")
    else:
        logger.info("âœ“ ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ (ä»…ä¼˜åŒ–åˆ†ç±»å™¨å‚æ•°ï¼Œéª¨å¹²ç½‘ç»œå·²å†»ç»“)")
    
    logger.info("")
    logger.info("ğŸ’¡ è®­ç»ƒæµç¨‹è¯´æ˜:")
    if finetune_backbone_requested:
        if finetune_backbone_warmup_epochs > 0:
            logger.info(f"  å‰{finetune_backbone_warmup_epochs}è½®: éª¨å¹²ç½‘ç»œå†»ç»“ï¼Œä»…è®­ç»ƒåˆ†ç±»å™¨")
            logger.info("  åç»­é˜¶æ®µ: éª¨å¹²ç½‘ç»œå°LRå¾®è°ƒ + åˆ†ç±»å™¨è”åˆè®­ç»ƒ")
        else:
            logger.info("  æ¯ä¸ªbatch: åŸå§‹æ•°æ® â†’ éª¨å¹²ç½‘ç»œ(å¯è®­ç»ƒ) â†’ ç‰¹å¾ â†’ åˆ†ç±»å™¨(è®­ç»ƒ) â†’ æŸå¤±")
            logger.info("  éª¨å¹²ç½‘ç»œå‚æ•°ä¼šéšåˆ†ç±»å™¨ä¸€èµ·æ›´æ–°")
    else:
        logger.info("  æ¯ä¸ªbatch: åŸå§‹æ•°æ® â†’ éª¨å¹²ç½‘ç»œ(å†»ç»“) â†’ ç‰¹å¾ â†’ åˆ†ç±»å™¨(è®­ç»ƒ) â†’ æŸå¤±")
        logger.info("  éª¨å¹²ç½‘ç»œå®æ—¶æå–ç‰¹å¾ï¼Œä½†å‚æ•°ä¸æ›´æ–°")
    logger.info("")
    logger.info("="*70)
    val_split = float(getattr(config, 'FINETUNE_VAL_SPLIT', 0.2))
    val_per_class = int(getattr(config, 'FINETUNE_VAL_PER_CLASS', 0))
    if val_per_class > 0:
        logger.info(f"ğŸ“Š è®­ç»ƒ/éªŒè¯åˆ’åˆ† (val_per_class={val_per_class} per class)")
    elif val_split > 0:
        logger.info(f"ğŸ“Š è®­ç»ƒ/éªŒè¯åˆ’åˆ† (val_split={val_split:.2f})")
    else:
        logger.info("ğŸ“Š å…¨é‡è®­ç»ƒï¼ˆä¸åˆ’åˆ†éªŒè¯é›†ï¼‰")
    logger.info("="*70)
    logger.info(f"  æ ·æœ¬æ€»æ•°: {len(X_train)} ä¸ªæ ·æœ¬ (åŒ…å«åŸå§‹ + åˆæˆ)")
    logger.info("")

    X_val_split = None
    y_val_split = None
    sample_weights_val_split = None

    if val_per_class > 0:
        rng = np.random.RandomState(int(getattr(config, 'SEED', 42)))
        y_np = np.asarray(y_train).astype(int)
        all_idx = np.arange(len(y_np))

        idx0 = all_idx[y_np == 0]
        idx1 = all_idx[y_np == 1]
        n0 = int(min(val_per_class, len(idx0)))
        n1 = int(min(val_per_class, len(idx1)))
        if n0 == 0 or n1 == 0:
            logger.warning("âš ï¸  FINETUNE_VAL_PER_CLASS is set but one class has 0 samples; falling back to no validation split.")
            X_train_split = X_train
            y_train_split = y_train
            sample_weights_split = sample_weights
        else:
            val_idx0 = rng.choice(idx0, size=n0, replace=False)
            val_idx1 = rng.choice(idx1, size=n1, replace=False)
            val_idx = np.concatenate([val_idx0, val_idx1])
            train_idx = np.setdiff1d(all_idx, val_idx, assume_unique=False)

            X_train_split = X_train[train_idx]
            y_train_split = y_train[train_idx]
            sample_weights_split = sample_weights[train_idx]
            X_val_split = X_train[val_idx]
            y_val_split = y_train[val_idx]
            sample_weights_val_split = sample_weights[val_idx]

            logger.info(f"  è®­ç»ƒé›†: {len(X_train_split)}")
            logger.info(f"  éªŒè¯é›†: {len(X_val_split)} (per_class={val_per_class})")
            logger.info("")
    elif val_split > 0:
        from sklearn.model_selection import train_test_split
        X_train_split, X_val_split, y_train_split, y_val_split, sample_weights_split, sample_weights_val_split = train_test_split(
            X_train,
            y_train,
            sample_weights,
            test_size=val_split,
            random_state=int(getattr(config, 'SEED', 42)),
            stratify=y_train
        )
        logger.info(f"  è®­ç»ƒé›†: {len(X_train_split)}")
        logger.info(f"  éªŒè¯é›†: {len(X_val_split)}")
        logger.info("")
    else:
        X_train_split = X_train
        y_train_split = y_train
        sample_weights_split = sample_weights
    
    # ==================== æ¸©å®¤è®­ç»ƒç­–ç•¥ï¼šå¼ºåˆ¶1:1å¹³è¡¡é‡‡æ · ====================
    use_balanced_sampling = getattr(config, 'USE_BALANCED_SAMPLING', True)
    use_hard_mining = False
    
    # Dataset is created once; sampler may be rebuilt during training when hard mining is enabled.
    train_dataset = TensorDataset(
        torch.FloatTensor(X_train_split),
        torch.LongTensor(y_train_split),
        torch.FloatTensor(sample_weights_split)
    )

    def _build_train_loader_with_sampler(_weights_np: np.ndarray):
        from torch.utils.data import WeightedRandomSampler
        sampler = WeightedRandomSampler(
            weights=torch.as_tensor(_weights_np, dtype=torch.double),
            num_samples=len(_weights_np),
            replacement=True
        )
        return DataLoader(
            train_dataset,
            batch_size=config.FINETUNE_BATCH_SIZE,
            sampler=sampler
        )

    if use_balanced_sampling:
        logger.info("ğŸŒ¡ï¸  æ¸©å®¤è®­ç»ƒç­–ç•¥ï¼šå¯ç”¨åŠ æƒå¹³è¡¡é‡‡æ ·")
        target_ratio = float(getattr(config, 'BALANCED_SAMPLING_RATIO', 1.0))
        if not np.isfinite(target_ratio) or target_ratio <= 0.0:
            target_ratio = 1.0
        logger.info(f"  ç›®æ ‡ï¼šæ­£å¸¸:æ¶æ„ = {target_ratio:.2f}:1")
        logger.info("")
        
        class_counts = np.bincount(y_train_split)
        logger.info(f"  åŸå§‹åˆ†å¸ƒ: æ­£å¸¸={class_counts[0]}, æ¶æ„={class_counts[1]}")
        logger.info(f"  åŸå§‹æ¯”ä¾‹: {class_counts[0]}:{class_counts[1]} â‰ˆ {class_counts[0]/class_counts[1]:.1f}:1")
        
        # è®¾ç½®é‡‡æ ·æƒé‡ä»¥è¾¾åˆ° æ­£å¸¸:æ¶æ„ = target_ratio:1
        # å¦‚æœæ­£å¸¸ç±»æœ‰Nä¸ªæ ·æœ¬ï¼Œæ¶æ„ç±»æœ‰Mä¸ªæ ·æœ¬
        # æˆ‘ä»¬å¸Œæœ›é‡‡æ ·åçš„æœŸæœ›æ¯”ä¾‹æ˜¯ target_ratio:1
        # è®¾æ­£å¸¸ç±»æƒé‡ä¸ºw0ï¼Œæ¶æ„ç±»æƒé‡ä¸ºw1
        # åˆ™ (N*w0) : (M*w1) = target_ratio : 1
        # å³ï¼šw0 = target_ratio * M / N * w1
        # è®¾w1 = 1ï¼Œåˆ™ w0 = target_ratio * M / N
        
        n_benign = class_counts[0]
        n_malicious = class_counts[1]
        
        # è®¡ç®—ç±»åˆ«æƒé‡ï¼šæ­£å¸¸ç±»æƒé‡ = target_ratio * (æ¶æ„æ•°é‡ / æ­£å¸¸æ•°é‡)
        weight_benign = target_ratio * n_malicious / max(n_benign, 1)
        weight_malicious = 1.0
        
        class_weights = np.array([weight_benign, weight_malicious])
        sample_sampling_weights = class_weights[y_train_split]
        if weight_benign > weight_malicious:
            weight_note = "æ­£å¸¸æ ·æœ¬æƒé‡æ›´é«˜"
        elif weight_benign < weight_malicious:
            weight_note = "æ¶æ„æ ·æœ¬æƒé‡æ›´é«˜"
        else:
            weight_note = "æƒé‡ç›¸åŒ"
        logger.info(f"  é‡‡æ ·æƒé‡: æ­£å¸¸ç±»={weight_benign:.4f}, æ¶æ„ç±»={weight_malicious:.4f}")
        logger.info(f"  æƒé‡å€¾å‘: {weight_note}")
        logger.info(f"  æœŸæœ›é‡‡æ ·æ¯”ä¾‹: {target_ratio:.2f}:1 (æ­£å¸¸:æ¶æ„)")

        train_loader = _build_train_loader_with_sampler(sample_sampling_weights)
        logger.info(f"  âœ“ å¹³è¡¡é‡‡æ ·å™¨åˆ›å»ºå®Œæˆ")
        logger.info(f"  âœ“ æ¯ä¸ªbatchæœŸæœ›æ¯”ä¾‹: {target_ratio:.2f}:1 (æ­£å¸¸:æ¶æ„)")
    else:
        logger.info("âš ï¸  ä½¿ç”¨åŸå§‹åˆ†å¸ƒè®­ç»ƒï¼ˆæœªå¯ç”¨å¹³è¡¡é‡‡æ ·ï¼‰")
        train_loader = DataLoader(train_dataset, batch_size=config.FINETUNE_BATCH_SIZE, shuffle=True)
    
    logger.info(f"âœ“ æ•°æ®åŠ è½½å™¨å‡†å¤‡å®Œæˆ ({len(train_loader)} ä¸ªæ‰¹æ¬¡)")
    logger.info("")
    logger.info("å¼€å§‹è®­ç»ƒ...")
    logger.info("-"*70)

    use_mixed_stream = False
    
    # Training loop
    history = {
        'train_loss': [],
        'supervision': [],
        'train_f1': [],
        'val_f1': [],
        'val_threshold': [],
        'train_f1_star': [],
        'train_threshold_star': []
    }
    
    classifier.train()

    best_f1 = -1.0
    best_epoch = -1
    best_state = None
    best_threshold = float(getattr(config, 'MALICIOUS_THRESHOLD', 0.5))
    finetuned_backbone_path = None
    
    # Early stopping variables
    use_early_stopping = bool(getattr(config, 'FINETUNE_EARLY_STOPPING', False))
    es_warmup_epochs = int(getattr(config, 'FINETUNE_ES_WARMUP_EPOCHS', 30))
    es_patience = int(getattr(config, 'FINETUNE_ES_PATIENCE', 25))
    es_min_delta = float(getattr(config, 'FINETUNE_ES_MIN_DELTA', 0.002))
    no_improve_count = 0
    best_es_metric = -1.0
    allow_train_metric_es = bool(getattr(config, 'FINETUNE_ES_ALLOW_TRAIN_METRIC', False))
    if use_early_stopping and X_val_split is None and not allow_train_metric_es:
        use_early_stopping = False
        logger.info("âš ï¸  æ—©åœå·²è‡ªåŠ¨ç¦ç”¨ï¼šæœªè®¾ç½®éªŒè¯é›† (FINETUNE_VAL_SPLIT=0)ï¼Œè®­ç»ƒé›†æŒ‡æ ‡ä¸é€‚åˆä½œä¸ºæ—©åœä¾æ®")
        logger.info("")
    
    if use_early_stopping:
        logger.info(f"âœ“ æ—©åœæœºåˆ¶å·²å¯ç”¨:")
        logger.info(f"  - é¢„çƒ­è½®æ•°: {es_warmup_epochs} (å‰{es_warmup_epochs}è½®ä¸è§¦å‘æ—©åœ)")
        logger.info(f"  - è€å¿ƒå€¼: {es_patience} (è¿ç»­{es_patience}è½®ä¸æ”¹å–„åˆ™åœæ­¢)")
        logger.info(f"  - æ”¹å–„é˜ˆå€¼: {es_min_delta:.4f} (F1æ”¹å–„éœ€è¶…è¿‡{es_min_delta*100:.2f}%)")
        logger.info("")
    
    for epoch in range(config.FINETUNE_EPOCHS):
        if finetune_backbone_requested and (not backbone_finetune_active) and (epoch >= finetune_backbone_warmup_epochs):
            _set_backbone_trainable(True)
            logger.info(
                f"ğŸ”¥ Stage 3: Backbone finetuning activated at epoch {epoch + 1} "
                f"(scope={finetune_scope}, lr={finetune_backbone_lr})"
            )
        epoch_loss = 0.0
        epoch_losses = {
            'supervision': 0.0,
        }
        
        # æ”¶é›†è®­ç»ƒé›†é¢„æµ‹ç”¨äºè®¡ç®— F1
        all_train_probs = []
        all_train_labels = []
        
        for X_batch, y_batch, w_batch in train_loader:
            X_batch = X_batch.to(config.DEVICE)
            y_batch = y_batch.to(config.DEVICE)
            w_batch = w_batch.to(config.DEVICE)

            optimizer.zero_grad()

            if input_is_features:
                z = X_batch
            else:
                if backbone_finetune_active:
                    z = backbone(X_batch, return_sequence=False)
                else:
                    with torch.no_grad():
                        z = backbone(X_batch, return_sequence=False)

            loss, loss_dict = criterion(classifier.dual_mlp, z, y_batch, w_batch, epoch, config.FINETUNE_EPOCHS)
            loss.backward()
            optimizer.step()

            epoch_loss += loss_dict['total']
            epoch_losses['supervision'] += loss_dict['supervision']

            with torch.no_grad():
                logits_a, logits_b = classifier.dual_mlp(z, return_separate=True)
                logits_avg = (logits_a + logits_b) / 2.0
                probs = torch.softmax(logits_avg, dim=1)
                all_train_probs.append(probs.cpu().numpy())
                all_train_labels.append(y_batch.cpu().numpy())
        
        n_batches = int(max(len(train_loader), 1))
        epoch_loss /= n_batches
        for key in epoch_losses:
            epoch_losses[key] /= n_batches
        
        # è®¡ç®—è®­ç»ƒé›† Binary F1-Score (å›ºå®šé˜ˆå€¼0.5ï¼Œä»…ç”¨äºå‚è€ƒ)
        train_probs = np.concatenate(all_train_probs)
        train_labels = np.concatenate(all_train_labels)
        train_preds = (train_probs[:, 1] >= 0.5).astype(int)
        from sklearn.metrics import f1_score
        train_f1 = f1_score(train_labels, train_preds, pos_label=1, zero_division=0)

        # è®­ç»ƒé›†å•ç±»F1*(pos=1) & æœ€ä¼˜é˜ˆå€¼ï¼ˆç”¨äºæ— éªŒè¯é›†æ—¶é€‰æ‹© best checkpointï¼‰
        train_f1_star = None
        train_threshold_star = None
        if X_val_split is None:
            from MoudleCode.utils.helpers import find_optimal_threshold
            train_threshold_star, _train_metric, _ = find_optimal_threshold(train_labels, train_probs, metric='f1_binary', positive_class=1)
            train_preds_star = (train_probs[:, 1] >= float(train_threshold_star)).astype(int)
            train_f1_star = f1_score(train_labels, train_preds_star, pos_label=1, zero_division=0)

        val_f1 = None
        val_threshold = None

        if X_val_split is not None:
            classifier.eval()
            all_val_probs = []
            all_val_labels = []
            with torch.no_grad():
                val_dataset = TensorDataset(
                    torch.FloatTensor(X_val_split),
                    torch.LongTensor(y_val_split),
                    torch.FloatTensor(sample_weights_val_split)
                )
                val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
                for X_batch, y_batch, _w_batch in val_loader:
                    X_batch = X_batch.to(config.DEVICE)
                    y_batch = y_batch.to(config.DEVICE)
                    if input_is_features:
                        z = X_batch
                    else:
                        z = backbone(X_batch, return_sequence=False)
                    logits_a, logits_b = classifier.dual_mlp(z, return_separate=True)
                    logits_avg = (logits_a + logits_b) / 2.0
                    probs = torch.softmax(logits_avg, dim=1)
                    all_val_probs.append(probs.cpu().numpy())
                    all_val_labels.append(y_batch.cpu().numpy())

            val_probs = np.concatenate(all_val_probs)
            val_labels = np.concatenate(all_val_labels)

            from MoudleCode.utils.helpers import find_optimal_threshold
            val_threshold, val_metric, _ = find_optimal_threshold(val_labels, val_probs, metric='f1_binary', positive_class=1)
            val_preds = (val_probs[:, 1] >= float(val_threshold)).astype(int)
            val_f1 = f1_score(val_labels, val_preds, pos_label=1, zero_division=0)

            if val_f1 > best_f1:
                best_f1 = float(val_f1)
                best_epoch = int(epoch + 1)
                best_threshold = float(val_threshold)
                best_state = {k: v.detach().cpu().clone() for k, v in classifier.state_dict().items()}
        else:
            if train_f1_star is None:
                raise RuntimeError("Expected train_f1_star to be computed when val_split is disabled")
            if train_f1_star > best_f1:
                best_f1 = float(train_f1_star)
                best_epoch = int(epoch + 1)
                best_threshold = float(train_threshold_star)
                best_state = {k: v.detach().cpu().clone() for k, v in classifier.state_dict().items()}
        
        classifier.eval()
        classifier.train()
        
        # ä¿å­˜å†å²
        history['train_loss'].append(epoch_loss)
        history['supervision'].append(epoch_losses['supervision'])
        history['train_f1'].append(train_f1)
        history['val_f1'].append(val_f1 if val_f1 is not None else np.nan)
        history['val_threshold'].append(val_threshold if val_threshold is not None else np.nan)
        history['train_f1_star'].append(train_f1_star if train_f1_star is not None else np.nan)
        history['train_threshold_star'].append(train_threshold_star if train_threshold_star is not None else np.nan)
        
        # Early stopping check
        if use_early_stopping and (epoch + 1) >= es_warmup_epochs:
            current_metric = val_f1 if val_f1 is not None else train_f1_star
            if current_metric is not None:
                current_metric = float(current_metric)
                if current_metric > (best_es_metric + es_min_delta):
                    best_es_metric = current_metric
                    no_improve_count = 0
                else:
                    no_improve_count += 1
                    
                    if no_improve_count >= es_patience:
                        logger.info("")
                        logger.info("="*70)
                        logger.info(f"ğŸ›‘ æ—©åœè§¦å‘ (Early Stopping Triggered)")
                        logger.info("="*70)
                        logger.info(f"  å½“å‰è½®æ¬¡: Epoch {epoch+1}/{config.FINETUNE_EPOCHS}")
                        logger.info(f"  æœ€ä½³F1: {best_f1:.4f} (Epoch {best_epoch})")
                        logger.info(f"  å½“å‰F1: {current_metric:.4f}")
                        logger.info(f"  è¿ç»­{no_improve_count}è½®æœªæ”¹å–„è¶…è¿‡{es_min_delta:.4f}")
                        logger.info(f"  æå‰ç»ˆæ­¢è®­ç»ƒï¼ŒèŠ‚çœ {config.FINETUNE_EPOCHS - epoch - 1} è½®")
                        logger.info("="*70)
                        logger.info("")
                        break
        
        # æ¯ä¸ªepochéƒ½è¾“å‡ºè¯¦ç»†æ—¥å¿—
        progress = (epoch + 1) / config.FINETUNE_EPOCHS * 100
        sup_str = f" | Sup: {epoch_losses['supervision']:.4f}"
        if val_f1 is not None:
            logger.info(f"[Stage 3] Epoch [{epoch+1}/{config.FINETUNE_EPOCHS}] ({progress:.1f}%) | "
                      f"TrLoss: {epoch_loss:.4f} | "
                      f"TrF1@0.5: {train_f1:.4f} | "
                      f"ValF1*: {val_f1:.4f} | "
                      f"ValTh*: {float(val_threshold):.4f}{sup_str}")
        else:
            train_f1_star_disp = float(train_f1_star) if train_f1_star is not None else float('nan')
            train_th_star_disp = float(train_threshold_star) if train_threshold_star is not None else float('nan')
            logger.info(f"[Stage 3] Epoch [{epoch+1}/{config.FINETUNE_EPOCHS}] ({progress:.1f}%) | "
                      f"TrLoss: {epoch_loss:.4f} | "
                      f"TrF1@0.5: {train_f1:.4f} | "
                      f"TrF1*: {train_f1_star_disp:.4f} | "
                      f"TrTh*: {train_th_star_disp:.4f}{sup_str}")
    
    logger.info("-"*70)
    logger.info("âœ“ Stage 3 å®Œæˆ: åˆ†ç±»å™¨å¾®è°ƒå®Œæˆ")
    actual_epochs = len(history['train_loss'])
    logger.info(f"  è®­ç»ƒé›† - æœ€ç»ˆæŸå¤±: {history['train_loss'][-1]:.4f}, æœ€ç»ˆF1: {history['train_f1'][-1]:.4f}")
    logger.info(f"  å®é™…è®­ç»ƒè½®æ•°: {actual_epochs}/{config.FINETUNE_EPOCHS} epochs")
    if actual_epochs < config.FINETUNE_EPOCHS:
        logger.info(f"  âœ“ æ—©åœç”Ÿæ•ˆï¼ŒèŠ‚çœäº† {config.FINETUNE_EPOCHS - actual_epochs} è½®è®­ç»ƒ")
    if best_epoch > 0:
        if X_val_split is not None:
            logger.info(f"  æœ€ä½³ValF1*(pos=1, auto-threshold): {best_f1:.4f} (epoch {best_epoch}, th={best_threshold:.4f})")
        else:
            logger.info(f"  æœ€ä½³TrF1*(pos=1, auto-threshold): {best_f1:.4f} (epoch {best_epoch}, th={best_threshold:.4f})")
    logger.info("")
    
    # ç”Ÿæˆç‰¹å¾åˆ†å¸ƒå¯è§†åŒ–ï¼ˆStage 3åˆ†ç±»å™¨è®­ç»ƒåï¼‰
    logger.info("ğŸ“Š ç”Ÿæˆç‰¹å¾åˆ†å¸ƒå¯è§†åŒ–...")
    logger.info("  æå–è®­ç»ƒé›†ç‰¹å¾ç”¨äºå¯è§†åŒ–...")
    
    classifier.eval()
    if backbone_finetune_active:
        backbone.eval()
    with torch.no_grad():
        if input_is_features:
            train_features = np.asarray(X_train, dtype=np.float32)
        else:
            # Extract features from training set for visualization
            X_train_tensor = torch.FloatTensor(X_train).to(config.DEVICE)
            features_list = []
            batch_size = 64
            for i in range(0, len(X_train_tensor), batch_size):
                X_batch = X_train_tensor[i:i+batch_size]
                z_batch = backbone(X_batch, return_sequence=False)
                features_list.append(z_batch.cpu().numpy())
            train_features = np.concatenate(features_list, axis=0)
    
    feature_dist_path = os.path.join(config.CLASSIFICATION_DIR, "figures", "feature_distribution_stage3.png")
    plot_feature_space(train_features, y_train, feature_dist_path,
                      title="Stage 3: Feature Distribution (After Classifier Training)", method='tsne')
    logger.info(f"  âœ“ ç‰¹å¾åˆ†å¸ƒå›¾ (t-SNE): {feature_dist_path}")
    logger.info("")
    
    optimal_threshold = float(best_threshold)
    
    logger.info("ğŸ“ è¾“å‡ºæ–‡ä»¶è·¯å¾„:")

    # Save finetuned backbone (never overwrite original backbone checkpoint)
    if backbone_finetune_started:
        finetuned_backbone_path = os.path.join(config.CLASSIFICATION_DIR, "models", "backbone_finetuned.pth")
        torch.save(backbone.state_dict(), finetuned_backbone_path)
        logger.info(f"  âœ“ å¾®è°ƒåçš„éª¨å¹²ç½‘ç»œ: {finetuned_backbone_path}")

    # Save best model by validation F1(pos=1) if validation is enabled
    if best_state is not None:
        # Save best model
        best_path = os.path.join(config.CLASSIFICATION_DIR, "models", "classifier_best_f1.pth")
        torch.save(best_state, best_path)
        logger.info(f"  âœ“ æœ€ä½³æ¨¡å‹: {best_path}")

    # Save final model
    final_path = os.path.join(config.CLASSIFICATION_DIR, "models", "classifier_final.pth")
    torch.save(classifier.dual_mlp.state_dict(), final_path)
    logger.info(f"  âœ“ æœ€ç»ˆæ¨¡å‹: {final_path}")
    
    # Save training history
    history_path = os.path.join(config.CLASSIFICATION_DIR, "models", "training_history.npz")
    np.savez(history_path, **{k: np.array(v) for k, v in history.items()})
    logger.info(f"  âœ“ è®­ç»ƒå†å²: {history_path}")
    
    # Save model metadata (including backbone path used for training)
    # This helps test.py know which backbone to load
    if finetuned_backbone_path is not None:
        backbone_path = finetuned_backbone_path
    elif backbone_path is None:
        backbone_path = os.path.join(config.FEATURE_EXTRACTION_DIR, "models", "backbone_pretrained.pth")
    
    metadata_path = os.path.join(config.CLASSIFICATION_DIR, "models", "model_metadata.json")
    metadata = {
        'backbone_path': backbone_path,  # The actual backbone path used in training
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'n_samples': len(X_train),
        'n_original': n_original if n_original is not None else len(X_train),
        'finetune_epochs': config.FINETUNE_EPOCHS,
        'input_is_features': input_is_features,  # Critical: record whether input was features or sequences
        'feature_dim': int(getattr(config, 'OUTPUT_DIM', config.MODEL_DIM)),  # Backbone output dimension
    }
    import json
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=2)
    logger.info(f"  âœ“ æ¨¡å‹å…ƒæ•°æ®: {metadata_path}")
    logger.info(f"    (è®°å½•äº†ä½¿ç”¨çš„éª¨å¹²ç½‘ç»œ: {os.path.basename(backbone_path)})")
    if input_is_features:
        logger.info(f"    (è®°å½•äº†è¾“å…¥ç±»å‹: ç‰¹å¾å‘é‡, ç»´åº¦={metadata['feature_dim']})")
    
    return classifier, history, optimal_threshold

def main(args):
    """Main training function"""
    
    # Setup
    set_seed(config.SEED)
    config.create_dirs()
    logger = setup_logger(os.path.join(config.OUTPUT_ROOT, "logs"), name='train')
    
    # ==================== å¼ºåˆ¶å¯ç”¨æœ€ä¼˜é…ç½® ====================
    # åŸºäºå®éªŒç»“æœï¼ˆclean_train_test: F1=0.7735 vs stage3_only: F1=0.7350ï¼‰
    # å¼ºåˆ¶å¯ç”¨éª¨å¹²ç½‘ç»œå¾®è°ƒä»¥è·å¾—æœ€ä½³æ€§èƒ½
    config.FINETUNE_BACKBONE = True
    config.FINETUNE_BACKBONE_SCOPE = 'projection'
    config.FINETUNE_BACKBONE_LR = 2e-5
    config.FINETUNE_BACKBONE_WARMUP_EPOCHS = 30
    
    # é”å®šæœ€ä¼˜æŸå¤±é…ç½®
    config.USE_FOCAL_LOSS = True
    config.FOCAL_ALPHA = 0.5
    config.FOCAL_GAMMA = 2.0
    config.USE_MARGIN_LOSS = False
    config.USE_SOFT_F1_LOSS = False
    config.SOFT_ORTH_WEIGHT_START = 0.0
    config.SOFT_ORTH_WEIGHT_END = 0.0
    config.CONSISTENCY_WEIGHT_START = 0.0
    config.CONSISTENCY_WEIGHT_END = 0.0
    config.STAGE3_ONLINE_AUGMENTATION = False
    config.STAGE3_USE_ST_MIXUP = False
    # ==================== æœ€ä¼˜é…ç½®é”å®šå®Œæˆ ====================
    
    logger.info("="*70)
    logger.info("MEDAL-Lite Training Pipeline")
    logger.info("="*70)
    logger.info("âœ… å·²é”å®šæœ€ä¼˜é…ç½®:")
    logger.info(f"  - éª¨å¹²å¾®è°ƒ: {config.FINETUNE_BACKBONE} (scope={config.FINETUNE_BACKBONE_SCOPE}, lr={config.FINETUNE_BACKBONE_LR}, warmup={config.FINETUNE_BACKBONE_WARMUP_EPOCHS})")
    logger.info(f"  - FocalLoss: alpha={config.FOCAL_ALPHA}, gamma={config.FOCAL_GAMMA}")
    logger.info(f"  - å…¶ä»–æŸå¤±: å…¨éƒ¨å…³é—­")
    logger.info("")
    
    # GPUä¿¡æ¯
    if torch.cuda.is_available():
        logger.info(f"âœ“ GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
        logger.info(f"  æ˜¾å­˜æ€»é‡: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
        logger.info(f"  CUDAç‰ˆæœ¬: {torch.version.cuda}")
        logger.info(f"  ä½¿ç”¨è®¾å¤‡: {config.DEVICE}")
    else:
        logger.warning("âš  GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUè®­ç»ƒï¼ˆé€Ÿåº¦ä¼šè¾ƒæ…¢ï¼‰")
        logger.info(f"  ä½¿ç”¨è®¾å¤‡: {config.DEVICE}")
    
    logger.info(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Get stage range first to determine what to run
    start_stage = getattr(args, 'start_stage', 1)
    end_stage = getattr(args, 'end_stage', 3)
    
    # Convert start_stage/end_stage to int if it's a string
    if isinstance(start_stage, str):
        try:
            start_stage = int(start_stage)
        except ValueError:
            logger.error(f"âŒ æ— æ•ˆçš„èµ·å§‹é˜¶æ®µ: {start_stage}")
            return

    if isinstance(end_stage, str):
        try:
            end_stage = int(end_stage)
        except ValueError:
            logger.error(f"âŒ æ— æ•ˆçš„ç»“æŸé˜¶æ®µ: {end_stage}")
            return

    if end_stage < start_stage:
        logger.error(f"âŒ æ— æ•ˆé˜¶æ®µèŒƒå›´: start_stage={start_stage} > end_stage={end_stage}")
        return
    
    # ========================
    # Load Dataset (needed for Stage 1/2, and also for Stage 3-only runs)
    # ========================
    X_train = None
    y_train_clean = None
    y_train_noisy = None
    
    if start_stage <= 3:
        # Need to load raw dataset for Stage 1/2, and for Stage 3-only (skip Stage 2)
        logger.info("\n" + "="*70)
        logger.info("ğŸ“¦ æ•°æ®é›†åŠ è½½ Dataset Loading")
        logger.info("="*70)
        logger.info(f"è®­ç»ƒé›†é…ç½®:")
        logger.info(f"  æ­£å¸¸æµé‡è·¯å¾„: {config.BENIGN_TRAIN}")
        logger.info(f"  æ¶æ„æµé‡è·¯å¾„: {config.MALICIOUS_TRAIN}")
        logger.info(f"  åºåˆ—é•¿åº¦: {config.SEQUENCE_LENGTH} ä¸ªæ•°æ®åŒ…")
        logger.info(f"  è¯´æ˜: å°†è¯»å–ä¸Šè¿°è·¯å¾„ä¸‹æ‰€æœ‰pcapæ–‡ä»¶ï¼Œæµæ•°åœ¨å¤„ç†æ—¶ç»Ÿè®¡")
        logger.info("")
        
        # ä¼˜å…ˆä½¿ç”¨é¢„å¤„ç†å¥½çš„æ•°æ®
        if PREPROCESS_AVAILABLE and check_preprocessed_exists('train'):
            logger.info("âœ“ å‘ç°é¢„å¤„ç†æ–‡ä»¶ï¼Œç›´æ¥åŠ è½½...")
            X_train, y_train_clean, train_files = load_preprocessed('train')
            X_train = normalize_burstsize_inplace(X_train)
            logger.info(f"  ä»é¢„å¤„ç†æ–‡ä»¶åŠ è½½: {X_train.shape[0]} ä¸ªæ ·æœ¬")
        else:
            # ä»PCAPæ–‡ä»¶åŠ è½½
            logger.info("å¼€å§‹åŠ è½½è®­ç»ƒæ•°æ®é›†ï¼ˆä»PCAPæ–‡ä»¶ï¼‰...")
            logger.info("ğŸ’¡ æç¤º: è¿è¡Œ 'python preprocess.py' å¯é¢„å¤„ç†æ•°æ®ï¼ŒåŠ é€Ÿåç»­è®­ç»ƒ")
            X_train, y_train_clean, train_files = load_dataset(
                benign_dir=config.BENIGN_TRAIN,
                malicious_dir=config.MALICIOUS_TRAIN,
                sequence_length=config.SEQUENCE_LENGTH
            )
            X_train = normalize_burstsize_inplace(X_train)
        
        if X_train is None:
            logger.error("âŒ è®­ç»ƒæ•°æ®é›†åŠ è½½å¤±è´¥!")
            return
        
        logger.info("")
        logger.info("âœ“ è®­ç»ƒæ•°æ®é›†åŠ è½½å®Œæˆ")
        logger.info(f"  æ•°æ®å½¢çŠ¶: {X_train.shape} (æ ·æœ¬æ•°Ã—åºåˆ—é•¿åº¦Ã—ç‰¹å¾ç»´åº¦)")
        logger.info(f"  æ­£å¸¸æ ·æœ¬: {(y_train_clean==0).sum()} ä¸ª")
        logger.info(f"  æ¶æ„æ ·æœ¬: {(y_train_clean==1).sum()} ä¸ª")
        logger.info("")
        
        # æ³¨æ„ï¼šStage 1 (ç‰¹å¾æå–) ä¸éœ€è¦æ ‡ç­¾ï¼Œæ˜¯æ— ç›‘ç£å­¦ä¹ 
        # åªæœ‰ Stage 2 (æ ‡ç­¾çŸ«æ­£) å’Œ Stage 3 (åˆ†ç±») æ‰éœ€è¦æ ‡ç­¾
        # å› æ­¤ï¼Œæ ‡ç­¾å™ªå£°æ³¨å…¥å»¶è¿Ÿåˆ°éœ€è¦æ—¶å†è¿›è¡Œ
        if start_stage >= 2 and start_stage != 3:
            # Inject label noise (only needed for Stage 2, NOT for Stage 3-only)
            logger.info(f"ğŸ”€ æ³¨å…¥æ ‡ç­¾å™ªå£° ({config.LABEL_NOISE_RATE*100:.0f}%)...")
            y_train_noisy, noise_mask = inject_label_noise(y_train_clean, config.LABEL_NOISE_RATE)
            logger.info(f"âœ“ å™ªå£°æ ‡ç­¾åˆ›å»ºå®Œæˆ: {noise_mask.sum()} ä¸ªæ ‡ç­¾è¢«ç¿»è½¬")
            logger.info(f"  åŸå§‹æ ‡ç­¾åˆ†å¸ƒ: æ­£å¸¸={(y_train_clean==0).sum()}, æ¶æ„={(y_train_clean==1).sum()}")
            logger.info(f"  å™ªå£°æ ‡ç­¾åˆ†å¸ƒ: æ­£å¸¸={(y_train_noisy==0).sum()}, æ¶æ„={(y_train_noisy==1).sum()}")
        else:
            # Stage 1 ä¸éœ€è¦æ ‡ç­¾ï¼ŒStage 3-only ä¸éœ€è¦å™ªå£°æ ‡ç­¾
            if start_stage == 1:
                logger.info("ğŸ’¡ Stage 1 (ç‰¹å¾æå–) æ˜¯æ— ç›‘ç£å­¦ä¹ ï¼Œä¸ä½¿ç”¨æ ‡ç­¾")
            elif start_stage == 3:
                logger.info("ğŸ’¡ Stage 3-only: ä½¿ç”¨å¹²å‡€æ ‡ç­¾ï¼ˆä¸æ³¨å…¥å™ªå£°ï¼‰")
            y_train_noisy = None
            noise_mask = None
    else:
        # Starting from later stages, skip raw dataset loading
        logger.info("\n" + "="*70)
        logger.info("â­ï¸  è·³è¿‡åŸå§‹æ•°æ®é›†åŠ è½½")
        logger.info("="*70)
        logger.info("")
    
    # ========================
    # Stage 1: Pre-train Backbone (unsupervised, no labels needed)
    # ========================
    backbone = build_backbone(config, logger=logger)
    backbone = backbone.to(config.DEVICE)  # Ensure all layers are on correct device
    
    if start_stage <= 1:
        # æ ¹æ®å¯¹æ¯”å­¦ä¹ æ–¹æ³•é€‰æ‹©æ‰¹æ¬¡å¤§å°
        use_instance_contrastive = getattr(config, 'USE_INSTANCE_CONTRASTIVE', False)
        contrastive_method = getattr(config, 'CONTRASTIVE_METHOD', 'infonce')
        
        method_lower = str(contrastive_method).lower()
        if use_instance_contrastive and method_lower == 'nnclr':
            # NNCLR éœ€è¦æ›´å°çš„æ‰¹æ¬¡ä»¥é¿å…æ˜¾å­˜æº¢å‡º
            batch_size = getattr(config, 'PRETRAIN_BATCH_SIZE_NNCLR', 64)
            logger.info(f"âœ“ æ£€æµ‹åˆ° NNCLR æ–¹æ³•ï¼Œä½¿ç”¨ä¸“ç”¨æ‰¹æ¬¡å¤§å°: {batch_size}")
            logger.info(f"  (NNCLR æ˜¾å­˜å ç”¨é«˜ï¼Œéœ€è¦å‡å°æ‰¹æ¬¡)")
        elif use_instance_contrastive and method_lower == 'simsiam':
            batch_size = getattr(config, 'PRETRAIN_BATCH_SIZE_SIMSIAM', config.PRETRAIN_BATCH_SIZE)
            logger.info(f"âœ“ æ£€æµ‹åˆ° SimSiam æ–¹æ³•ï¼Œä½¿ç”¨ä¸“ç”¨æ‰¹æ¬¡å¤§å°: {batch_size}")
        else:
            # SimMTM æˆ– InfoNCE ä½¿ç”¨é»˜è®¤æ‰¹æ¬¡å¤§å°
            batch_size = config.PRETRAIN_BATCH_SIZE
        
        # SimMTM is unsupervised, so we only need X_train, not labels
        dataset = TensorDataset(torch.FloatTensor(X_train))
        train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        backbone, pretrain_history = stage1_pretrain_backbone(backbone, train_loader, config, logger)
        # Backbone is already saved in stage1_pretrain_backbone function
        if end_stage <= 1:
            logger.info("\n" + "="*70)
            logger.info("âœ… å·²å®Œæˆåˆ° Stage 1ï¼ŒæŒ‰ end_stage è®¾ç½®æå‰ç»“æŸ")
            logger.info("="*70)
            return backbone
    else:
        # Load pre-trained backbone (required for Stage 2 and 3)
        # ä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„backbone_path
        if hasattr(args, 'backbone_path') and args.backbone_path:
            backbone_path = args.backbone_path
            logger.info(f"ä½¿ç”¨æŒ‡å®šçš„éª¨å¹²ç½‘ç»œ: {backbone_path}")
        else:
            backbone_path = os.path.join(config.FEATURE_EXTRACTION_DIR, "models", "backbone_pretrained.pth")
        
        retrain_backbone = bool(getattr(args, 'retrain_backbone', False))
        if retrain_backbone:
            logger.warning("âš  --retrain_backbone å·²æŒ‡å®šï¼šå°†ä½¿ç”¨éšæœºåˆå§‹åŒ–éª¨å¹²ç½‘ç»œï¼ˆä¸åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼‰")
            backbone.freeze()
        else:
            if os.path.exists(backbone_path):
                logger.info("ğŸ“¥ è¾“å…¥æ•°æ®è·¯å¾„:")
                logger.info(f"  âœ“ éª¨å¹²ç½‘ç»œæ¨¡å‹: {backbone_path}")
                logger.info("")
                logger.info(f"âœ“ åŠ è½½å·²æœ‰éª¨å¹²ç½‘ç»œ: {backbone_path}")
                try:
                    backbone_state = torch.load(backbone_path, map_location=config.DEVICE, weights_only=True)
                except TypeError:
                    backbone_state = torch.load(backbone_path, map_location=config.DEVICE)
                load_state_dict_shape_safe(backbone, backbone_state, logger, prefix="backbone")
                logger.info(f"âœ“ åŠ è½½å·²æœ‰éª¨å¹²ç½‘ç»œ: {backbone_path}")
            else:
                logger.error(f"âŒ æ‰¾ä¸åˆ°é¢„è®­ç»ƒéª¨å¹²ç½‘ç»œ: {backbone_path}")
                logger.error("   è¯·å…ˆè¿è¡Œ Stage 1 æˆ–ä» Stage 1 å¼€å§‹è®­ç»ƒ")
                return
    
    # ========================
    # Stage 2: Label Correction & Augmentation
    # ========================
    if start_stage <= 2 and end_stage >= 2:
        # å¦‚æœä»Stage 2å¼€å§‹ï¼Œä½†è¿˜æ²¡æœ‰æ³¨å…¥å™ªå£°ï¼Œç°åœ¨æ³¨å…¥
        if y_train_noisy is None and y_train_clean is not None:
            logger.info("")
            logger.info(f"ğŸ”€ æ³¨å…¥æ ‡ç­¾å™ªå£° ({config.LABEL_NOISE_RATE*100:.0f}%)...")
            y_train_noisy, noise_mask = inject_label_noise(y_train_clean, config.LABEL_NOISE_RATE)
            logger.info(f"âœ“ å™ªå£°æ ‡ç­¾åˆ›å»ºå®Œæˆ: {noise_mask.sum()} ä¸ªæ ‡ç­¾è¢«ç¿»è½¬")
            logger.info(f"  åŸå§‹æ ‡ç­¾åˆ†å¸ƒ: æ­£å¸¸={(y_train_clean==0).sum()}, æ¶æ„={(y_train_clean==1).sum()}")
            logger.info(f"  å™ªå£°æ ‡ç­¾åˆ†å¸ƒ: æ­£å¸¸={(y_train_noisy==0).sum()}, æ¶æ„={(y_train_noisy==1).sum()}")
        
        stage2_mode = getattr(args, 'stage2_mode', 'standard')
        X_augmented, y_augmented, sample_weights, correction_stats, tabddpm, n_original = stage2_label_correction_and_augmentation(
            backbone, X_train, y_train_noisy, y_train_clean, config, logger, stage2_mode=stage2_mode
        )
        # TabDDPM is already saved in stage2_label_correction_and_augmentation function
        if end_stage <= 2:
            logger.info("\n" + "="*70)
            logger.info("âœ… å·²å®Œæˆåˆ° Stage 2ï¼ŒæŒ‰ end_stage è®¾ç½®æå‰ç»“æŸ")
            logger.info("="*70)
            return backbone
    elif end_stage >= 3:
        # è·³è¿‡Stage 2ï¼š
        # - å¦‚æœæ˜¯ Stage 3-only(start_stage==3)ï¼Œæ ¹æ® FINETUNE_BACKBONE é…ç½®å†³å®šä½¿ç”¨åŸå§‹åºåˆ—è¿˜æ˜¯ç‰¹å¾
        # - å¦åˆ™ï¼šå…¼å®¹æ—§æµç¨‹ï¼ˆå¦‚æœå­˜åœ¨ augmented_data.npz åˆ™åŠ è½½ï¼‰
        augmented_data_path = os.path.join(config.DATA_AUGMENTATION_DIR, "models", "augmented_features.npz")
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨éª¨å¹²ç½‘ç»œå¾®è°ƒ
        finetune_backbone_enabled = bool(getattr(config, 'FINETUNE_BACKBONE', False))
        logger.info(f"ğŸ” è°ƒè¯•ä¿¡æ¯: FINETUNE_BACKBONE = {finetune_backbone_enabled}")
        
        if int(start_stage) == 3 and X_train is not None:
            logger.info("\n" + "="*70)
            logger.info("âœ… Stage 3-only: å·²è·³è¿‡ Stage 2ï¼ˆä¸ä½¿ç”¨ TabDDPM ç¦»çº¿å¢å¼ºæ•°æ®ï¼‰")
            logger.info("="*70)
            logger.info("")
            
            if finetune_backbone_enabled:
                # ä½¿ç”¨åŸå§‹åºåˆ—ä»¥æ”¯æŒéª¨å¹²ç½‘ç»œå¾®è°ƒ
                X_augmented = X_train  # ä¿æŒåŸå§‹åºåˆ—æ ¼å¼ (N, L, D)
                y_augmented = y_train_clean  # Use clean labels for Stage 3-only
                sample_weights = np.ones(len(X_train), dtype=np.float32)
                n_original = len(X_train)
                logger.info(f"âœ“ ä½¿ç”¨åŸå§‹åºåˆ—æ•°æ®: {len(X_train)} ä¸ªæ ·æœ¬ (æ”¯æŒéª¨å¹²ç½‘ç»œå¾®è°ƒ)")
                logger.info(f"  æ•°æ®å½¢çŠ¶: {X_train.shape} (æ ·æœ¬æ•°Ã—åºåˆ—é•¿åº¦Ã—ç‰¹å¾ç»´åº¦)")
            else:
                # æå–ç‰¹å¾å‘é‡ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
                X_train_tensor = torch.FloatTensor(X_train).to(config.DEVICE)
                with torch.no_grad():
                    backbone = backbone.to(config.DEVICE)  # Ensure backbone is on correct device
                    Z_clean = backbone(X_train_tensor, return_sequence=False).detach().cpu().numpy().astype(np.float32)

                X_augmented = Z_clean
                y_augmented = y_train_clean  # Use clean labels for Stage 3-only
                sample_weights = np.ones(len(Z_clean), dtype=np.float32)
                n_original = len(Z_clean)
                logger.info(f"âœ“ ä½¿ç”¨åŸå§‹å¹²å‡€æ•°æ®(ç‰¹å¾): {len(Z_clean)} ä¸ªæ ·æœ¬")
            
            if os.path.exists(augmented_data_path):
                logger.info(f"  (å·²å¿½ç•¥ç¦»çº¿å¢å¼ºæ–‡ä»¶: {augmented_data_path})")
        elif os.path.exists(augmented_data_path):
            # åŠ è½½å·²æœ‰çš„å¢å¼ºæ•°æ®ï¼ˆå…¼å®¹æ—§æµç¨‹ï¼‰
            logger.info("ğŸ“¥ è¾“å…¥æ•°æ®è·¯å¾„:")
            logger.info(f"  âœ“ å¢å¼ºç‰¹å¾: {augmented_data_path}")
            backbone_path = os.path.join(config.FEATURE_EXTRACTION_DIR, "models", "backbone_pretrained.pth")
            logger.info(f"  âœ“ éª¨å¹²ç½‘ç»œæ¨¡å‹: {backbone_path}")
            logger.info("")
            logger.info(f"âœ“ åŠ è½½å·²æœ‰å¢å¼ºç‰¹å¾: {augmented_data_path}")
            data = np.load(augmented_data_path)
            X_augmented = data['Z_augmented']
            y_augmented = data['y_augmented']
            sample_weights = data['sample_weights'] if 'sample_weights' in data else np.ones(len(X_augmented))
            # Get n_original from saved data
            if 'n_original' in data:
                n_original = int(data['n_original'])
            else:
                # Fallback: assume all data is original if metadata not found
                n_original = len(X_augmented)
                logger.warning("âš ï¸  æœªæ‰¾åˆ°åŸå§‹æ•°æ®æ•°é‡æ ‡è®°ï¼Œå‡è®¾æ‰€æœ‰æ•°æ®å‡ä¸ºåŸå§‹æ•°æ®")
        elif X_train is not None:
            # ä½¿ç”¨åŸå§‹æ•°æ®ï¼ˆä¸åšå¢å¼ºå’ŒçŸ«æ­£ï¼‰
            # æ³¨æ„ï¼šè¿™ä¸ªåˆ†æ”¯ä¸åº”è¯¥åœ¨ Stage 3-only æ—¶æ‰§è¡Œï¼ˆå·²åœ¨ä¸Šé¢å¤„ç†ï¼‰
            logger.info("\n" + "="*70)
            logger.info("âš ï¸  è·³è¿‡ Stage 2ï¼Œä½¿ç”¨åŸå§‹æ•°æ®ï¼ˆå¸¦å™ªå£°æ ‡ç­¾ï¼‰")
            logger.info("="*70)
            logger.info("")
            
            # å¦‚æœè¿˜æ²¡æœ‰æ³¨å…¥å™ªå£°ï¼Œç°åœ¨æ³¨å…¥ï¼ˆä½† Stage 3-only ä¸ä¼šèµ°åˆ°è¿™é‡Œï¼‰
            if y_train_noisy is None and start_stage != 3:
                logger.info(f"ğŸ”€ æ³¨å…¥æ ‡ç­¾å™ªå£° ({config.LABEL_NOISE_RATE*100:.0f}%)...")
                y_train_noisy, noise_mask = inject_label_noise(y_train_clean, config.LABEL_NOISE_RATE)
                logger.info(f"âœ“ å™ªå£°æ ‡ç­¾åˆ›å»ºå®Œæˆ: {noise_mask.sum()} ä¸ªæ ‡ç­¾è¢«ç¿»è½¬")
                logger.info(f"  åŸå§‹æ ‡ç­¾åˆ†å¸ƒ: æ­£å¸¸={(y_train_clean==0).sum()}, æ¶æ„={(y_train_clean==1).sum()}")
                logger.info(f"  å™ªå£°æ ‡ç­¾åˆ†å¸ƒ: æ­£å¸¸={(y_train_noisy==0).sum()}, æ¶æ„={(y_train_noisy==1).sum()}")
            
            X_train_tensor = torch.FloatTensor(X_train).to(config.DEVICE)
            with torch.no_grad():
                backbone = backbone.to(config.DEVICE)  # Ensure backbone is on correct device
                Z_clean = backbone(X_train_tensor, return_sequence=False).detach().cpu().numpy().astype(np.float32)

            X_augmented = Z_clean
            y_augmented = y_train_noisy if y_train_noisy is not None else y_train_clean
            sample_weights = np.ones(len(Z_clean), dtype=np.float32)
            n_original = len(Z_clean)
            
            logger.info(f"âœ“ ä½¿ç”¨åŸå§‹æ•°æ®(ç‰¹å¾): {len(Z_clean)} ä¸ªæ ·æœ¬")
            logger.info("  âš ï¸  æ³¨æ„ï¼šæœªè¿›è¡Œæ ‡ç­¾çŸ«æ­£å’Œæ•°æ®å¢å¼ºï¼Œå¯èƒ½å½±å“æ€§èƒ½")
        else:
            logger.error(f"âŒ æ‰¾ä¸åˆ°å¢å¼ºæ•°æ®: {augmented_data_path}")
            logger.error("   ä¸”æœªåŠ è½½åŸå§‹æ•°æ®")
            logger.error("   è¯·å…ˆè¿è¡Œ Stage 2 æˆ–ä» Stage 1 å¼€å§‹è®­ç»ƒ")
            return
    
    # ========================
    # Stage 3: Fine-tune Classifier
    # ========================
    if end_stage >= 3 and start_stage <= 3:
        # ç¡®å®šå®é™…ä½¿ç”¨çš„backboneè·¯å¾„ï¼ˆç”¨äºè®°å½•å…ƒæ•°æ®ï¼‰
        if hasattr(args, 'backbone_path') and args.backbone_path:
            actual_backbone_path = args.backbone_path
        else:
            actual_backbone_path = os.path.join(config.FEATURE_EXTRACTION_DIR, "models", "backbone_pretrained.pth")
        
        classifier, finetune_history, optimal_threshold = stage3_finetune_classifier(
            backbone, X_augmented, y_augmented, sample_weights, config, logger, 
            n_original=n_original, backbone_path=actual_backbone_path
        )
    else:
        logger.info("\n" + "="*70)
        logger.info("â­ï¸  è·³è¿‡ Stage 3ï¼ˆæŒ‰ end_stage è®¾ç½®ï¼‰")
        logger.info("="*70)
        return backbone
    
    # Plot training history
    history_fig_path = os.path.join(config.CLASSIFICATION_DIR, "figures", "training_history.png")
    plot_training_history(finetune_history, history_fig_path)
    logger.info(f"  âœ“ è®­ç»ƒå†å²å›¾è¡¨: {history_fig_path}")
    
    logger.info("")
    logger.info("="*70)
    logger.info("ğŸ‰ è®­ç»ƒå®Œæˆ! Training Complete!")
    logger.info("="*70)
    logger.info("")
    logger.info("ğŸ“Š è®­ç»ƒæ€»ç»“:")
    logger.info(f"  Stage 1: éª¨å¹²ç½‘ç»œé¢„è®­ç»ƒ - {config.PRETRAIN_EPOCHS} epochs")
    logger.info(f"  Stage 2: æ ‡ç­¾çŸ«æ­£+æ•°æ®å¢å¼º - å®Œæˆ")
    logger.info(f"  Stage 3: åˆ†ç±»å™¨å¾®è°ƒ - {config.FINETUNE_EPOCHS} epochs")
    logger.info("")
    logger.info("ğŸ“¥ è¾“å…¥æ•°æ®è·¯å¾„:")
    logger.info(f"  âœ“ è®­ç»ƒæ•°æ®: {config.BENIGN_TRAIN} (æ­£å¸¸), {config.MALICIOUS_TRAIN} (æ¶æ„)")
    logger.info("")
    logger.info("ğŸ“ è¾“å‡ºæ–‡ä»¶è·¯å¾„:")
    logger.info(f"  âœ“ ç‰¹å¾æå–: {config.FEATURE_EXTRACTION_DIR}")
    logger.info(f"    - éª¨å¹²ç½‘ç»œ: {os.path.join(config.FEATURE_EXTRACTION_DIR, 'models', 'backbone_pretrained.pth')}")
    logger.info(f"    - è®­ç»ƒç‰¹å¾: {os.path.join(config.FEATURE_EXTRACTION_DIR, 'models', 'train_features.npy')}")
    logger.info(f"  âœ“ æ ‡ç­¾çŸ«æ­£: {config.LABEL_CORRECTION_DIR}")
    logger.info(f"    - çŸ«æ­£ç»“æœ: {os.path.join(config.LABEL_CORRECTION_DIR, 'models', 'correction_results.npz')}")
    logger.info(f"  âœ“ æ•°æ®å¢å¼º: {config.DATA_AUGMENTATION_DIR}")
    logger.info(f"    - TabDDPMæ¨¡å‹: {os.path.join(config.DATA_AUGMENTATION_DIR, 'models', 'tabddpm_feature.pth')}")
    logger.info(f"    - å¢å¼ºç‰¹å¾: {os.path.join(config.DATA_AUGMENTATION_DIR, 'models', 'augmented_features.npz')}")
    logger.info(f"  âœ“ åˆ†ç±»å™¨:   {config.CLASSIFICATION_DIR}")
    logger.info(f"    - åˆ†ç±»å™¨æ¨¡å‹: {os.path.join(config.CLASSIFICATION_DIR, 'models', 'classifier_final.pth')}")
    logger.info(f"    - è®­ç»ƒå†å²: {os.path.join(config.CLASSIFICATION_DIR, 'models', 'training_history.npz')}")
    logger.info("")
    logger.info("ğŸ’¡ ä¸‹ä¸€æ­¥: è¿è¡Œ test.py è¯„ä¼°æ¨¡å‹æ€§èƒ½")
    logger.info("="*70)
    
    return classifier


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train MEDAL-Lite model")
    parser.add_argument("--noise_rate", type=float, default=0.30, help="Label noise rate")
    parser.add_argument("--start_stage", type=int, default=1, choices=[1, 2, 3], 
                       help="Start from which stage (1=backbone pretrain, 2=label correction, 3=classifier finetune)")
    parser.add_argument("--end_stage", type=int, default=3, choices=[1, 2, 3],
                       help="End at which stage (1/2/3). Use end_stage=2 for Stage2-only run")
    parser.add_argument("--stage2_mode", type=str, default="standard", choices=["standard", "clean_augment_only"])
    parser.add_argument("--retrain_backbone", action="store_true",
                       help="Use randomly initialized backbone instead of loading pretrained weights")
    parser.add_argument("--backbone_path", type=str, default=None,
                       help="Path to specific backbone model (e.g., backbone_SimCLR_500.pth)")
    
    args = parser.parse_args()
    
    # Override config if arguments provided
    if args.noise_rate is not None:
        config.LABEL_NOISE_RATE = args.noise_rate
    
    main(args)
