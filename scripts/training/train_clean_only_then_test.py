"""
MEDAL-Lite å¹²å‡€æ•°æ®è®­ç»ƒ+æµ‹è¯•è„šæœ¬ (é‡æ„ç‰ˆ)
=========================================
ä½¿ç”¨å¹²å‡€æ ‡ç­¾ï¼ˆæ— å™ªå£°ï¼‰è®­ç»ƒåˆ†ç±»å™¨ï¼Œå¤ç”¨ä¸»æµç¨‹ä»£ç 
"""
import os
import sys
from pathlib import Path

project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

import json
import argparse
from datetime import datetime

import numpy as np
import torch
import random
import hashlib

from MoudleCode.utils.config import config
from MoudleCode.utils.helpers import set_seed, setup_logger
from MoudleCode.utils.logging_utils import (
    log_section_header, log_data_stats, log_input_paths, log_output_paths, log_final_summary
)
from MoudleCode.preprocessing.pcap_parser import load_dataset
from MoudleCode.feature_extraction.backbone import build_backbone

try:
    from scripts.utils.preprocess import check_preprocessed_exists, load_preprocessed, normalize_burstsize_inplace
    PREPROCESS_AVAILABLE = True
except Exception:
    PREPROCESS_AVAILABLE = False

from scripts.training.train import stage4_finetune_classifier
from scripts.testing.test import main as test_main


def _safe_makedirs(path: str) -> None:
    os.makedirs(path, exist_ok=True)


def _rng_fingerprint_short() -> str:
    h = hashlib.sha256()
    try:
        h.update(repr(random.getstate()).encode('utf-8'))
    except Exception:
        h.update(b'py_random_error')
    try:
        ns = np.random.get_state()
        h.update(str(ns[0]).encode('utf-8'))
        h.update(np.asarray(ns[1], dtype=np.uint32).tobytes())
        h.update(str(ns[2]).encode('utf-8'))
        h.update(str(ns[3]).encode('utf-8'))
        h.update(str(ns[4]).encode('utf-8'))
    except Exception:
        h.update(b'numpy_random_error')
    try:
        h.update(torch.get_rng_state().detach().cpu().numpy().tobytes())
    except Exception:
        h.update(b'torch_cpu_rng_error')
    try:
        if torch.cuda.is_available():
            for s in torch.cuda.get_rng_state_all():
                h.update(s.detach().cpu().numpy().tobytes())
        else:
            h.update(b'no_cuda')
    except Exception:
        h.update(b'torch_cuda_rng_error')
    return h.hexdigest()[:16]


def _seed_snapshot(args_seed: int) -> str:
    torch_seed = None
    try:
        torch_seed = int(torch.initial_seed())
    except Exception:
        torch_seed = None
    return (
        f"args.seed={int(args_seed)} | "
        f"config.SEED={int(getattr(config, 'SEED', -1))} | "
        f"torch.initial_seed={torch_seed}"
    )


def _load_train_dataset():
    """åŠ è½½è®­ç»ƒæ•°æ®é›†"""
    if PREPROCESS_AVAILABLE and check_preprocessed_exists('train'):
        X_train, y_train, _ = load_preprocessed('train')
        X_train = normalize_burstsize_inplace(X_train)
        return X_train, y_train

    X_train, y_train, _ = load_dataset(
        benign_dir=config.BENIGN_TRAIN,
        malicious_dir=config.MALICIOUS_TRAIN,
        sequence_length=config.SEQUENCE_LENGTH,
    )
    X_train = normalize_burstsize_inplace(X_train)
    return X_train, y_train


def main():
    parser = argparse.ArgumentParser(description="å¹²å‡€æ•°æ®è®­ç»ƒ+æµ‹è¯•")
    parser.add_argument('--use_ground_truth', action='store_true', help='ä½¿ç”¨çœŸå®æ ‡ç­¾')
    parser.add_argument('--retrain_backbone', action='store_true', help='é‡æ–°è®­ç»ƒéª¨å¹²ç½‘ç»œ')
    parser.add_argument('--backbone_path', type=str, default='', help='éª¨å¹²ç½‘ç»œè·¯å¾„')
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--run_tag', type=str, default='')
    args = parser.parse_args()

    rng_fp_before_seed = _rng_fingerprint_short()
    set_seed(args.seed)
    rng_fp_after_seed = _rng_fingerprint_short()
    config.create_dirs()
    logger = setup_logger(os.path.join(config.OUTPUT_ROOT, 'logs'), name='clean_train_test')

    logger.info(f"ğŸ”§ RNGæŒ‡çº¹(seedå‰): {rng_fp_before_seed}")
    logger.info(f"ğŸ”§ RNGæŒ‡çº¹(seedå): {rng_fp_after_seed} ({_seed_snapshot(args.seed)})")

    # é…ç½®ï¼šä½¿ç”¨æœ€ä¼˜å‚æ•°ï¼ˆå¹²å‡€æ•°æ®æ¨¡å¼ï¼‰
    config.USE_FOCAL_LOSS = True
    config.USE_BCE_LOSS = False
    config.USE_SOFT_F1_LOSS = False
    config.STAGE3_ONLINE_AUGMENTATION = False
    config.STAGE3_USE_ST_MIXUP = False
    config.FINETUNE_BACKBONE = True  # å¯ç”¨éª¨å¹²å¾®è°ƒ
    config.FINETUNE_VAL_SPLIT = 0.0
    config.FINETUNE_ES_ALLOW_TRAIN_METRIC = True
    config.STAGE3_MIXED_STREAM = False  # å¹²å‡€æ•°æ®æ¨¡å¼ä¸ä½¿ç”¨æ··åˆè®­ç»ƒ
    config.CLASSIFIER_INPUT_IS_FEATURES = False  # è¾“å…¥æ˜¯åŸå§‹åºåˆ—ï¼Œä¸æ˜¯ç‰¹å¾

    run_tag = args.run_tag.strip() or datetime.now().strftime('%Y%m%d_%H%M%S')
    run_dir = os.path.join(config.LABEL_CORRECTION_DIR, 'analysis', 'clean_only_runs', run_tag)
    _safe_makedirs(run_dir)
    
    # éš”ç¦»è¾“å‡ºç›®å½•
    original_classification_dir = config.CLASSIFICATION_DIR
    original_result_dir = config.RESULT_DIR
    config.CLASSIFICATION_DIR = os.path.join(run_dir, 'classification')
    config.RESULT_DIR = os.path.join(run_dir, 'result')
    
    for d in [config.CLASSIFICATION_DIR, config.RESULT_DIR]:
        _safe_makedirs(d)
        _safe_makedirs(os.path.join(d, 'models'))
        _safe_makedirs(os.path.join(d, 'figures'))
        _safe_makedirs(os.path.join(d, 'logs'))

    try:
        log_section_header(logger, "ğŸš€ å¹²å‡€æ•°æ®è®­ç»ƒ+æµ‹è¯•æ¨¡å¼")
        logger.info(f'è¿è¡Œç›®å½•: {run_dir}')
        logger.info(f'æ—¶é—´æˆ³: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}')
        logger.info('')
        logger.info('ğŸ“‹ è®­ç»ƒæ¨¡å¼è¯´æ˜:')
        logger.info('  - æ•°æ®æ¥æº: çº¯åŸå§‹è®­ç»ƒæ•°æ®ï¼ˆæ— å¢å¼ºï¼‰')
        logger.info('  - æ ‡ç­¾: çœŸå®æ ‡ç­¾ï¼ˆæ— å™ªå£°ï¼‰')
        logger.info('  - æ ‡ç­¾çŸ«æ­£: è·³è¿‡')
        logger.info('  - æ•°æ®å¢å¼º: è·³è¿‡')
        logger.info('  - éª¨å¹²å¾®è°ƒ: å¯ç”¨ï¼ˆä½¿ç”¨åŸå§‹åºåˆ—ï¼‰')
        logger.info('')
        
        # è¾“å‡ºé…ç½®
        config.log_stage_config(logger, "Stage 3")
        
        # åŠ è½½æ•°æ®
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(åŠ è½½è®­ç»ƒæ•°æ®å‰): {_rng_fingerprint_short()} ({_seed_snapshot(args.seed)})")
        X_train, y_train_true = _load_train_dataset()
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(åŠ è½½è®­ç»ƒæ•°æ®å): {_rng_fingerprint_short()} ({_seed_snapshot(args.seed)})")
        if X_train is None:
            raise RuntimeError('è®­ç»ƒæ•°æ®é›†åŠ è½½å¤±è´¥')

        y_corrected = y_train_true.astype(int)
        correction_weight = np.ones(len(y_corrected), dtype=np.float32)

        log_data_stats(logger, {
            "æ•°æ®å½¢çŠ¶": f"{X_train.shape} (æ ·æœ¬æ•°Ã—åºåˆ—é•¿åº¦Ã—ç‰¹å¾ç»´åº¦)",
            "è®­ç»ƒæ ·æœ¬æ€»æ•°": len(X_train),
            "æ­£å¸¸æ ·æœ¬": int((y_corrected == 0).sum()),
            "æ¶æ„æ ·æœ¬": int((y_corrected == 1).sum()),
            "æ•°æ®ç±»å‹": "åŸå§‹åºåˆ—ï¼ˆæ”¯æŒéª¨å¹²å¾®è°ƒï¼‰"
        }, "è®­ç»ƒæ•°æ®ç»Ÿè®¡")
        
        # åŠ è½½éª¨å¹²ç½‘ç»œ
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(æ„å»ºbackboneå‰): {_rng_fingerprint_short()} ({_seed_snapshot(args.seed)})")
        backbone = build_backbone(config, logger=logger)
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(æ„å»ºbackboneå): {_rng_fingerprint_short()} ({_seed_snapshot(args.seed)})")
        backbone_path = args.backbone_path if args.backbone_path else os.path.join(config.FEATURE_EXTRACTION_DIR, 'models', 'backbone_pretrained.pth')
        
        if os.path.exists(backbone_path) and not args.retrain_backbone:
            logger.info(f'âœ“ åŠ è½½éª¨å¹²ç½‘ç»œ: {backbone_path}')
            logger.info(f"ğŸ”§ RNGæŒ‡çº¹(åŠ è½½backboneæƒé‡å‰): {_rng_fingerprint_short()} ({_seed_snapshot(args.seed)})")
            try:
                state_dict = torch.load(backbone_path, map_location=config.DEVICE, weights_only=True)
            except TypeError:
                state_dict = torch.load(backbone_path, map_location=config.DEVICE)
            backbone.load_state_dict(state_dict, strict=False)
            logger.info(f"ğŸ”§ RNGæŒ‡çº¹(åŠ è½½backboneæƒé‡å): {_rng_fingerprint_short()} ({_seed_snapshot(args.seed)})")
            backbone.freeze()
        else:
            logger.warning('âš  ä½¿ç”¨éšæœºåˆå§‹åŒ–éª¨å¹²ç½‘ç»œ')
            backbone.freeze()
        
        # Stage 4: åˆ†ç±»å™¨è®­ç»ƒ
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(Stage4è°ƒç”¨å‰): {_rng_fingerprint_short()} ({_seed_snapshot(args.seed)})")
        stage4_finetune_classifier(
            backbone, X_train, y_corrected, correction_weight,
            config, logger, n_original=len(X_train), backbone_path=backbone_path
        )
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(Stage4è¿”å›å): {_rng_fingerprint_short()} ({_seed_snapshot(args.seed)})")

        # æµ‹è¯•
        log_section_header(logger, "ğŸ§ª æµ‹è¯•è¯„ä¼°")
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(æµ‹è¯•å‰): {_rng_fingerprint_short()} ({_seed_snapshot(args.seed)})")
        classifier_best = os.path.join(config.CLASSIFICATION_DIR, 'models', 'classifier_best_f1.pth')
        
        meta_path = os.path.join(config.CLASSIFICATION_DIR, 'models', 'model_metadata.json')
        backbone_path_for_test = backbone_path
        if os.path.exists(meta_path):
            with open(meta_path, 'r') as f:
                meta = json.load(f)
            bp = meta.get('backbone_path', '')
            if bp:
                if os.path.exists(bp):
                    backbone_path_for_test = bp
                    logger.info(f"âœ“ ä½¿ç”¨å…ƒæ•°æ®ä¸­è®°å½•çš„éª¨å¹²ç½‘ç»œ: {bp}")
                else:
                    logger.warning(f"âš  å…ƒæ•°æ®ä¸­è®°å½•çš„éª¨å¹²ç½‘ç»œä¸å­˜åœ¨: {bp}")
                    logger.warning(f"  å›é€€åˆ°è®­ç»ƒæ—¶ä½¿ç”¨çš„éª¨å¹²ç½‘ç»œ: {backbone_path}")

        test_args = argparse.Namespace(backbone_path=backbone_path_for_test)
        test_main(test_args)
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(æµ‹è¯•å): {_rng_fingerprint_short()} ({_seed_snapshot(args.seed)})")

        log_final_summary(logger, "å®Œæˆ", {}, {
            "è¿è¡Œç›®å½•": run_dir,
            "åˆ†ç±»å™¨": classifier_best,
            "æµ‹è¯•ç»“æœ": config.RESULT_DIR
        })
        
    finally:
        config.CLASSIFICATION_DIR = original_classification_dir
        config.RESULT_DIR = original_result_dir


if __name__ == '__main__':
    main()
