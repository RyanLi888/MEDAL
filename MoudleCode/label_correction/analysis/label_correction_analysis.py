"""
Label Correction Analysis Script
Only runs label correction part and generates detailed analysis documents and feature distribution plots.

This script:
1. Loads dataset and injects label noise
2. Extracts features using pre-trained backbone
3. Runs Hybrid Court label correction (CL + MADE + KNN)
4. Generates detailed per-sample analysis document
5. Creates feature distribution plots for clean, noisy, and corrected data
"""
import sys
import os
from pathlib import Path
# Ensure project root is on sys.path so `MoudleCode.*` imports work when invoked directly
PROJECT_ROOT = Path(__file__).resolve().parents[3]
sys.path.append(str(PROJECT_ROOT))

import torch
import numpy as np
import pandas as pd
import argparse
from datetime import datetime
import json
import matplotlib.pyplot as plt
import matplotlib as mpl
from matplotlib import font_manager
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA

from MoudleCode.utils.config import config
from MoudleCode.utils.helpers import (
    set_seed, setup_logger, inject_label_noise
)
from MoudleCode.preprocessing.pcap_parser import load_dataset
from MoudleCode.feature_extraction.backbone import MicroBiMambaBackbone
from MoudleCode.label_correction.hybrid_court import HybridCourt

import logging

# å°è¯•å¯¼å…¥Excelå†™å…¥åº“
try:
    import openpyxl
    from openpyxl.styles import Font, PatternFill, Alignment
    EXCEL_AVAILABLE = True
except ImportError:
    EXCEL_AVAILABLE = False
    print("è­¦å‘Š: openpyxlæœªå®‰è£…ï¼Œå°†åªç”ŸæˆCSVæ–‡ä»¶ã€‚å®‰è£…å‘½ä»¤: pip install openpyxl")

# ç®€åŒ–çš„é¢„å¤„ç†æ–‡ä»¶æ£€æµ‹å’ŒåŠ è½½
def check_preprocessed_exists(split='train'):
    """æ£€æŸ¥é¢„å¤„ç†æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    preprocessed_dir = PROJECT_ROOT / 'output' / 'preprocessed'
    if not preprocessed_dir.exists():
        return False
    
    X_file = preprocessed_dir / f'{split}_X.npy'
    y_file = preprocessed_dir / f'{split}_y.npy'
    files_file = preprocessed_dir / f'{split}_files.npy'
    
    return X_file.exists() and y_file.exists() and files_file.exists()

def load_preprocessed(split='train'):
    """åŠ è½½é¢„å¤„ç†æ–‡ä»¶"""
    preprocessed_dir = PROJECT_ROOT / 'output' / 'preprocessed'
    
    X = np.load(preprocessed_dir / f'{split}_X.npy')
    y = np.load(preprocessed_dir / f'{split}_y.npy')
    files = np.load(preprocessed_dir / f'{split}_files.npy', allow_pickle=True)
    
    return X, y, files

logger = None  # Will be initialized in main()


def _configure_matplotlib_chinese_fonts():
    try:
        preferred = [
            'Noto Sans CJK SC',
            'Noto Sans CJK TC',
            'Noto Sans CJK JP',
            'Noto Sans SC',
            'Noto Sans TC',
            'Source Han Sans SC',
            'Source Han Sans CN',
            'WenQuanYi Zen Hei',
            'WenQuanYi Micro Hei',
            'Microsoft YaHei',
            'SimHei'
        ]
        available = {f.name for f in font_manager.fontManager.ttflist}
        chosen = None
        for name in preferred:
            if name in available:
                chosen = name
                break

        if chosen is not None:
            current = mpl.rcParams.get('font.sans-serif', [])
            if not isinstance(current, list):
                current = [current]
            mpl.rcParams['font.family'] = 'sans-serif'
            mpl.rcParams['font.sans-serif'] = [chosen] + [f for f in current if f != chosen]

        mpl.rcParams['axes.unicode_minus'] = False
    except Exception:
        pass


_configure_matplotlib_chinese_fonts()


def extract_features_with_backbone(backbone, X_data, config, logger):
    """
    Extract features using pre-trained backbone
    
    Args:
        backbone: Pre-trained MicroBiMambaBackbone
        X_data: (N, L, 5) input sequences
        config: configuration object
        logger: logger
        
    Returns:
        features: (N, d) extracted features
    """
    logger.info("Extracting features using backbone...")
    
    backbone.freeze()
    backbone.eval()
    backbone.to(config.DEVICE)
    
    features_list = []
    batch_size = 64
    X_tensor = torch.FloatTensor(X_data).to(config.DEVICE)
    
    total_batches = (len(X_tensor) + batch_size - 1) // batch_size
    
    with torch.no_grad():
        for i in range(0, len(X_tensor), batch_size):
            batch_idx = i // batch_size + 1
            X_batch = X_tensor[i:i+batch_size]
            z_batch = backbone(X_batch, return_sequence=False)
            features_list.append(z_batch.cpu().numpy())
            
            if batch_idx % 10 == 0 or batch_idx == total_batches:
                progress = batch_idx / total_batches * 100
                logger.info(f"  Feature extraction progress: {batch_idx}/{total_batches} batches ({progress:.1f}%)")
    
    features = np.concatenate(features_list, axis=0)
    logger.info(f"âœ“ Feature extraction complete: {features.shape}")
    
    return features


def run_hybrid_court_with_detailed_tracking(features, noisy_labels, config, logger, y_true=None):
    """
    Run Hybrid Court v9 ä¸‰é˜¶æ®µæ ‡ç­¾çŸ«æ­£ with detailed tracking
    
    Args:
        features: ç‰¹å¾çŸ©é˜µ
        noisy_labels: å™ªå£°æ ‡ç­¾
        config: é…ç½®å¯¹è±¡
        logger: æ—¥å¿—å™¨
        y_true: çœŸå®æ ‡ç­¾ (å¯é€‰ï¼Œç”¨äºè¯¦ç»†ç»Ÿè®¡)
    
    Returns:
        results: dict containing all intermediate and final results
    """
    logger.info("")
    logger.info("="*70)
    logger.info("Running Hybrid Court v9 ä¸‰é˜¶æ®µæ ‡ç­¾çŸ«æ­£")
    logger.info("="*70)
    
    hybrid_court = HybridCourt(config)
    n_samples = len(noisy_labels)
    
    # è°ƒç”¨ä¸‰é˜¶æ®µæ ‡ç­¾çŸ«æ­£
    clean_labels, action_mask, confidence, correction_weight, density_scores, neighbor_consistency, pred_probs = hybrid_court.correct_labels(
        features, noisy_labels, device=config.DEVICE, y_true=y_true
    )
    
    # ä»å„å­æ¨¡å—ä¸­è¯»å–ç¼“å­˜çš„ä¸­é—´ç»“æœ
    suspected_noise = getattr(hybrid_court.cl, "last_suspected_noise", None)
    pred_labels = getattr(hybrid_court.cl, "last_pred_labels", None)
    is_dense = getattr(hybrid_court.made, "last_is_dense", None)
    neighbor_labels = getattr(hybrid_court.knn, "last_neighbor_labels", None)
    tier_info = getattr(hybrid_court, "last_tier_info", [''] * n_samples)
    
    # è·å–è¿­ä»£CLå’Œé”šç‚¹KNNç»“æœ
    iter_pred_probs = getattr(hybrid_court, "iter_pred_probs_all", None)
    anchor_votes = getattr(hybrid_court, "anchor_votes_all", None)
    anchor_consistency = getattr(hybrid_court, "anchor_consistency_all", None)
    
    # ç”Ÿæˆå†³ç­–åŸå› 
    decision_reasons = []
    for i in range(n_samples):
        tier = tier_info[i] if i < len(tier_info) else ''
        
        # è·å–è¿­ä»£CLå’Œé”šç‚¹KNNä¿¡æ¯
        iter_cl_current = float(iter_pred_probs[i, int(noisy_labels[i])]) if iter_pred_probs is not None else None
        iter_cl_target = float(iter_pred_probs[i, 1 - int(noisy_labels[i])]) if iter_pred_probs is not None else None
        anchor_vote = int(anchor_votes[i]) if anchor_votes is not None else None
        anchor_cons = float(anchor_consistency[i]) if anchor_consistency is not None else None
        
        reason = _generate_decision_reason(
            tier, 
            int(noisy_labels[i]), 
            int(clean_labels[i]),
            int(action_mask[i]),
            bool(suspected_noise[i]) if suspected_noise is not None else False,
            bool(is_dense[i]) if is_dense is not None else True,
            int(neighbor_labels[i]) if neighbor_labels is not None else -1,
            float(neighbor_consistency[i]),
            float(pred_probs[i, 0]),
            float(pred_probs[i, 1]),
            float(density_scores[i]),
            iter_cl_current,
            iter_cl_target,
            anchor_vote,
            anchor_cons
        )
        decision_reasons.append(reason)
    
    # Compile results
    results = {
        'features': features,
        'noisy_labels': noisy_labels,
        'clean_labels': clean_labels,
        'action_mask': action_mask,
        'confidence': confidence,
        'correction_weight': correction_weight,
        # CL results
        'cl_suspected_noise': suspected_noise,
        'cl_pred_labels': pred_labels,
        'cl_pred_probs': pred_probs,
        # MADE results
        'made_is_dense': is_dense,
        'made_density_scores': density_scores,
        # KNN results
        'knn_neighbor_labels': neighbor_labels,
        'knn_neighbor_consistency': neighbor_consistency,
        # Iterative CL results
        'iter_cl_probs': iter_pred_probs,
        # Anchor KNN results
        'anchor_votes': anchor_votes,
        'anchor_consistency': anchor_consistency,
        # Decision tracking
        'decision_reasons': decision_reasons,
        'tier_info': tier_info
    }
    
    # è¯¦ç»†çš„Tierç»Ÿè®¡
    _log_tier_statistics(results, y_true, logger)
    
    return results


def _generate_decision_reason(tier, noisy_label, clean_label, action, is_suspected, is_dense, 
                               knn_label, knn_cons, cl_prob_0, cl_prob_1, density,
                               iter_cl_current=None, iter_cl_target=None, 
                               anchor_vote=None, anchor_cons=None):
    """ç”Ÿæˆè¯¦ç»†çš„å†³ç­–åŸå› è¯´æ˜"""
    current_label_name = 'æ­£å¸¸' if noisy_label == 0 else 'æ¶æ„'
    target_label_name = 'æ¶æ„' if noisy_label == 0 else 'æ­£å¸¸'
    new_label_name = 'æ­£å¸¸' if clean_label == 0 else 'æ¶æ„'
    
    # åŸå§‹CLç½®ä¿¡åº¦
    orig_cl_current = cl_prob_0 if noisy_label == 0 else cl_prob_1
    orig_cl_target = cl_prob_1 if noisy_label == 0 else cl_prob_0
    
    # KNNæŠ•ç¥¨ç»“æœ
    knn_vote_name = 'æ­£å¸¸' if knn_label == 0 else 'æ¶æ„'
    knn_support = 'æ”¯æŒ' if knn_label == noisy_label else 'åå¯¹'
    
    # Tier 1: Core - å®šæµ·ç¥é’ˆ
    if 'Tier 1: Core' in tier:
        reasons = []
        if orig_cl_current >= 0.70:
            reasons.append(f"åŸCLé«˜({orig_cl_current:.3f}â‰¥0.70)")
        if knn_cons >= 0.70 and knn_label == noisy_label:
            reasons.append(f"KNNå¼ºæ”¯æŒ({knn_cons:.3f}â‰¥0.70)")
        if not reasons:
            reasons.append(f"MADE+KNNè¡¥å……(KNN={knn_cons:.3f})")
        return f"[Phase1-Core] {' æˆ– '.join(reasons)} â†’ æ ¸å¿ƒæ ·æœ¬(w=1.0)"
    
    # Tier 2: Flip - æ™ºèƒ½ç¿»è½¬
    elif 'Tier 2: Flip' in tier:
        cl_diff = orig_cl_target - orig_cl_current
        if noisy_label == 0:  # æ­£å¸¸â†’æ¶æ„
            return (f"[Phase2-Flip] CLå·®å€¼={cl_diff:.3f}â‰¥0.15 + æ­£å¸¸æ ·æœ¬ + "
                   f"åŸCL={orig_cl_current:.3f}â‰¤0.95 + MADE={density:.1f}â‰¤35 â†’ "
                   f"ç¿»è½¬{current_label_name}â†’{new_label_name}(w=1.0)")
        else:  # æ¶æ„â†’æ­£å¸¸
            return (f"[Phase2-Flip] CLå·®å€¼={cl_diff:.3f}â‰¥0.15 + æ¶æ„æ ·æœ¬ + "
                   f"MADE={density:.1f}â‰¥15 + åŸCL={orig_cl_current:.3f}â‰¥0.5 â†’ "
                   f"ç¿»è½¬{current_label_name}â†’{new_label_name}(w=1.0)")
    
    # Tier 3: Keep - é«˜è´¨é‡ä¿æŒ
    elif 'Tier 3' in tier or 'Keep' in tier:
        if 'High' in tier or '3a' in tier:
            return (f"[Phase2-Keep-High] åŸCL={orig_cl_current:.3f}â‰¥0.55 + KNNæ”¯æŒ({knn_cons:.3f}â‰¥0.55) + "
                   f"MADEæ­£å¸¸({density:.1f}) â†’ ä¿æŒ{current_label_name}(w=0.8-1.0)")
        else:
            return (f"[Phase2-Keep-Low] æ¶æ„æ ·æœ¬ + MADEå¼‚å¸¸é«˜({density:.1f}) â†’ "
                   f"å¯èƒ½è¯¯æ ‡æ­£å¸¸,ä¿æŒ{current_label_name}(w=0.4)")
    
    # Tier 4: Reweight - ä¸ç¡®å®šæ ·æœ¬
    elif 'Tier 4' in tier or 'Reweight' in tier:
        reasons = []
        if orig_cl_current < 0.55:
            reasons.append(f"åŸCLä½({orig_cl_current:.3f}<0.55)")
        if knn_cons < 0.55:
            reasons.append(f"KNNå¼±({knn_cons:.3f}<0.55)")
        if not is_suspected and knn_label == noisy_label:
            reasons.append("CLä¸æ€€ç–‘+KNNæ”¯æŒ")
        
        if not reasons:
            reasons.append("æœªè¾¾Core/Flip/Keepæ ‡å‡†")
        
        return f"[Phase2-Reweight] {' + '.join(reasons)} â†’ é™æƒä¿æŒ{current_label_name}(w=0.5)"
    
    # Tier 5: Rescued - Phase 3æ‹¯æ•‘
    elif 'Tier 5' in tier or 'Rescued' in tier:
        if anchor_cons is not None and anchor_vote is not None:
            anchor_vote_name = 'æ­£å¸¸' if anchor_vote == 0 else 'æ¶æ„'
            if 'Keep' in tier or '5a' in tier:
                return (f"[Phase3-Rescued-Keep] iter_T={iter_cl_target:.3f}â‰¥0.60 + "
                       f"anchorå¼ºæ”¯æŒå½“å‰({anchor_vote_name},{anchor_cons:.3f}â‰¥0.60) â†’ "
                       f"æ‹¯æ•‘ä¿æŒ{current_label_name}(w=0.85)")
            else:
                return (f"[Phase3-Rescued-Flip] iter_T={iter_cl_target:.3f}â‰¥0.60 + "
                       f"anchorå¼ºåå¯¹å½“å‰({anchor_vote_name},{anchor_cons:.3f}â‰¥0.60) â†’ "
                       f"æ‹¯æ•‘ç¿»è½¬{current_label_name}â†’{new_label_name}(w=0.75)")
        else:
            return f"[Phase3-Rescued] é”šç‚¹æ‹¯æ•‘ â†’ {new_label_name}"
    
    # Dropped - ä¸¢å¼ƒ
    elif 'Dropped' in tier:
        drop_reasons = []
        if iter_cl_current is not None:
            if iter_cl_current < 0.48:
                drop_reasons.append(f"iter_CLæä½({iter_cl_current:.3f}<0.48)")
            elif iter_cl_current < 0.55 and anchor_cons is not None and anchor_cons < 0.55:
                drop_reasons.append(f"iter_CLä½({iter_cl_current:.3f}<0.55) + anchorä½({anchor_cons:.3f}<0.55)")
        
        if not drop_reasons:
            drop_reasons.append("è´¨é‡è¿‡ä½")
        
        return f"[Dropped] {' æˆ– '.join(drop_reasons)} â†’ ä¸¢å¼ƒ(w=0.0)"
    
    # æœªçŸ¥Tier
    else:
        action_names = ['ä¿æŒ', 'ç¿»è½¬', 'ä¸¢å¼ƒ', 'é‡åŠ æƒ']
        action_name = action_names[action] if action < len(action_names) else 'æœªçŸ¥'
        return f"[æœªåˆ†ç±»] tier={tier}, action={action_name}, åŸCL={orig_cl_current:.3f}, KNN={knn_cons:.3f}"


def _log_tier_statistics(results, y_true, logger):
    """è¾“å‡ºè¯¦ç»†çš„Tierç»Ÿè®¡"""
    tier_info = results.get('tier_info', [])
    clean_labels = results['clean_labels']
    correction_weight = results['correction_weight']
    n_samples = len(clean_labels)
    
    # ç»Ÿè®¡å„Tier
    tier_counts = {}
    tier_correct = {}
    tier_weights = {}
    
    for i, tier in enumerate(tier_info):
        if tier not in tier_counts:
            tier_counts[tier] = 0
            tier_correct[tier] = 0
            tier_weights[tier] = correction_weight[i]
        tier_counts[tier] += 1
        if y_true is not None and clean_labels[i] == y_true[i]:
            tier_correct[tier] += 1
    
    logger.info("")
    logger.info("="*90)
    logger.info("ğŸ“Š ä¸‰é˜¶æ®µæ ‡ç­¾çŸ«æ­£ - è¯¦ç»†Tierç»Ÿè®¡")
    logger.info("="*90)
    
    tier_order = [
        'Tier 1: Core',
        'Tier 2: Flip',
        'Tier 3a: Keep-High',
        'Tier 3b: Keep-Low',
        'Tier 4a: Reweight-High',
        'Tier 4b: Reweight-Low',
        'Tier 5a: Rescued-Keep',
        'Tier 5b: Rescued-Flip',
        'Dropped (Low CL)',
        'Dropped'
    ]
    
    role_map = {
        'Tier 1: Core': '[å®šæµ·ç¥é’ˆ] ç»å¯¹çº¯å‡€çš„åŸºçŸ³æ•°æ®',
        'Tier 2: Flip': '[å¼ºåŠ›çº é”™] æˆåŠŸæŒ½å›çš„æ ·æœ¬',
        'Tier 3a: Keep-High': '[éš¾ä¾‹ç²¾å] ä¼˜è´¨ä¿æŒæ ·æœ¬',
        'Tier 3b: Keep-Low': '[é£é™©éš”ç¦»] è¾¹ç¼˜æ ·æœ¬ä½æƒé‡',
        'Tier 4a: Reweight-High': '[æ³›åŒ–ä¸»åŠ›] æ¸…æ´—åçš„é•¿å°¾æ•°æ®',
        'Tier 4b: Reweight-Low': '[å™ªå£°ç›‘ç‹±] å…³æŠ¼æ®‹ç•™å™ªå£°',
        'Tier 5a: Rescued-Keep': '[äºŒæ¬¡æ‹¯æ•‘] é”šç‚¹KNNæ‹¯æ•‘Keep',
        'Tier 5b: Rescued-Flip': '[äºŒæ¬¡æ‹¯æ•‘] é”šç‚¹KNNæ‹¯æ•‘Flip',
        'Dropped (Low CL)': '[å·²ä¸¢å¼ƒ] CLä¿¡å¿ƒè¿‡ä½',
        'Dropped': '[å·²ä¸¢å¼ƒ]'
    }
    
    logger.info(f"{'Tier':<30s} | {'æƒé‡':>6s} | {'æ ·æœ¬æ•°':>8s} | {'çº¯åº¦':>8s} | {'å«å™ªæ•°':>8s} | è§’è‰²å®šä½")
    logger.info("-" * 90)
    
    total_weighted_correct = 0
    total_weighted_count = 0
    
    for tier in tier_order:
        if tier in tier_counts:
            count = tier_counts[tier]
            correct = tier_correct.get(tier, 0)
            weight = tier_weights.get(tier, 0)
            purity = 100 * correct / count if count > 0 else 0
            noise_count = count - correct
            role = role_map.get(tier, '')
            
            logger.info(f"{tier:<30s} | {weight:>6.2f} | {count:>8d} | {purity:>7.1f}% | {noise_count:>8d} | {role}")
            
            if 'Dropped' not in tier:
                total_weighted_correct += correct * weight
                total_weighted_count += count * weight
    
    logger.info("-" * 90)
    
    if total_weighted_count > 0:
        weighted_purity = 100 * total_weighted_correct / total_weighted_count
        logger.info(f"ğŸ“ˆ åŠ æƒçº¯åº¦: {weighted_purity:.2f}%")
    
    logger.info("="*90)


def generate_sample_analysis_document(results, y_true, noise_mask, save_path, logger):
    """
    Generate detailed per-sample analysis document in strategy-grouped format
    
    Args:
        results: dict from run_hybrid_court_with_detailed_tracking
        y_true: (N,) ground truth labels
        noise_mask: (N,) boolean array indicating which labels were flipped to create noise
        save_path: path to save the document
        logger: logger
    """
    logger.info("")
    logger.info("="*70)
    logger.info("ç”Ÿæˆæ ·æœ¬çº§åˆ†ææ–‡æ¡£ï¼ˆç­–ç•¥åˆ†ç»„æ ¼å¼ï¼‰")
    logger.info("="*70)
    
    def _classify_error_type(is_noise, action, true_label, noisy_label, corrected_label):
        """åˆ†ç±»é”™è¯¯ç±»å‹"""
        if corrected_label == true_label:
            return 'æ­£ç¡®'
        
        if is_noise:
            # å™ªå£°æ ·æœ¬
            if action == 1:  # ç¿»è½¬
                return 'ç¿»è½¬é”™è¯¯-å™ªå£°æœªçŸ«æ­£'
            else:
                return 'ä¿æŒé”™è¯¯-å™ªå£°æœªæ£€æµ‹'
        else:
            # å¹²å‡€æ ·æœ¬
            if action == 1:  # ç¿»è½¬
                return 'ç¿»è½¬é”™è¯¯-è¯¯æ€å¹²å‡€æ ·æœ¬'
            else:
                return 'ä¿æŒé”™è¯¯-ä¸åº”å‘ç”Ÿ'
    
    n_samples = len(y_true)
    tier_info = results.get('tier_info', [''] * n_samples)
    
    # è·å–è¿­ä»£CLå’Œé”šç‚¹KNNçš„ç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    iter_cl_probs = results.get('iter_cl_probs', None)
    anchor_votes = results.get('anchor_votes', None)
    anchor_consistency = results.get('anchor_consistency', None)
    
    # Prepare data for DataFrame
    data = []
    
    for i in range(n_samples):
        # Basic info
        sample_id = i
        true_label = int(y_true[i])
        true_label_name = "æ­£å¸¸" if true_label == 0 else "æ¶æ„"
        is_noise = bool(noise_mask[i])
        noisy_label = int(results['noisy_labels'][i])
        noisy_label_name = "æ­£å¸¸" if noisy_label == 0 else "æ¶æ„"
        
        # CL (Confident Learning) results
        cl_suspected_noise = bool(results['cl_suspected_noise'][i])
        cl_pred_label = int(results['cl_pred_labels'][i])
        cl_pred_label_name = "æ­£å¸¸" if cl_pred_label == 0 else "æ¶æ„"
        cl_pred_prob_benign = float(results['cl_pred_probs'][i, 0])
        cl_pred_prob_malicious = float(results['cl_pred_probs'][i, 1])
        
        # MADE (Density Estimation) results
        made_is_dense = bool(results['made_is_dense'][i])
        made_density_score = float(results['made_density_scores'][i])
        
        # KNN (Semantic Voting) results
        knn_neighbor_label = int(results['knn_neighbor_labels'][i])
        knn_neighbor_label_name = "æ­£å¸¸" if knn_neighbor_label == 0 else "æ¶æ„"
        knn_consistency = float(results['knn_neighbor_consistency'][i])
        
        # Iterative CL results (if available)
        if iter_cl_probs is not None:
            iter_cl_current = float(iter_cl_probs[i, noisy_label])
            iter_cl_target = float(iter_cl_probs[i, 1 - noisy_label])
        else:
            iter_cl_current = None
            iter_cl_target = None
        
        # Anchor KNN results (if available)
        if anchor_votes is not None:
            anchor_vote = int(anchor_votes[i])
            anchor_vote_name = "æ­£å¸¸" if anchor_vote == 0 else "æ¶æ„"
            anchor_cons = float(anchor_consistency[i])
        else:
            anchor_vote = None
            anchor_vote_name = None
            anchor_cons = None
        
        # Final correction decision
        action = int(results['action_mask'][i])
        action_names = ['ä¿æŒ', 'ç¿»è½¬', 'ä¸¢å¼ƒ', 'é‡åŠ æƒ']
        action_name = action_names[action]
        corrected_label = int(results['clean_labels'][i])
        corrected_label_name = "æ­£å¸¸" if corrected_label == 0 else "æ¶æ„"
        confidence = float(results['confidence'][i])
        correction_weight = float(results['correction_weight'][i])
        decision_reason = results['decision_reasons'][i]
        tier = tier_info[i] if i < len(tier_info) else ''
        
        # Correction correctness (compared to ground truth)
        if action == 2:  # Dropped
            correction_correct = "ä¸é€‚ç”¨(å·²ä¸¢å¼ƒ)"
        else:
            correction_correct = "æ­£ç¡®" if corrected_label == true_label else "é”™è¯¯"
        
        # Compile row with ALL detailed metrics
        row = {
            # åŸºæœ¬ä¿¡æ¯
            'æ ·æœ¬ID': sample_id,
            'çœŸå®æ ‡ç­¾': true_label_name,
            'çœŸå®æ ‡ç­¾å€¼': true_label,
            'æ˜¯å¦å™ªå£°': 'æ˜¯' if is_noise else 'å¦',
            'å™ªå£°æ ‡ç­¾': noisy_label_name,
            'å™ªå£°æ ‡ç­¾å€¼': noisy_label,
            
            # CL (Confident Learning) è¯¦ç»†ç»“æœ
            'CLç–‘ä¼¼å™ªå£°': 'æ˜¯' if cl_suspected_noise else 'å¦',
            'CLé¢„æµ‹æ ‡ç­¾': cl_pred_label_name,
            'CLé¢„æµ‹æ ‡ç­¾å€¼': cl_pred_label,
            'CLæ­£å¸¸æ¦‚ç‡': cl_pred_prob_benign,
            'CLæ¶æ„æ¦‚ç‡': cl_pred_prob_malicious,
            'CLå½“å‰æ ‡ç­¾ç½®ä¿¡åº¦': cl_pred_prob_benign if noisy_label == 0 else cl_pred_prob_malicious,
            'CLç›®æ ‡æ ‡ç­¾ç½®ä¿¡åº¦': cl_pred_prob_malicious if noisy_label == 0 else cl_pred_prob_benign,
            
            # MADE (Density Estimation) è¯¦ç»†ç»“æœ
            'MADEé«˜å¯†åº¦': 'æ˜¯' if made_is_dense else 'å¦',
            'MADEå¯†åº¦åˆ†æ•°': made_density_score,
            'MADEå¯†åº¦ç­‰çº§': 'High' if made_density_score > 60 else ('Medium' if made_density_score > 0 else 'Low'),
            
            # KNN (Semantic Voting) è¯¦ç»†ç»“æœ
            'KNNé‚»å±…æ ‡ç­¾': knn_neighbor_label_name,
            'KNNé‚»å±…æ ‡ç­¾å€¼': knn_neighbor_label,
            'KNNä¸€è‡´æ€§': knn_consistency,
            'KNNæ˜¯å¦æ”¯æŒå½“å‰æ ‡ç­¾': 'æ˜¯' if knn_neighbor_label == noisy_label else 'å¦',
            'KNNä¸€è‡´æ€§ç­‰çº§': 'High' if knn_consistency >= 0.7 else ('Medium' if knn_consistency >= 0.5 else 'Low'),
        }
        
        # Add iterative CL columns if available
        if iter_cl_current is not None:
            row['iter_CLå½“å‰æ ‡ç­¾ç½®ä¿¡åº¦'] = iter_cl_current
            row['iter_CLç›®æ ‡æ ‡ç­¾ç½®ä¿¡åº¦'] = iter_cl_target
            row['iter_CLç½®ä¿¡åº¦å·®å€¼'] = abs(iter_cl_current - iter_cl_target)
        
        # Add anchor KNN columns if available
        if anchor_vote is not None:
            row['anchor_KNNæŠ•ç¥¨'] = anchor_vote_name
            row['anchor_KNNæŠ•ç¥¨å€¼'] = anchor_vote
            row['anchor_KNNä¸€è‡´æ€§'] = anchor_cons
            row['anchor_KNNæ˜¯å¦æ”¯æŒå½“å‰'] = 'æ˜¯' if anchor_vote == noisy_label else 'å¦'
        
        # æœ€ç»ˆå†³ç­–è¯¦ç»†ä¿¡æ¯
        row.update({
            'çŸ«æ­£åŠ¨ä½œ': action_name,
            'çŸ«æ­£åŠ¨ä½œå€¼': action,
            'çŸ«æ­£åæ ‡ç­¾': corrected_label_name,
            'çŸ«æ­£åæ ‡ç­¾å€¼': corrected_label,
            'ç³»ç»Ÿç½®ä¿¡åº¦': confidence,
            'æ ·æœ¬æƒå€¼': correction_weight,
            'Tieråˆ†çº§': tier,
            'Tieré˜¶æ®µ': 'Phase 1' if 'Tier 1' in tier else ('Phase 2' if any(t in tier for t in ['Tier 2', 'Tier 3', 'Tier 4']) else ('Phase 3' if 'Tier 5' in tier else 'Dropped')),
            'å†³ç­–ç†ç”±': decision_reason,
            
            # çŸ«æ­£ç»“æœè¯„ä¼°
            'çŸ«æ­£æ˜¯å¦æ­£ç¡®': correction_correct,
            'æ˜¯å¦ç¿»è½¬': 'æ˜¯' if action == 1 else 'å¦',
            'æ˜¯å¦ä¸¢å¼ƒ': 'æ˜¯' if action == 2 else 'å¦',
            'æ ‡ç­¾æ˜¯å¦æ”¹å˜': 'æ˜¯' if corrected_label != noisy_label else 'å¦',
            
            # é”™è¯¯ç±»å‹åˆ†æ
            'é”™è¯¯ç±»å‹': _classify_error_type(
                is_noise, action, true_label, noisy_label, corrected_label
            ) if action != 2 else 'å·²ä¸¢å¼ƒ'
        })
        
        data.append(row)
    
    # Create DataFrame
    df = pd.DataFrame(data)
    
    # Save to CSV (åŸºç¡€ç‰ˆæœ¬ï¼Œæ‰€æœ‰æ•°æ®åœ¨ä¸€ä¸ªæ–‡ä»¶)
    csv_path = save_path.replace('.txt', '.csv').replace('.log', '.csv')
    df.to_csv(csv_path, index=False, encoding='utf-8-sig')
    logger.info(f"âœ“ CSVæ–‡ä»¶å·²ä¿å­˜: {csv_path}")
    
    # Save to Excel with multiple sheets (å¦‚æœopenpyxlå¯ç”¨)
    if EXCEL_AVAILABLE:
        excel_path = csv_path.replace('.csv', '.xlsx')
        try:
            from openpyxl.styles import PatternFill, Font
            
            with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
                # é‡æ–°æ’åˆ—åˆ—é¡ºåºï¼ŒæŠŠå…³é”®ä¿¡æ¯æ”¾å‰é¢
                key_columns = [
                    'æ ·æœ¬ID', 'çŸ«æ­£æ˜¯å¦æ­£ç¡®', 'é”™è¯¯ç±»å‹', 'Tieråˆ†çº§', 'Tieré˜¶æ®µ',
                    'æ˜¯å¦å™ªå£°', 'çœŸå®æ ‡ç­¾', 'çœŸå®æ ‡ç­¾å€¼', 'å™ªå£°æ ‡ç­¾', 'å™ªå£°æ ‡ç­¾å€¼',
                    'çŸ«æ­£åæ ‡ç­¾', 'çŸ«æ­£åæ ‡ç­¾å€¼', 'çŸ«æ­£åŠ¨ä½œ', 'å†³ç­–ç†ç”±',
                    'ç³»ç»Ÿç½®ä¿¡åº¦', 'æ ·æœ¬æƒå€¼', 'æ˜¯å¦ç¿»è½¬', 'æ˜¯å¦ä¸¢å¼ƒ', 'æ ‡ç­¾æ˜¯å¦æ”¹å˜'
                ]
                # åªé€‰æ‹©å­˜åœ¨çš„åˆ—
                existing_key_columns = [col for col in key_columns if col in df.columns]
                other_columns = [col for col in df.columns if col not in existing_key_columns]
                df_reordered = df[existing_key_columns + other_columns]
                
                # Sheet 1: æ€»è§ˆç»Ÿè®¡
                tier_summary = []
                for tier in sorted(df['Tieråˆ†çº§'].unique()):
                    tier_df = df[df['Tieråˆ†çº§'] == tier]
                    valid_df = tier_df[tier_df['çŸ«æ­£æ˜¯å¦æ­£ç¡®'] != 'ä¸é€‚ç”¨(å·²ä¸¢å¼ƒ)']
                    correct_count = len(tier_df[tier_df['çŸ«æ­£æ˜¯å¦æ­£ç¡®'] == 'æ­£ç¡®'])
                    error_count = len(tier_df[tier_df['çŸ«æ­£æ˜¯å¦æ­£ç¡®'] == 'é”™è¯¯'])
                    dropped_count = len(tier_df[tier_df['çŸ«æ­£æ˜¯å¦æ­£ç¡®'] == 'ä¸é€‚ç”¨(å·²ä¸¢å¼ƒ)'])
                    
                    tier_summary.append({
                        'Tier': tier,
                        'æ€»æ ·æœ¬æ•°': len(tier_df),
                        'âœ“æ­£ç¡®å¤„ç†': correct_count,
                        'âœ—é”™è¯¯å¤„ç†': error_count,
                        'âŠ—å·²ä¸¢å¼ƒ': dropped_count,
                        'å‡†ç¡®ç‡': f"{correct_count / len(valid_df) * 100:.2f}%" if len(valid_df) > 0 else 'N/A',
                        'å¹³å‡æƒå€¼': f"{tier_df['æ ·æœ¬æƒå€¼'].mean():.4f}",
                        'å™ªå£°æ ·æœ¬æ•°': len(tier_df[tier_df['æ˜¯å¦å™ªå£°'] == 'æ˜¯']),
                        'å¹²å‡€æ ·æœ¬æ•°': len(tier_df[tier_df['æ˜¯å¦å™ªå£°'] == 'å¦'])
                    })
                
                summary_df = pd.DataFrame(tier_summary)
                summary_df.to_excel(writer, sheet_name='ğŸ“Šæ€»è§ˆç»Ÿè®¡', index=False)
                
                # ä¸ºæ€»è§ˆç»Ÿè®¡æ·»åŠ æ ¼å¼
                ws = writer.sheets['ğŸ“Šæ€»è§ˆç»Ÿè®¡']
                for row in range(2, len(summary_df) + 2):
                    # æ­£ç¡®å¤„ç† - ç»¿è‰²
                    ws.cell(row=row, column=3).fill = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")
                    # é”™è¯¯å¤„ç† - çº¢è‰²
                    ws.cell(row=row, column=4).fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
                    # å·²ä¸¢å¼ƒ - ç°è‰²
                    ws.cell(row=row, column=5).fill = PatternFill(start_color="D9D9D9", end_color="D9D9D9", fill_type="solid")
                
                # Sheet 2: æ‰€æœ‰æ ·æœ¬ï¼ˆå®Œæ•´æ•°æ®ï¼‰
                df_reordered.to_excel(writer, sheet_name='æ‰€æœ‰æ ·æœ¬', index=False)
                ws_all = writer.sheets['æ‰€æœ‰æ ·æœ¬']
                
                # ä¸ºæ‰€æœ‰æ ·æœ¬æ·»åŠ æ¡ä»¶æ ¼å¼
                for row in range(2, len(df_reordered) + 2):
                    correction_status = df_reordered.iloc[row-2]['çŸ«æ­£æ˜¯å¦æ­£ç¡®']
                    if correction_status == 'æ­£ç¡®':
                        # æ•´è¡Œæµ…ç»¿è‰²
                        for col in range(1, len(df_reordered.columns) + 1):
                            ws_all.cell(row=row, column=col).fill = PatternFill(start_color="E8F5E9", end_color="E8F5E9", fill_type="solid")
                    elif correction_status == 'é”™è¯¯':
                        # æ•´è¡Œæµ…çº¢è‰²
                        for col in range(1, len(df_reordered.columns) + 1):
                            ws_all.cell(row=row, column=col).fill = PatternFill(start_color="FFEBEE", end_color="FFEBEE", fill_type="solid")
                
                # Sheet 3-N: æ¯ä¸ªTierçš„è¯¦ç»†æ•°æ®ï¼ˆæŒ‰ç­–ç•¥åˆ†ç»„ï¼‰
                tier_order = [
                    'Tier 1: Core',
                    'Tier 2: Flip',
                    'Tier 3: Keep',
                    'Tier 4: Reweight',
                    'Tier 5a: Rescued-Keep',
                    'Tier 5b: Rescued-Flip',
                    'Tier 5b: Rescued-Flip (Dropå‰æ‹¯æ•‘)',
                    'Dropped'
                ]
                
                for tier in tier_order:
                    tier_data = df_reordered[df_reordered['Tieråˆ†çº§'] == tier]
                    
                    if len(tier_data) == 0:
                        # å°è¯•æ¨¡ç³ŠåŒ¹é…
                        tier_base = tier.split('(')[0].strip()
                        tier_data = df_reordered[df_reordered['Tieråˆ†çº§'].str.contains(tier_base, na=False, regex=False)]
                    
                    if len(tier_data) > 0:
                        # åˆ†ç¦»æ­£ç¡®å’Œé”™è¯¯å¤„ç†çš„æ ·æœ¬
                        correct_samples = tier_data[tier_data['çŸ«æ­£æ˜¯å¦æ­£ç¡®'] == 'æ­£ç¡®']
                        error_samples = tier_data[tier_data['çŸ«æ­£æ˜¯å¦æ­£ç¡®'] == 'é”™è¯¯']
                        dropped_samples = tier_data[tier_data['çŸ«æ­£æ˜¯å¦æ­£ç¡®'] == 'ä¸é€‚ç”¨(å·²ä¸¢å¼ƒ)']
                        
                        # åˆ›å»ºsheetåç§°ï¼ˆExcelé™åˆ¶31å­—ç¬¦ï¼‰
                        sheet_name = tier.replace(':', '').replace('(', '').replace(')', '').replace(' ', '_')[:28]
                        
                        # åˆå¹¶æ•°æ®ï¼šæ­£ç¡®çš„åœ¨å‰ï¼Œé”™è¯¯çš„åœ¨å
                        combined_data = pd.concat([correct_samples, error_samples, dropped_samples])
                        combined_data.to_excel(writer, sheet_name=sheet_name, index=False)
                        
                        # æ·»åŠ é¢œè‰²æ ‡è®°
                        ws = writer.sheets[sheet_name]
                        current_row = 2
                        
                        # æ­£ç¡®æ ·æœ¬ - ç»¿è‰²èƒŒæ™¯
                        for _ in range(len(correct_samples)):
                            for col in range(1, len(combined_data.columns) + 1):
                                ws.cell(row=current_row, column=col).fill = PatternFill(start_color="E8F5E9", end_color="E8F5E9", fill_type="solid")
                            current_row += 1
                        
                        # é”™è¯¯æ ·æœ¬ - çº¢è‰²èƒŒæ™¯
                        for _ in range(len(error_samples)):
                            for col in range(1, len(combined_data.columns) + 1):
                                ws.cell(row=current_row, column=col).fill = PatternFill(start_color="FFEBEE", end_color="FFEBEE", fill_type="solid")
                            current_row += 1
                        
                        # ä¸¢å¼ƒæ ·æœ¬ - ç°è‰²èƒŒæ™¯
                        for _ in range(len(dropped_samples)):
                            for col in range(1, len(combined_data.columns) + 1):
                                ws.cell(row=current_row, column=col).fill = PatternFill(start_color="F5F5F5", end_color="F5F5F5", fill_type="solid")
                            current_row += 1
                        
                        logger.info(f"  âœ“ {sheet_name}: {len(correct_samples)}æ­£ç¡® / {len(error_samples)}é”™è¯¯ / {len(dropped_samples)}ä¸¢å¼ƒ")
                
                # é¢å¤–çš„åˆ†æsheet
                # Sheet: æ‰€æœ‰é”™è¯¯æ ·æœ¬æ±‡æ€»
                error_samples = df_reordered[df_reordered['çŸ«æ­£æ˜¯å¦æ­£ç¡®'] == 'é”™è¯¯']
                if len(error_samples) > 0:
                    error_samples.to_excel(writer, sheet_name='âŒæ‰€æœ‰é”™è¯¯æ ·æœ¬', index=False)
                    logger.info(f"  âœ“ é”™è¯¯æ ·æœ¬æ±‡æ€»: {len(error_samples)}ä¸ª")
                
                # Sheet: è¯¯æ€åˆ†æï¼ˆè¢«é”™è¯¯å¤„ç†çš„å¹²å‡€æ ·æœ¬ï¼‰
                false_positive = df_reordered[(df_reordered['æ˜¯å¦å™ªå£°'] == 'å¦') & (df_reordered['çŸ«æ­£æ˜¯å¦æ­£ç¡®'] == 'é”™è¯¯')]
                if len(false_positive) > 0:
                    false_positive.to_excel(writer, sheet_name='âš ï¸è¯¯æ€å¹²å‡€æ ·æœ¬', index=False)
                    logger.info(f"  âœ“ è¯¯æ€æ ·æœ¬: {len(false_positive)}ä¸ª")
                
                # Sheet: æ¼ç½‘ä¹‹é±¼ï¼ˆæœªè¢«çŸ«æ­£çš„å™ªå£°ï¼‰
                false_negative = df_reordered[(df_reordered['æ˜¯å¦å™ªå£°'] == 'æ˜¯') & (df_reordered['çŸ«æ­£æ˜¯å¦æ­£ç¡®'] == 'é”™è¯¯')]
                if len(false_negative) > 0:
                    false_negative.to_excel(writer, sheet_name='ğŸŸæ¼ç½‘å™ªå£°', index=False)
                    logger.info(f"  âœ“ æ¼ç½‘å™ªå£°: {len(false_negative)}ä¸ª")
            
            logger.info(f"âœ“ Excelæ–‡ä»¶å·²ä¿å­˜ï¼ˆå¤šsheetï¼Œå¸¦é¢œè‰²æ ‡è®°ï¼‰: {excel_path}")
            logger.info(f"  åŒ…å« {len(pd.ExcelFile(excel_path).sheet_names)} ä¸ªå·¥ä½œè¡¨")
            logger.info(f"  é¢œè‰²è¯´æ˜: ç»¿è‰²=æ­£ç¡®å¤„ç†, çº¢è‰²=é”™è¯¯å¤„ç†, ç°è‰²=å·²ä¸¢å¼ƒ")
        except Exception as e:
            logger.warning(f"âš  Excelæ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")
            import traceback
            logger.warning(traceback.format_exc())
    else:
        logger.info("  æç¤º: å®‰è£…openpyxlå¯ç”Ÿæˆå¤šsheet Excelæ–‡ä»¶ (pip install openpyxl)")
    
    # ========================================
    # æ–°æ ¼å¼ï¼šæŒ‰ç­–ç•¥åˆ†ç»„çš„æ—¥å¿—æ€»ç»“
    # ========================================
    
    # æŒ‰Tieråˆ†ç»„æ ·æœ¬
    tier_groups = {}
    for i, row in df.iterrows():
        tier = row['Tieråˆ†çº§']
        if tier not in tier_groups:
            tier_groups[tier] = {'correct': [], 'incorrect': [], 'dropped': []}
        
        if row['çŸ«æ­£æ˜¯å¦æ­£ç¡®'] == 'æ­£ç¡®':
            tier_groups[tier]['correct'].append(row)
        elif row['çŸ«æ­£æ˜¯å¦æ­£ç¡®'] == 'é”™è¯¯':
            tier_groups[tier]['incorrect'].append(row)
        else:  # ä¸é€‚ç”¨(å·²ä¸¢å¼ƒ)
            tier_groups[tier]['dropped'].append(row)
    
    # å®šä¹‰Tieré¡ºåºå’Œè§’è‰²è¯´æ˜ï¼ˆåŒ…å«è¯¦ç»†è·¯å¾„ï¼‰
    tier_order = [
        'Tier 1: Core',
        'Tier 2: Flip',
        'Tier 3: Keep',
        'Tier 4: Reweight',
        'Tier 5a: Rescued-Keep',
        'Tier 5b: Rescued-Flip',
        'Tier 5b: Rescued-Flip (Dropå‰æ‹¯æ•‘)',
        'Dropped'
    ]
    
    role_map = {
        'Tier 1: Core': 'Phase 1 - å®šæµ·ç¥é’ˆï¼šç»å¯¹çº¯å‡€çš„åŸºçŸ³æ•°æ®',
        'Tier 2: Flip': 'Phase 2 - æ™ºèƒ½ç¿»è½¬ï¼šåŸºäºCLå·®å€¼(â‰¥0.15)å’ŒMADEå¯†åº¦çš„é›¶è¯¯æ€ç¿»è½¬',
        'Tier 3: Keep': 'Phase 2 - ä¿æŒæ ·æœ¬ï¼šé«˜è´¨é‡ä¿æŒ',
        'Tier 4: Reweight': 'Phase 2 - é‡åŠ æƒæ ·æœ¬ï¼šä¸ç¡®å®šæ ·æœ¬é™æƒ',
        'Tier 5a: Rescued-Keep': 'Phase 3 - é”šç‚¹æ‹¯æ•‘ï¼šæ‹¯æ•‘çš„ä¿æŒæ ·æœ¬',
        'Tier 5b: Rescued-Flip': 'Phase 3 - é”šç‚¹æ‹¯æ•‘ï¼šæ‹¯æ•‘çš„ç¿»è½¬æ ·æœ¬',
        'Tier 5b: Rescued-Flip (Dropå‰æ‹¯æ•‘)': 'Phase 3 - é”šç‚¹æ‹¯æ•‘ï¼šDropå‰æ‹¯æ•‘çš„ç¿»è½¬æ ·æœ¬',
        'Dropped': 'å·²ä¸¢å¼ƒï¼šè´¨é‡è¿‡ä½'
    }
    
    # ç­–ç•¥è¯¦ç»†è¯´æ˜
    strategy_details = {
        'Tier 1: Core': {
            'phase': 'Phase 1: æ ¸å¿ƒä¸¥é€‰',
            'condition': 'CLç½®ä¿¡åº¦â‰¥é˜ˆå€¼ æˆ– KNNä¸€è‡´æ€§â‰¥é˜ˆå€¼',
            'action': 'ä¿æŒåŸæ ‡ç­¾',
            'weight': '1.0'
        },
        'Tier 2: Flip': {
            'phase': 'Phase 2: æ™ºèƒ½ç¿»è½¬',
            'condition': 'CLå·®å€¼â‰¥0.15 + æ­£å¸¸æ ·æœ¬(CLâ‰¤0.95ä¸”å¯†åº¦â‰¤35ç¿»è½¬) / æ¶æ„æ ·æœ¬(å¯†åº¦â‰¥15ä¸”CLâ‰¥0.5ç¿»è½¬)',
            'action': 'ç¿»è½¬æ ‡ç­¾',
            'weight': '1.0'
        },
        'Tier 3: Keep': {
            'phase': 'Phase 2: åˆ†çº§æŒ½æ•‘ - Keepç­–ç•¥',
            'condition': 'KNNæ”¯æŒ + åŸCLâ‰¥0.55 + KNNâ‰¥0.55 + MADEæ­£å¸¸',
            'action': 'ä¿æŒåŸæ ‡ç­¾',
            'weight': '0.8-1.0'
        },
        'Tier 4: Reweight': {
            'phase': 'Phase 2: åˆ†çº§æŒ½æ•‘ - Reweightç­–ç•¥',
            'condition': 'æœªè¾¾åˆ°Core/Flip/Keepæ ‡å‡†',
            'action': 'ä¿æŒåŸæ ‡ç­¾ä½†é™æƒ',
            'weight': '0.5'
        },
        'Tier 5a: Rescued-Keep': {
            'phase': 'Phase 3: é”šç‚¹æ‹¯æ•‘',
            'condition': 'iter_Tâ‰¥0.6 + anchorå¼ºæ”¯æŒå½“å‰(â‰¥0.6)',
            'action': 'ä¿æŒåŸæ ‡ç­¾',
            'weight': '0.85'
        },
        'Tier 5b: Rescued-Flip': {
            'phase': 'Phase 3: é”šç‚¹æ‹¯æ•‘',
            'condition': 'iter_Tâ‰¥0.6 + anchorå¼ºåå¯¹å½“å‰(â‰¥0.6)',
            'action': 'ç¿»è½¬æ ‡ç­¾',
            'weight': '0.75'
        },
        'Tier 5b: Rescued-Flip (Dropå‰æ‹¯æ•‘)': {
            'phase': 'Phase 3: é”šç‚¹æ‹¯æ•‘ï¼ˆDropå‰ï¼‰',
            'condition': 'iter_Tâ‰¥0.6 + anchoræ”¯æŒç›®æ ‡â‰¥0.6 + orig_KNNæ”¯æŒç›®æ ‡â‰¥0.55',
            'action': 'ç¿»è½¬æ ‡ç­¾ï¼ˆé¿å…Dropï¼‰',
            'weight': '1.0'
        },
        'Dropped': {
            'phase': 'Phase 3: æœ€ç»ˆæ¸…ç†',
            'condition': 'iter_CL<0.48 æˆ– (iter_CL<0.55 + anchor<0.55)',
            'action': 'ä¸¢å¼ƒæ ·æœ¬',
            'weight': '0.0'
        }
    }
    
    # å°†æ–‡ä»¶åç¼€æ”¹ä¸º.logä»¥æ”¯æŒæ›´å¥½çš„è¡¨æ ¼æ ¼å¼
    log_path = save_path.replace('.txt', '.log')
    
    # Save to formatted log file (æ–°æ ¼å¼ - è¡¨æ ¼å½¢å¼)
    with open(log_path, 'w', encoding='utf-8') as f:
        f.write("="*120 + "\n")
        f.write("MEDAL-Lite æ ‡ç­¾çŸ«æ­£åˆ†æ - ç­–ç•¥åˆ†ç»„æ—¥å¿—æŠ¥å‘Š\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("="*120 + "\n\n")
        
        # Summary statistics
        f.write("ğŸ“Š æ€»ä½“ç»Ÿè®¡\n")
        f.write("-"*120 + "\n")
        f.write(f"æ€»æ ·æœ¬æ•°: {n_samples}\n")
        f.write(f"  æ­£å¸¸æµé‡:  {(y_true == 0).sum()} ({100*(y_true == 0).sum()/n_samples:.1f}%)\n")
        f.write(f"  æ¶æ„æµé‡:  {(y_true == 1).sum()} ({100*(y_true == 1).sum()/n_samples:.1f}%)\n\n")
        
        f.write(f"å™ªå£°æ³¨å…¥ç‡: {100*noise_mask.sum()/n_samples:.1f}%\n")
        f.write(f"  å™ªå£°æ ·æœ¬æ•°: {noise_mask.sum()}\n\n")
        
        action_mask = results['action_mask']
        f.write(f"çŸ«æ­£åŠ¨ä½œç»Ÿè®¡:\n")
        f.write(f"  ä¿æŒ:     {(action_mask == 0).sum()} ({100*(action_mask == 0).sum()/n_samples:.1f}%)\n")
        f.write(f"  ç¿»è½¬:     {(action_mask == 1).sum()} ({100*(action_mask == 1).sum()/n_samples:.1f}%)\n")
        f.write(f"  ä¸¢å¼ƒ:     {(action_mask == 2).sum()} ({100*(action_mask == 2).sum()/n_samples:.1f}%)\n")
        f.write(f"  é‡åŠ æƒ:   {(action_mask == 3).sum()} ({100*(action_mask == 3).sum()/n_samples:.1f}%)\n\n")
        
        # Correction accuracy (excluding dropped samples)
        keep_mask = action_mask != 2
        corrected_labels = results['clean_labels'][keep_mask]
        true_labels_kept = y_true[keep_mask]
        correction_accuracy = (corrected_labels == true_labels_kept).mean()
        f.write(f"çŸ«æ­£å‡†ç¡®ç‡ (ä¸å«ä¸¢å¼ƒæ ·æœ¬): {correction_accuracy*100:.2f}%\n")
        f.write(f"  æ­£ç¡®: {(corrected_labels == true_labels_kept).sum()}\n")
        f.write(f"  é”™è¯¯: {(corrected_labels != true_labels_kept).sum()}\n\n")
        
        f.write("="*120 + "\n\n")
        
        # æŒ‰ç­–ç•¥åˆ†ç»„çš„è¯¦ç»†åˆ†æ
        f.write("ğŸ“‹ ç­–ç•¥åˆ†ç»„è¯¦ç»†åˆ†æï¼ˆæŒ‰é˜¶æ®µå’Œè·¯å¾„ï¼‰\n")
        f.write("="*120 + "\n\n")
        
        # æŒ‰Phaseåˆ†ç»„
        phase_groups = {
            'Phase 1: æ ¸å¿ƒä¸¥é€‰': [],
            'Phase 2: åˆ†çº§æŒ½æ•‘': [],
            'Phase 3: é”šç‚¹æ‹¯æ•‘': [],
            'å·²ä¸¢å¼ƒ': []
        }
        
        for tier in tier_order:
            if tier not in tier_groups or len(tier_groups[tier]['correct']) + len(tier_groups[tier]['incorrect']) + len(tier_groups[tier]['dropped']) == 0:
                continue
            
            if 'Tier 1' in tier:
                phase_groups['Phase 1: æ ¸å¿ƒä¸¥é€‰'].append(tier)
            elif 'Tier 2' in tier or 'Tier 3' in tier or 'Tier 4' in tier:
                phase_groups['Phase 2: åˆ†çº§æŒ½æ•‘'].append(tier)
            elif 'Tier 5' in tier:
                phase_groups['Phase 3: é”šç‚¹æ‹¯æ•‘'].append(tier)
            elif 'Dropped' in tier:
                phase_groups['å·²ä¸¢å¼ƒ'].append(tier)
        
        # æŒ‰Phaseè¾“å‡º
        for phase_name, tiers in phase_groups.items():
            if not tiers:
                continue
            
            f.write("\n" + "="*120 + "\n")
            f.write(f"ã€{phase_name}ã€‘\n")
            f.write("="*120 + "\n\n")
            
            for tier in tiers:
                if tier not in tier_groups:
                    continue
                
                correct_samples = tier_groups[tier]['correct']
                incorrect_samples = tier_groups[tier]['incorrect']
                dropped_samples = tier_groups[tier]['dropped']
                total_samples = len(correct_samples) + len(incorrect_samples) + len(dropped_samples)
                
                if total_samples == 0:
                    continue
                
                role = role_map.get(tier, '')
                
                # è®¡ç®—å‡†ç¡®ç‡ï¼ˆä¸åŒ…æ‹¬ä¸¢å¼ƒçš„æ ·æœ¬ï¼‰
                valid_samples = len(correct_samples) + len(incorrect_samples)
                accuracy = len(correct_samples) / valid_samples * 100 if valid_samples > 0 else 0
                
                # è·å–è¯¥Tierçš„æƒé‡
                if correct_samples:
                    weight = correct_samples[0]['æ ·æœ¬æƒå€¼']
                elif incorrect_samples:
                    weight = incorrect_samples[0]['æ ·æœ¬æƒå€¼']
                elif dropped_samples:
                    weight = dropped_samples[0]['æ ·æœ¬æƒå€¼']
                else:
                    weight = 'N/A'
                
                f.write(f"â”Œâ”€ ã€ç­–ç•¥ã€‘{tier}\n")
                f.write(f"â”‚  è§’è‰²å®šä½: {role}\n")
                
                # è¾“å‡ºç­–ç•¥è¯¦ç»†è¯´æ˜
                if tier in strategy_details:
                    details = strategy_details[tier]
                    f.write(f"â”‚  å†³ç­–æ¡ä»¶: {details['condition']}\n")
                    f.write(f"â”‚  æ‰§è¡ŒåŠ¨ä½œ: {details['action']}\n")
                    f.write(f"â”‚  æ ·æœ¬æƒé‡: {details['weight']}\n")
                
                f.write(f"â”‚  æ ·æœ¬ç»Ÿè®¡: æ€»æ•°={total_samples} | å®é™…æƒé‡={weight} | å‡†ç¡®ç‡={accuracy:.1f}%\n")
                f.write(f"â”‚             æ­£ç¡®={len(correct_samples)} | é”™è¯¯={len(incorrect_samples)} | ä¸¢å¼ƒ={len(dropped_samples)}\n")
                f.write(f"â””â”€" + "-"*116 + "\n\n")
                
                # æ­£ç¡®å¤„ç†çš„æ ·æœ¬ - è¡¨æ ¼å½¢å¼
                if correct_samples:
                    f.write(f"  âœ“ æ­£ç¡®å¤„ç†çš„æ ·æœ¬ ({len(correct_samples)}ä¸ª):\n")
                    f.write("  " + "-"*150 + "\n")
                    # è¡¨å¤´ - æ·»åŠ æ›´å¤šå…³é”®æŒ‡æ ‡
                    f.write(f"  {'æ ·æœ¬ID':>8s} | {'çœŸå®':>4s} | {'å™ªå£°':>4s} | {'çŸ«æ­£å':>6s} | "
                           f"{'åŸCL':>6s} | {'iter_CL':>7s} | {'iter_T':>7s} | "
                           f"{'anchor':>7s} | {'orig_KNN':>8s} | å†³ç­–ç†ç”±\n")
                    f.write("  " + "-"*150 + "\n")
                    
                    # æ˜¾ç¤ºå‰10ä¸ªæ ·æœ¬
                    for row in correct_samples[:10]:
                        # åŸå§‹CLç½®ä¿¡åº¦
                        orig_cl = row['CLæ­£å¸¸æ¦‚ç‡'] if row['å™ªå£°æ ‡ç­¾']=='æ­£å¸¸' else row['CLæ¶æ„æ¦‚ç‡']
                        # è¿­ä»£CL
                        iter_cl_current = row.get('iter_CLå½“å‰æ ‡ç­¾ç½®ä¿¡åº¦', 'N/A')
                        iter_cl_target = row.get('iter_CLç›®æ ‡æ ‡ç­¾ç½®ä¿¡åº¦', 'N/A')
                        # é”šç‚¹KNN
                        anchor_cons = row.get('anchor_KNNä¸€è‡´æ€§', 'N/A')
                        # åŸå§‹KNN
                        orig_knn = row.get('KNNä¸€è‡´æ€§', 'N/A')
                        
                        reason_short = row['å†³ç­–ç†ç”±'][:40] + '...' if len(row['å†³ç­–ç†ç”±']) > 40 else row['å†³ç­–ç†ç”±']
                        f.write(f"  {row['æ ·æœ¬ID']:>8d} | {str(row['çœŸå®æ ‡ç­¾']):>4} | {str(row['å™ªå£°æ ‡ç­¾']):>4} | "
                               f"{str(row['çŸ«æ­£åæ ‡ç­¾']):>6} | "
                               f"{str(orig_cl)[:6]:>6} | {str(iter_cl_current)[:7]:>7} | {str(iter_cl_target)[:7]:>7} | "
                               f"{str(anchor_cons)[:7]:>7} | {str(orig_knn)[:8]:>8} | {reason_short}\n")
                    
                    if len(correct_samples) > 10:
                        f.write(f"  ... è¿˜æœ‰ {len(correct_samples) - 10} ä¸ªæ­£ç¡®æ ·æœ¬ï¼ˆè¯¦è§Excelæ–‡ä»¶ï¼‰\n")
                    f.write("\n")
                
                # é”™è¯¯å¤„ç†çš„æ ·æœ¬ - è¡¨æ ¼å½¢å¼ï¼ˆæ˜¾ç¤ºæ‰€æœ‰ï¼‰
                if incorrect_samples:
                    f.write(f"  âœ— é”™è¯¯å¤„ç†çš„æ ·æœ¬ ({len(incorrect_samples)}ä¸ª):\n")
                    f.write("  " + "-"*150 + "\n")
                    # è¡¨å¤´
                    f.write(f"  {'æ ·æœ¬ID':>8s} | {'çœŸå®':>4s} | {'å™ªå£°':>4s} | {'çŸ«æ­£å':>6s} | "
                           f"{'åŸCL':>6s} | {'iter_CL':>7s} | {'iter_T':>7s} | "
                           f"{'anchor':>7s} | {'orig_KNN':>8s} | é”™è¯¯åŸå› \n")
                    f.write("  " + "-"*150 + "\n")
                    
                    for row in incorrect_samples:
                        # åŸå§‹CLç½®ä¿¡åº¦
                        orig_cl = row['CLæ­£å¸¸æ¦‚ç‡'] if row['å™ªå£°æ ‡ç­¾']=='æ­£å¸¸' else row['CLæ¶æ„æ¦‚ç‡']
                        # è¿­ä»£CL
                        iter_cl_current = row.get('iter_CLå½“å‰æ ‡ç­¾ç½®ä¿¡åº¦', 'N/A')
                        iter_cl_target = row.get('iter_CLç›®æ ‡æ ‡ç­¾ç½®ä¿¡åº¦', 'N/A')
                        # é”šç‚¹KNN
                        anchor_cons = row.get('anchor_KNNä¸€è‡´æ€§', 'N/A')
                        # åŸå§‹KNN
                        orig_knn = row.get('KNNä¸€è‡´æ€§', 'N/A')
                        
                        error_reason = f"çœŸå®={row['çœŸå®æ ‡ç­¾']}, çŸ«æ­£ä¸º{row['çŸ«æ­£åæ ‡ç­¾']}"
                        f.write(f"  {row['æ ·æœ¬ID']:>8d} | {str(row['çœŸå®æ ‡ç­¾']):>4} | {str(row['å™ªå£°æ ‡ç­¾']):>4} | "
                               f"{str(row['çŸ«æ­£åæ ‡ç­¾']):>6} | "
                               f"{str(orig_cl)[:6]:>6} | {str(iter_cl_current)[:7]:>7} | {str(iter_cl_target)[:7]:>7} | "
                               f"{str(anchor_cons)[:7]:>7} | {str(orig_knn)[:8]:>8} | {error_reason}\n")
                    f.write("\n")
                
                # ä¸¢å¼ƒçš„æ ·æœ¬ - è¡¨æ ¼å½¢å¼
                if dropped_samples:
                    f.write(f"  ğŸ—‘ ä¸¢å¼ƒçš„æ ·æœ¬ ({len(dropped_samples)}ä¸ª):\n")
                    f.write("  " + "-"*150 + "\n")
                    # è¡¨å¤´
                    f.write(f"  {'æ ·æœ¬ID':>8s} | {'çœŸå®':>4s} | {'å™ªå£°':>4s} | {'æ˜¯å¦å™ªå£°':>8s} | "
                           f"{'åŸCL':>6s} | {'iter_CL':>7s} | {'iter_T':>7s} | "
                           f"{'anchor':>7s} | {'orig_KNN':>8s} | ä¸¢å¼ƒåŸå› \n")
                    f.write("  " + "-"*150 + "\n")
                    
                    # æ˜¾ç¤ºå‰10ä¸ªä¸¢å¼ƒæ ·æœ¬
                    for row in dropped_samples[:10]:
                        # åŸå§‹CLç½®ä¿¡åº¦
                        orig_cl = row['CLæ­£å¸¸æ¦‚ç‡'] if row['å™ªå£°æ ‡ç­¾']=='æ­£å¸¸' else row['CLæ¶æ„æ¦‚ç‡']
                        # è¿­ä»£CL
                        iter_cl_current = row.get('iter_CLå½“å‰æ ‡ç­¾ç½®ä¿¡åº¦', 'N/A')
                        iter_cl_target = row.get('iter_CLç›®æ ‡æ ‡ç­¾ç½®ä¿¡åº¦', 'N/A')
                        # é”šç‚¹KNN
                        anchor_cons = row.get('anchor_KNNä¸€è‡´æ€§', 'N/A')
                        # åŸå§‹KNN
                        orig_knn = row.get('KNNä¸€è‡´æ€§', 'N/A')
                        
                        drop_reason = "CLæä½" if 'CL' in row['å†³ç­–ç†ç”±'] else "æ— æ³•æŒ½æ•‘"
                        f.write(f"  {row['æ ·æœ¬ID']:>8d} | {str(row['çœŸå®æ ‡ç­¾']):>4} | {str(row['å™ªå£°æ ‡ç­¾']):>4} | "
                               f"{str(row['æ˜¯å¦å™ªå£°']):>8} | "
                               f"{str(orig_cl)[:6]:>6} | {str(iter_cl_current)[:7]:>7} | {str(iter_cl_target)[:7]:>7} | "
                               f"{str(anchor_cons)[:7]:>7} | {str(orig_knn)[:8]:>8} | {drop_reason}\n")
                    
                    if len(dropped_samples) > 10:
                        f.write(f"  ... è¿˜æœ‰ {len(dropped_samples) - 10} ä¸ªä¸¢å¼ƒæ ·æœ¬ï¼ˆè¯¦è§Excelæ–‡ä»¶ï¼‰\n")
                    f.write("\n")
                
                f.write("\n")
        
        # æ·»åŠ è¯¯æ€åˆ†æï¼ˆè¢«é”™è¯¯ä¸¢å¼ƒçš„å¹²å‡€æ ·æœ¬ï¼‰
        f.write("="*120 + "\n")
        f.write("âš ï¸  è¯¯æ€åˆ†æ - è¢«é”™è¯¯ä¸¢å¼ƒçš„å¹²å‡€æ ·æœ¬\n")
        f.write("="*120 + "\n\n")
        
        # æ‰¾å‡ºæ‰€æœ‰è¢«ä¸¢å¼ƒä½†å®é™…æ˜¯å¹²å‡€çš„æ ·æœ¬
        false_drops = []
        for i, row in df.iterrows():
            if row['çŸ«æ­£åŠ¨ä½œ'] == 'ä¸¢å¼ƒ' and row['æ˜¯å¦å™ªå£°'] == 'å¦':
                false_drops.append(row)
        
        if false_drops:
            f.write(f"ğŸ“Š è¢«è¯¯æ€çš„å¹²å‡€æ ·æœ¬è¯¦æƒ… (å…±{len(false_drops)}ä¸ª):\n")
            f.write("-"*120 + "\n")
            f.write(f"  {'æ ·æœ¬ID':>8s} | {'æ ‡ç­¾':>4s} | {'iter_CL':>7s} | {'iter_CL_T':>9s} | {'anchor':>7s} | {'orig_KNN':>8s} | åŸå› \n")
            f.write("-"*120 + "\n")
            
            for row in false_drops:
                # ä½¿ç”¨iter_CLå½“å‰æ ‡ç­¾ç½®ä¿¡åº¦ï¼ˆè¿™æ‰æ˜¯Dropåˆ¤æ–­çš„ä¾æ®ï¼‰
                iter_cl_current = row.get('iter_CLå½“å‰æ ‡ç­¾ç½®ä¿¡åº¦', 'N/A')
                iter_cl_target = row.get('iter_CLç›®æ ‡æ ‡ç­¾ç½®ä¿¡åº¦', 'N/A')
                anchor_cons = row.get('anchor_KNNä¸€è‡´æ€§', 'N/A')
                orig_knn_cons = row.get('KNNä¸€è‡´æ€§', 'N/A')
                
                f.write(f"  {row['æ ·æœ¬ID']:>8d} | {str(row['çœŸå®æ ‡ç­¾']):>4} | {str(iter_cl_current):>7} | "
                       f"{str(iter_cl_target):>9} | {str(anchor_cons):>7} | {str(orig_knn_cons):>8} | CLæä½\n")
        else:
            f.write("âœ“ æ²¡æœ‰è¯¯æ€å¹²å‡€æ ·æœ¬\n")
        
        f.write("\n")
        
        # æ·»åŠ æ¼ç½‘ä¹‹é±¼åˆ†æï¼ˆæœªè¢«æ£€æµ‹åˆ°çš„å™ªå£°æ ·æœ¬ï¼‰
        f.write("="*120 + "\n")
        f.write("ğŸŸ æ¼ç½‘ä¹‹é±¼åˆ†æ - æœªè¢«çŸ«æ­£çš„å™ªå£°æ ·æœ¬\n")
        f.write("="*120 + "\n\n")
        
        # æ‰¾å‡ºæ‰€æœ‰æ˜¯å™ªå£°ä½†æœªè¢«ç¿»è½¬çš„æ ·æœ¬
        missed_noise = []
        for i, row in df.iterrows():
            if row['æ˜¯å¦å™ªå£°'] == 'æ˜¯' and row['çŸ«æ­£åŠ¨ä½œ'] != 'ç¿»è½¬':
                missed_noise.append(row)
        
        if missed_noise:
            f.write(f"ğŸ“Š æ¼ç½‘çš„å™ªå£°æ ·æœ¬è¯¦æƒ… (å…±{len(missed_noise)}ä¸ª):\n")
            f.write("-"*120 + "\n")
            f.write(f"  {'æ ·æœ¬ID':>8s} | {'çœŸå®':>4s} | {'å™ªå£°':>4s} | {'åŠ¨ä½œ':>4s} | "
                   f"{'CLç½®ä¿¡':>7s} | {'KNNä¸€è‡´':>7s} | {'MADEå¯†åº¦':>9s} | åŸå› \n")
            f.write("-"*120 + "\n")
            
            for row in missed_noise[:20]:  # æ˜¾ç¤ºå‰20ä¸ª
                cl_conf = row['CLæ­£å¸¸æ¦‚ç‡'] if row['å™ªå£°æ ‡ç­¾']=='æ­£å¸¸' else row['CLæ¶æ„æ¦‚ç‡']
                reason = "æœªè¢«CLæ£€æµ‹" if row['CLç–‘ä¼¼å™ªå£°'] == 'å¦' else "KNNæ”¯æŒé”™è¯¯æ ‡ç­¾"
                f.write(f"  {row['æ ·æœ¬ID']:>8d} | {str(row['çœŸå®æ ‡ç­¾']):>4} | {str(row['å™ªå£°æ ‡ç­¾']):>4} | "
                       f"{str(row['çŸ«æ­£åŠ¨ä½œ']):>4} | {str(cl_conf):>7} | "
                       f"{str(row['KNNä¸€è‡´æ€§']):>7} | {str(row['MADEå¯†åº¦åˆ†æ•°']):>9} | {reason}\n")
            
            if len(missed_noise) > 20:
                f.write(f"  ... è¿˜æœ‰ {len(missed_noise) - 20} ä¸ªæ¼ç½‘æ ·æœ¬ï¼ˆè¯¦è§CSVæ–‡ä»¶ï¼‰\n")
        else:
            f.write("âœ“ æ‰€æœ‰å™ªå£°æ ·æœ¬éƒ½è¢«æˆåŠŸæ£€æµ‹\n")
        
        f.write("\n")
        f.write("="*120 + "\n")
        f.write("æŠ¥å‘Šç»“æŸ\n")
        f.write("="*120 + "\n")
    
    logger.info(f"âœ“ ç­–ç•¥åˆ†ç»„æ—¥å¿—å·²ä¿å­˜: {log_path}")
    logger.info(f"  å·²åˆ†ææ ·æœ¬æ€»æ•°: {n_samples}")
    logger.info(f"  ç­–ç•¥åˆ†ç»„æ•°: {len([t for t in tier_order if t in tier_groups])}")
    
    return df


def plot_feature_distributions(results, y_true, noise_mask, save_dir, logger):
    """
    Generate feature distribution plots for clean, noisy, and corrected data
    
    Args:
        results: dict from run_hybrid_court_with_detailed_tracking
        y_true: (N,) ground truth labels
        noise_mask: (N,) boolean array indicating noise
        save_dir: directory to save plots
        logger: logger
    """
    logger.info("")
    logger.info("="*70)
    logger.info("Generating Feature Distribution Plots")
    logger.info("="*70)
    
    features = results['features']
    noisy_labels = results['noisy_labels']
    clean_labels = results['clean_labels']
    action_mask = results['action_mask']
    
    os.makedirs(save_dir, exist_ok=True)
    
    # Use t-SNE for dimensionality reduction
    logger.info("Running t-SNE for dimensionality reduction...")
    tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)
    features_2d = tsne.fit_transform(features)
    logger.info("âœ“ t-SNE complete")
    
    # Define colors
    color_benign = '#2E86AB'  # Blue
    color_malicious = '#A23B72'  # Red
    
    # 1. Ground Truth Labels (Clean Data)
    logger.info("ç»˜åˆ¶ 1/4: çœŸå®æ ‡ç­¾åˆ†å¸ƒ...")
    fig, ax = plt.subplots(figsize=(10, 8))
    
    benign_mask = y_true == 0
    malicious_mask = y_true == 1
    
    ax.scatter(features_2d[benign_mask, 0], features_2d[benign_mask, 1],
               c=color_benign, label=f'æ­£å¸¸æµé‡ (n={benign_mask.sum()})', 
               alpha=0.6, s=30, edgecolors='k', linewidths=0.3)
    ax.scatter(features_2d[malicious_mask, 0], features_2d[malicious_mask, 1],
               c=color_malicious, label=f'æ¶æ„æµé‡ (n={malicious_mask.sum()})', 
               alpha=0.6, s=30, edgecolors='k', linewidths=0.3)
    
    ax.set_xlabel('t-SNE ç¬¬ä¸€ä¸»æˆåˆ†', fontsize=12)
    ax.set_ylabel('t-SNE ç¬¬äºŒä¸»æˆåˆ†', fontsize=12)
    ax.set_title('ç‰¹å¾åˆ†å¸ƒ: çœŸå®æ ‡ç­¾ (å¹²å‡€æ•°æ®)', 
                fontsize=14, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    
    save_path = os.path.join(save_dir, '1_çœŸå®æ ‡ç­¾åˆ†å¸ƒ.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    logger.info(f"  âœ“ å·²ä¿å­˜: {save_path}")
    
    # 2. Noisy Labels (with noise highlighted)
    logger.info("ç»˜åˆ¶ 2/4: å™ªå£°æ ‡ç­¾åˆ†å¸ƒ...")
    fig, ax = plt.subplots(figsize=(10, 8))
    
    # Plot clean samples first
    clean_samples = ~noise_mask
    benign_clean = clean_samples & (noisy_labels == 0)
    malicious_clean = clean_samples & (noisy_labels == 1)
    
    ax.scatter(features_2d[benign_clean, 0], features_2d[benign_clean, 1],
               c=color_benign, label=f'æ­£å¸¸æµé‡ (å¹²å‡€, n={benign_clean.sum()})', 
               alpha=0.4, s=25, edgecolors='k', linewidths=0.3)
    ax.scatter(features_2d[malicious_clean, 0], features_2d[malicious_clean, 1],
               c=color_malicious, label=f'æ¶æ„æµé‡ (å¹²å‡€, n={malicious_clean.sum()})', 
               alpha=0.4, s=25, edgecolors='k', linewidths=0.3)
    
    # Plot noisy samples with special marker
    noisy_samples = noise_mask
    benign_noisy = noisy_samples & (noisy_labels == 0)
    malicious_noisy = noisy_samples & (noisy_labels == 1)
    
    ax.scatter(features_2d[benign_noisy, 0], features_2d[benign_noisy, 1],
               c=color_benign, label=f'æ­£å¸¸æµé‡ (å™ªå£°, n={benign_noisy.sum()})', 
               alpha=0.9, s=80, edgecolors='yellow', linewidths=2, marker='s')
    ax.scatter(features_2d[malicious_noisy, 0], features_2d[malicious_noisy, 1],
               c=color_malicious, label=f'æ¶æ„æµé‡ (å™ªå£°, n={malicious_noisy.sum()})', 
               alpha=0.9, s=80, edgecolors='yellow', linewidths=2, marker='s')
    
    ax.set_xlabel('t-SNE ç¬¬ä¸€ä¸»æˆåˆ†', fontsize=12)
    ax.set_ylabel('t-SNE ç¬¬äºŒä¸»æˆåˆ†', fontsize=12)
    ax.set_title(f'ç‰¹å¾åˆ†å¸ƒ: å™ªå£°æ ‡ç­¾ (å™ªå£°ç‡: {100*noise_mask.sum()/len(noise_mask):.1f}%)', 
                fontsize=14, fontweight='bold')
    ax.legend(fontsize=9, loc='best')
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    
    save_path = os.path.join(save_dir, '2_å™ªå£°æ ‡ç­¾åˆ†å¸ƒ.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    logger.info(f"  âœ“ å·²ä¿å­˜: {save_path}")
    
    # 3. Corrected Labels (excluding dropped samples)
    logger.info("ç»˜åˆ¶ 3/4: çŸ«æ­£åæ ‡ç­¾åˆ†å¸ƒ...")
    fig, ax = plt.subplots(figsize=(10, 8))
    
    kept_samples = action_mask != 2  # Exclude dropped samples
    
    benign_corrected = kept_samples & (clean_labels == 0)
    malicious_corrected = kept_samples & (clean_labels == 1)
    dropped_samples = action_mask == 2
    
    ax.scatter(features_2d[benign_corrected, 0], features_2d[benign_corrected, 1],
               c=color_benign, label=f'æ­£å¸¸æµé‡ (n={benign_corrected.sum()})', 
               alpha=0.6, s=30, edgecolors='k', linewidths=0.3)
    ax.scatter(features_2d[malicious_corrected, 0], features_2d[malicious_corrected, 1],
               c=color_malicious, label=f'æ¶æ„æµé‡ (n={malicious_corrected.sum()})', 
               alpha=0.6, s=30, edgecolors='k', linewidths=0.3)
    
    # Show dropped samples
    if dropped_samples.sum() > 0:
        ax.scatter(features_2d[dropped_samples, 0], features_2d[dropped_samples, 1],
                   c='gray', label=f'å·²ä¸¢å¼ƒ (n={dropped_samples.sum()})', 
                   alpha=0.5, s=50, marker='x', linewidths=1.5)
    
    ax.set_xlabel('t-SNE ç¬¬ä¸€ä¸»æˆåˆ†', fontsize=12)
    ax.set_ylabel('t-SNE ç¬¬äºŒä¸»æˆåˆ†', fontsize=12)
    ax.set_title('ç‰¹å¾åˆ†å¸ƒ: çŸ«æ­£åæ ‡ç­¾ (Hybrid CourtçŸ«æ­£å)', 
                fontsize=14, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    
    save_path = os.path.join(save_dir, '3_çŸ«æ­£åæ ‡ç­¾åˆ†å¸ƒ.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    logger.info(f"  âœ“ å·²ä¿å­˜: {save_path}")
    
    # 4. Action Visualization (Keep, Flip, Drop, Reweight)
    logger.info("ç»˜åˆ¶ 4/4: çŸ«æ­£åŠ¨ä½œåˆ†å¸ƒ...")
    fig, ax = plt.subplots(figsize=(12, 9))
    
    keep_mask = action_mask == 0
    flip_mask = action_mask == 1
    drop_mask = action_mask == 2
    reweight_mask = action_mask == 3
    
    # Plot in reverse order so important actions are on top
    if reweight_mask.sum() > 0:
        ax.scatter(features_2d[reweight_mask, 0], features_2d[reweight_mask, 1],
                   c='#F77F00', label=f'é‡åŠ æƒ (é•¿å°¾æ ·æœ¬, n={reweight_mask.sum()})', 
                   alpha=0.7, s=60, edgecolors='k', linewidths=0.5, marker='v')
    
    if keep_mask.sum() > 0:
        ax.scatter(features_2d[keep_mask, 0], features_2d[keep_mask, 1],
                   c='#06A77D', label=f'ä¿æŒ (æ ¸å¿ƒæ ·æœ¬, n={keep_mask.sum()})', 
                   alpha=0.5, s=25, edgecolors='k', linewidths=0.3)
    
    if flip_mask.sum() > 0:
        ax.scatter(features_2d[flip_mask, 0], features_2d[flip_mask, 1],
                   c='#D62828', label=f'ç¿»è½¬ (å·²çŸ«æ­£, n={flip_mask.sum()})', 
                   alpha=0.8, s=70, edgecolors='k', linewidths=0.8, marker='*')
    
    if drop_mask.sum() > 0:
        ax.scatter(features_2d[drop_mask, 0], features_2d[drop_mask, 1],
                   c='#6A4C93', label=f'ä¸¢å¼ƒ (ç³»ç»Ÿå™ªå£°, n={drop_mask.sum()})', 
                   alpha=0.8, s=80, edgecolors='k', linewidths=1, marker='X')
    
    ax.set_xlabel('t-SNE ç¬¬ä¸€ä¸»æˆåˆ†', fontsize=12)
    ax.set_ylabel('t-SNE ç¬¬äºŒä¸»æˆåˆ†', fontsize=12)
    ax.set_title('ç‰¹å¾åˆ†å¸ƒ: Hybrid CourtçŸ«æ­£åŠ¨ä½œ', 
                fontsize=14, fontweight='bold')
    ax.legend(fontsize=11, loc='best', framealpha=0.9)
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    
    save_path = os.path.join(save_dir, '4_çŸ«æ­£åŠ¨ä½œåˆ†å¸ƒ.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    logger.info(f"  âœ“ å·²ä¿å­˜: {save_path}")
    
    # 5. Comparison Plot (Before vs After)
    logger.info("ç»˜åˆ¶ 5/5: çŸ«æ­£å‰åå¯¹æ¯”...")
    fig, axes = plt.subplots(1, 2, figsize=(18, 7))
    
    # Before: Noisy Labels
    ax = axes[0]
    benign_noisy_all = noisy_labels == 0
    malicious_noisy_all = noisy_labels == 1
    
    ax.scatter(features_2d[benign_noisy_all, 0], features_2d[benign_noisy_all, 1],
               c=color_benign, label=f'æ­£å¸¸æµé‡ (n={benign_noisy_all.sum()})', 
               alpha=0.5, s=30, edgecolors='k', linewidths=0.3)
    ax.scatter(features_2d[malicious_noisy_all, 0], features_2d[malicious_noisy_all, 1],
               c=color_malicious, label=f'æ¶æ„æµé‡ (n={malicious_noisy_all.sum()})', 
               alpha=0.5, s=30, edgecolors='k', linewidths=0.3)
    
    # Highlight noise
    ax.scatter(features_2d[noise_mask, 0], features_2d[noise_mask, 1],
               facecolors='none', edgecolors='yellow', 
               label=f'å™ªå£°æ ·æœ¬ (n={noise_mask.sum()})', 
               s=100, linewidths=2, marker='o')
    
    ax.set_xlabel('t-SNE ç¬¬ä¸€ä¸»æˆåˆ†', fontsize=12)
    ax.set_ylabel('t-SNE ç¬¬äºŒä¸»æˆåˆ†', fontsize=12)
    ax.set_title(f'çŸ«æ­£å‰: å™ªå£°æ ‡ç­¾\n(å™ªå£°ç‡: {100*noise_mask.sum()/len(noise_mask):.1f}%)', 
                fontsize=13, fontweight='bold')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)
    
    # After: Corrected Labels
    ax = axes[1]
    benign_corrected_all = kept_samples & (clean_labels == 0)
    malicious_corrected_all = kept_samples & (clean_labels == 1)
    
    ax.scatter(features_2d[benign_corrected_all, 0], features_2d[benign_corrected_all, 1],
               c=color_benign, label=f'æ­£å¸¸æµé‡ (n={benign_corrected_all.sum()})', 
               alpha=0.5, s=30, edgecolors='k', linewidths=0.3)
    ax.scatter(features_2d[malicious_corrected_all, 0], features_2d[malicious_corrected_all, 1],
               c=color_malicious, label=f'æ¶æ„æµé‡ (n={malicious_corrected_all.sum()})', 
               alpha=0.5, s=30, edgecolors='k', linewidths=0.3)
    
    # Show corrections
    ax.scatter(features_2d[flip_mask, 0], features_2d[flip_mask, 1],
               facecolors='none', edgecolors='lime', 
               label=f'å·²ç¿»è½¬ (n={flip_mask.sum()})', 
               s=100, linewidths=2, marker='o')
    
    if drop_mask.sum() > 0:
        ax.scatter(features_2d[drop_mask, 0], features_2d[drop_mask, 1],
                   c='gray', label=f'å·²ä¸¢å¼ƒ (n={drop_mask.sum()})', 
                   alpha=0.6, s=50, marker='x', linewidths=1.5)
    
    # Calculate correction accuracy
    correction_accuracy = (clean_labels[kept_samples] == y_true[kept_samples]).mean()
    
    ax.set_xlabel('t-SNE ç¬¬ä¸€ä¸»æˆåˆ†', fontsize=12)
    ax.set_ylabel('t-SNE ç¬¬äºŒä¸»æˆåˆ†', fontsize=12)
    ax.set_title(f'çŸ«æ­£å: Hybrid Court\n(å‡†ç¡®ç‡: {correction_accuracy*100:.2f}%)', 
                fontsize=13, fontweight='bold')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    save_path = os.path.join(save_dir, '5_çŸ«æ­£å‰åå¯¹æ¯”.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    logger.info(f"  âœ“ å·²ä¿å­˜: {save_path}")
    
    logger.info("="*70)
    logger.info("âœ“ æ‰€æœ‰ç‰¹å¾åˆ†å¸ƒå›¾å·²ç”Ÿæˆ")


def main(args):
    """Main function"""
    global logger
    
    # Setup
    set_seed(config.SEED)
    config.create_dirs()
    
    # å™ªå£°ç‡ç™¾åˆ†æ¯”
    noise_pct = int(args.noise_rate * 100)
    
    # Create analysis output directory (åŒ…å«å™ªå£°ç‡)
    analysis_dir = os.path.join(config.LABEL_CORRECTION_DIR, "analysis", f"noise_{noise_pct}pct")
    os.makedirs(analysis_dir, exist_ok=True)
    os.makedirs(os.path.join(analysis_dir, "figures"), exist_ok=True)
    os.makedirs(os.path.join(analysis_dir, "documents"), exist_ok=True)
    
    # åˆ›å»ºå¸¦å™ªå£°ç‡çš„æ—¥å¿—æ–‡ä»¶
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_filename = f"noise_{noise_pct}pct_analysis_{timestamp}.log"
    log_dir = os.path.join(config.OUTPUT_ROOT, "logs")
    os.makedirs(log_dir, exist_ok=True)
    log_path = os.path.join(log_dir, log_filename)
    
    # åˆ›å»ºå…±äº«çš„handlers
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%H:%M:%S')
    
    fh = logging.FileHandler(log_path, encoding='utf-8')
    fh.setLevel(logging.INFO)
    fh.setFormatter(formatter)
    
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch.setFormatter(formatter)
    
    # é…ç½®root logger (æ•è·æ‰€æœ‰æ¨¡å—çš„æ—¥å¿—ï¼ŒåŒ…æ‹¬HybridCourt)
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    root_logger.handlers = []  # æ¸…é™¤å·²æœ‰handlers
    root_logger.addHandler(fh)
    root_logger.addHandler(ch)
    
    # è·å–æœ¬æ¨¡å—çš„logger (ä½¿ç”¨root loggerï¼Œä¸éœ€è¦å•ç‹¬é…ç½®)
    logger = logging.getLogger('label_correction_analysis')
    logger.setLevel(logging.INFO)
    logger.handlers = []  # æ¸…é™¤handlersï¼Œä½¿ç”¨rootçš„
    logger.propagate = True  # ä¼ æ’­åˆ°root logger
    
    logger.info("="*70)
    logger.info(f"MEDAL-Lite æ ‡ç­¾çŸ«æ­£åˆ†æ - å™ªå£°ç‡ {noise_pct}%")
    logger.info("="*70)
    logger.info(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"è®¾å¤‡: {config.DEVICE}")
    logger.info(f"å™ªå£°ç‡: {noise_pct}%")
    logger.info("")
    
    # Override config with arguments
    if args.noise_rate is not None:
        config.LABEL_NOISE_RATE = args.noise_rate
    
    # ========================
    # 1. Load Dataset
    # ========================
    logger.info("="*70)
    logger.info("Step 1: åŠ è½½æ•°æ®é›†")
    logger.info("="*70)
    logger.info(f"æ­£å¸¸è®­ç»ƒæ•°æ®:    {config.BENIGN_TRAIN}")
    logger.info(f"æ¶æ„è®­ç»ƒæ•°æ®:    {config.MALICIOUS_TRAIN}")
    logger.info(f"åºåˆ—é•¿åº¦:        {config.SEQUENCE_LENGTH}")
    logger.info("")
    
    # ä¼˜å…ˆä½¿ç”¨é¢„å¤„ç†å¥½çš„æ•°æ®
    if check_preprocessed_exists('train'):
        logger.info("âœ“ å‘ç°é¢„å¤„ç†æ–‡ä»¶ï¼Œç›´æ¥åŠ è½½...")
        X_train, y_train_clean, train_files = load_preprocessed('train')
        logger.info(f"  ä»é¢„å¤„ç†æ–‡ä»¶åŠ è½½: {X_train.shape[0]} ä¸ªæ ·æœ¬")
    else:
        # ä»PCAPæ–‡ä»¶åŠ è½½
        logger.info("å¼€å§‹åŠ è½½è®­ç»ƒæ•°æ®é›†ï¼ˆä»PCAPæ–‡ä»¶ï¼‰...")
        logger.info("ğŸ’¡ æç¤º: è¿è¡Œ 'python scripts/utils/preprocess.py --train_only' å¯é¢„å¤„ç†è®­ç»ƒé›†ï¼ŒåŠ é€Ÿåç»­åˆ†æ")
        X_train, y_train_clean, train_files = load_dataset(
            benign_dir=config.BENIGN_TRAIN,
            malicious_dir=config.MALICIOUS_TRAIN,
            sequence_length=config.SEQUENCE_LENGTH
        )
    
    if X_train is None:
        logger.error("âŒ æ•°æ®é›†åŠ è½½å¤±è´¥!")
        return
    
    logger.info("âœ“ æ•°æ®é›†åŠ è½½å®Œæˆ")
    logger.info(f"  æ•°æ®å½¢çŠ¶:     {X_train.shape}")
    logger.info(f"  æ­£å¸¸æ ·æœ¬:     {(y_train_clean==0).sum()}")
    logger.info(f"  æ¶æ„æ ·æœ¬:     {(y_train_clean==1).sum()}")
    logger.info("")
    
    # ========================
    # 2. Inject Label Noise
    # ========================
    logger.info("="*70)
    logger.info("Step 2: æ³¨å…¥æ ‡ç­¾å™ªå£°")
    logger.info("="*70)
    logger.info(f"å™ªå£°ç‡: {config.LABEL_NOISE_RATE*100:.0f}%")
    
    # å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿ç›¸åŒå™ªå£°ç‡çš„ç»“æœå¯å¤ç°
    set_seed(config.SEED)
    y_train_noisy, noise_mask = inject_label_noise(y_train_clean, config.LABEL_NOISE_RATE)
    
    logger.info(f"âœ“ å™ªå£°æ³¨å…¥å®Œæˆ: {noise_mask.sum()} ä¸ªæ ‡ç­¾è¢«ç¿»è½¬")
    logger.info(f"  åŸå§‹: æ­£å¸¸={(y_train_clean==0).sum()}, æ¶æ„={(y_train_clean==1).sum()}")
    logger.info(f"  å™ªå£°å: æ­£å¸¸={(y_train_noisy==0).sum()}, æ¶æ„={(y_train_noisy==1).sum()}")
    logger.info("")
    
    # ========================
    # 3. Extract Features
    # ========================
    logger.info("="*70)
    logger.info("Step 3: æå–ç‰¹å¾")
    logger.info("="*70)
    
    backbone = MicroBiMambaBackbone(config)
    
    # ç¡®å®šbackboneè·¯å¾„ï¼šä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
    if args.backbone_path:
        backbone_path = args.backbone_path
    else:
        backbone_path = os.path.join(config.FEATURE_EXTRACTION_DIR, "models", "backbone_pretrained.pth")
    
    # Try to load pre-trained backbone
    if os.path.exists(backbone_path) and not args.retrain_backbone:
        logger.info(f"åŠ è½½é¢„è®­ç»ƒbackbone: {backbone_path}")
        state = torch.load(backbone_path, map_location=config.DEVICE)
        try:
            backbone.load_state_dict(state)
        except RuntimeError as e:
            logger.warning(f"âš  éª¨å¹²ç½‘ç»œæ£€æŸ¥ç‚¹ä¸å½“å‰ç»“æ„ä¸å®Œå…¨åŒ¹é…ï¼Œå°†ä½¿ç”¨ strict=False åŠ è½½: {e}")
            missing, unexpected = backbone.load_state_dict(state, strict=False)
            if missing:
                logger.warning(f"  missing_keys: {missing}")
            if unexpected:
                logger.warning(f"  unexpected_keys: {unexpected}")
        logger.info("âœ“ BackboneåŠ è½½å®Œæˆ")
    else:
        if args.retrain_backbone:
            logger.warning("âš  æŒ‡å®šäº†--retrain_backboneï¼Œå°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„backbone")
        else:
            logger.warning(f"âš  æœªæ‰¾åˆ°é¢„è®­ç»ƒbackbone: {backbone_path}")
            logger.warning("  å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„backbone")
    
    features = extract_features_with_backbone(backbone, X_train, config, logger)
    
    # Save features
    features_path = os.path.join(analysis_dir, "extracted_features.npy")
    np.save(features_path, features)
    logger.info(f"âœ“ ç‰¹å¾å·²ä¿å­˜: {features_path}")
    logger.info("")
    
    # ========================
    # 4. Run Hybrid Court Label Correction
    # ========================
    logger.info("="*70)
    logger.info("Step 4: è¿è¡Œ Hybrid Court æ ‡ç­¾çŸ«æ­£")
    logger.info("="*70)
    
    results = run_hybrid_court_with_detailed_tracking(features, y_train_noisy, config, logger, y_true=y_train_clean)
    
    # Save results
    results_path = os.path.join(analysis_dir, "correction_results.npz")
    np.savez(results_path,
             y_clean=y_train_clean,
             y_noisy=y_train_noisy,
             y_corrected=results['clean_labels'],
             action_mask=results['action_mask'],
             confidence=results['confidence'],
             correction_weight=results['correction_weight'],
             noise_mask=noise_mask,
             features=features,
             cl_suspected_noise=results['cl_suspected_noise'],
             cl_pred_labels=results['cl_pred_labels'],
             cl_pred_probs=results['cl_pred_probs'],
             made_is_dense=results['made_is_dense'],
             made_density_scores=results['made_density_scores'],
             knn_neighbor_labels=results['knn_neighbor_labels'],
             knn_neighbor_consistency=results['knn_neighbor_consistency'],
             tier_info=np.array(results.get('tier_info', []), dtype=object))
    logger.info(f"âœ“ çŸ«æ­£ç»“æœå·²ä¿å­˜: {results_path}")
    logger.info("")
    
    # ========================
    # 5. Generate Analysis Document
    # ========================
    logger.info("="*70)
    logger.info("Step 5: ç”Ÿæˆæ ·æœ¬åˆ†ææ–‡æ¡£")
    logger.info("="*70)
    
    doc_path = os.path.join(analysis_dir, "documents", "sample_analysis.log")
    df_analysis = generate_sample_analysis_document(
        results, y_train_clean, noise_mask, doc_path, logger
    )
    logger.info("")
    
    # ========================
    # 6. Generate Feature Distribution Plots
    # ========================
    logger.info("="*70)
    logger.info("Step 6: ç”Ÿæˆç‰¹å¾åˆ†å¸ƒå›¾")
    logger.info("="*70)
    
    plot_dir = os.path.join(analysis_dir, "figures")
    plot_feature_distributions(results, y_train_clean, noise_mask, plot_dir, logger)
    logger.info("")
    
    # ========================
    # Summary
    # ========================
    logger.info("="*70)
    logger.info(f"ğŸ‰ å™ªå£°ç‡ {noise_pct}% æ ‡ç­¾çŸ«æ­£åˆ†æå®Œæˆ!")
    logger.info("="*70)
    logger.info("")
    logger.info("ğŸ“ è¾“å‡ºæ–‡ä»¶:")
    logger.info(f"  åˆ†æç›®å½•:       {analysis_dir}")
    logger.info(f"  çŸ«æ­£ç»“æœ:       {results_path}")
    logger.info(f"  æ ·æœ¬åˆ†æCSV:    {doc_path.replace('.log', '.csv')}")
    logger.info(f"  æ ·æœ¬åˆ†æLOG:    {doc_path}")
    logger.info(f"  ç‰¹å¾åˆ†å¸ƒå›¾:     {plot_dir}")
    logger.info(f"  æ—¥å¿—æ–‡ä»¶:       {os.path.join(log_dir, log_filename)}")
    logger.info("")
    logger.info("ğŸ“Š ç»Ÿè®¡æ‘˜è¦:")
    n_samples = len(y_train_clean)
    action_mask = results['action_mask']
    logger.info(f"  æ€»æ ·æœ¬æ•°:           {n_samples}")
    logger.info(f"  å™ªå£°æ³¨å…¥:           {noise_mask.sum()} ({100*noise_mask.sum()/n_samples:.1f}%)")
    logger.info(f"  Keep (core):        {(action_mask==0).sum()} ({100*(action_mask==0).sum()/n_samples:.1f}%)")
    logger.info(f"  Flip (corrected):   {(action_mask==1).sum()} ({100*(action_mask==1).sum()/n_samples:.1f}%)")
    logger.info(f"  Drop (noise):       {(action_mask==2).sum()} ({100*(action_mask==2).sum()/n_samples:.1f}%)")
    logger.info(f"  Reweight (tail):    {(action_mask==3).sum()} ({100*(action_mask==3).sum()/n_samples:.1f}%)")
    
    # Correction accuracy
    keep_mask = action_mask != 2
    correction_accuracy = (results['clean_labels'][keep_mask] == y_train_clean[keep_mask]).mean()
    logger.info(f"  çŸ«æ­£å‡†ç¡®ç‡:         {correction_accuracy*100:.2f}%")
    logger.info("")
    logger.info("="*70)
    
    # æ¸…ç†root loggerçš„handlers
    for handler in root_logger.handlers[:]:
        handler.close()
        root_logger.removeHandler(handler)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Label Correction Analysis for MEDAL-Lite"
    )
    parser.add_argument(
        "--noise_rate", 
        type=float, 
        default=0.30, 
        help="Label noise rate (default: 0.30)"
    )
    parser.add_argument(
        "--retrain_backbone",
        action='store_true',
        help="Use randomly initialized backbone instead of pre-trained"
    )
    parser.add_argument(
        "--backbone_path",
        type=str,
        default='',
        help="Path to backbone model (optional, default: backbone_pretrained.pth)"
    )
    
    args = parser.parse_args()
    
    main(args)

