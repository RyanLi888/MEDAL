"""
ç‰¹å¾æå–åˆ†æè„šæœ¬
åˆ†æéª¨å¹²ç½‘ç»œæå–çš„ç‰¹å¾è´¨é‡
"""
import sys
import os
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
sys.path.insert(0, project_root)

import torch
import numpy as np
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, f1_score, silhouette_score, classification_report
from sklearn.decomposition import PCA

from MoudleCode.utils.config import config
from MoudleCode.utils.helpers import set_seed, setup_logger
from MoudleCode.utils.visualization import plot_feature_space
from MoudleCode.preprocessing.pcap_parser import load_dataset
from MoudleCode.feature_extraction.backbone import MicroBiMambaBackbone

try:
    from scripts.utils.preprocess import check_preprocessed_exists, load_preprocessed
    PREPROCESS_AVAILABLE = True
except ImportError:
    PREPROCESS_AVAILABLE = False

def main():
    import argparse
    parser = argparse.ArgumentParser(description="ç‰¹å¾æå–åˆ†æ")
    parser.add_argument("--backbone_path", type=str, default=None, help="éª¨å¹²ç½‘ç»œè·¯å¾„")
    parser.add_argument("--train_backbone", action="store_true", help="æ˜¯å¦è®­ç»ƒæ–°çš„éª¨å¹²ç½‘ç»œ")
    args = parser.parse_args()
    
    # Setup
    set_seed(config.SEED)
    config.create_dirs()
    
    # åˆ›å»ºåˆ†æè¾“å‡ºç›®å½•
    analysis_dir = os.path.join(config.OUTPUT_ROOT, "feature_analysis")
    os.makedirs(analysis_dir, exist_ok=True)
    os.makedirs(os.path.join(analysis_dir, "figures"), exist_ok=True)
    os.makedirs(os.path.join(analysis_dir, "reports"), exist_ok=True)
    
    logger = setup_logger(os.path.join(config.OUTPUT_ROOT, "logs"), name='feature_analysis')
    
    logger.info("="*70)
    logger.info("ğŸ”¬ ç‰¹å¾æå–åˆ†ææ¨¡å¼")
    logger.info("="*70)
    logger.info(f"æ—¶é—´æˆ³: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("")
    
    # Load dataset
    logger.info("ğŸ“¦ åŠ è½½è®­ç»ƒæ•°æ®é›†...")
    if PREPROCESS_AVAILABLE and check_preprocessed_exists('train'):
        X_train, y_train, _ = load_preprocessed('train')
        logger.info(f"  ä»é¢„å¤„ç†æ–‡ä»¶åŠ è½½: {X_train.shape[0]} ä¸ªæ ·æœ¬")
    else:
        X_train, y_train, _ = load_dataset(
            benign_dir=config.BENIGN_TRAIN,
            malicious_dir=config.MALICIOUS_TRAIN,
            sequence_length=config.SEQUENCE_LENGTH
        )
    
    logger.info(f"âœ“ æ•°æ®åŠ è½½å®Œæˆ: {X_train.shape}")
    logger.info(f"  æ­£å¸¸æ ·æœ¬: {(y_train==0).sum()}, æ¶æ„æ ·æœ¬: {(y_train==1).sum()}")
    logger.info("")
    
    # Load or train backbone
    backbone = MicroBiMambaBackbone(config)
    
    if args.train_backbone or args.backbone_path is None:
        logger.info("ğŸ”§ è®­ç»ƒæ–°çš„éª¨å¹²ç½‘ç»œ...")
        from torch.utils.data import TensorDataset, DataLoader
        import torch.optim as optim
        from MoudleCode.feature_extraction.backbone import SimMTMLoss
        
        backbone.train()
        backbone.to(config.DEVICE)
        
        dataset = TensorDataset(torch.FloatTensor(X_train))
        train_loader = DataLoader(dataset, batch_size=config.PRETRAIN_BATCH_SIZE, shuffle=True)
        
        simmtm_loss_fn = SimMTMLoss(mask_rate=config.SIMMTM_MASK_RATE)
        optimizer = optim.AdamW(backbone.parameters(), lr=config.PRETRAIN_LR)
        
        for epoch in range(config.PRETRAIN_EPOCHS):
            epoch_loss = 0.0
            for batch_data in train_loader:
                if isinstance(batch_data, (list, tuple)):
                    X_batch = batch_data[0]
                else:
                    X_batch = batch_data
                X_batch = X_batch.to(config.DEVICE)
                
                optimizer.zero_grad()
                loss = simmtm_loss_fn(backbone, X_batch)
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
            
            if (epoch + 1) % 20 == 0:
                logger.info(f"  Epoch [{epoch+1}/{config.PRETRAIN_EPOCHS}] Loss: {epoch_loss/len(train_loader):.4f}")
        
        # Save backbone
        backbone_path = os.path.join(config.FEATURE_EXTRACTION_DIR, "models", f"backbone_analysis_{len(X_train)}.pth")
        torch.save(backbone.state_dict(), backbone_path)
        logger.info(f"âœ“ éª¨å¹²ç½‘ç»œè®­ç»ƒå®Œæˆ: {backbone_path}")
        logger.info("")
    else:
        backbone_path = args.backbone_path
        logger.info(f"ğŸ“¥ åŠ è½½éª¨å¹²ç½‘ç»œ: {backbone_path}")
        state = torch.load(backbone_path, map_location=config.DEVICE)
        try:
            backbone.load_state_dict(state)
        except RuntimeError as e:
            logger.warning(f"âš  éª¨å¹²ç½‘ç»œæ£€æŸ¥ç‚¹ä¸å½“å‰ç»“æ„ä¸å®Œå…¨åŒ¹é…ï¼Œå°†ä½¿ç”¨ strict=False åŠ è½½: {e}")
            missing, unexpected = backbone.load_state_dict(state, strict=False)
            if missing:
                logger.warning(f"  missing_keys: {missing}")
            if unexpected:
                logger.warning(f"  unexpected_keys: {unexpected}")
        logger.info("âœ“ éª¨å¹²ç½‘ç»œåŠ è½½å®Œæˆ")
        logger.info("")
    
    # Extract features
    logger.info("ğŸ” æå–ç‰¹å¾...")
    backbone.freeze()
    backbone.eval()
    backbone.to(config.DEVICE)
    
    features_list = []
    with torch.no_grad():
        X_tensor = torch.FloatTensor(X_train).to(config.DEVICE)
        batch_size = 64
        for i in range(0, len(X_tensor), batch_size):
            X_batch = X_tensor[i:i+batch_size]
            z_batch = backbone(X_batch, return_sequence=False)
            features_list.append(z_batch.cpu().numpy())
    
    features = np.concatenate(features_list, axis=0)
    logger.info(f"âœ“ ç‰¹å¾æå–å®Œæˆ: {features.shape}")
    logger.info("")
    
    # Save features
    features_path = os.path.join(analysis_dir, "extracted_features.npy")
    np.save(features_path, features)
    logger.info(f"ğŸ’¾ ç‰¹å¾å·²ä¿å­˜: {features_path}")
    logger.info("")
    
    # ========================
    # ç‰¹å¾è´¨é‡åˆ†æ
    # ========================
    logger.info("="*70)
    logger.info("ğŸ“Š ç‰¹å¾è´¨é‡åˆ†æ")
    logger.info("="*70)
    logger.info("")
    
    # 1. ç‰¹å¾å¯åˆ†æ€§è¯„ä¼°
    logger.info("1ï¸âƒ£  ç‰¹å¾å¯åˆ†æ€§è¯„ä¼° (Logistic Regression)")
    X_tr, X_te, y_tr, y_te = train_test_split(
        features, y_train, test_size=0.2, stratify=y_train, random_state=config.SEED
    )
    
    clf = LogisticRegression(max_iter=1000, class_weight='balanced')
    clf.fit(X_tr, y_tr)
    
    te_proba = clf.predict_proba(X_te)[:, 1]
    te_pred = (te_proba >= 0.5).astype(int)
    
    te_auc = roc_auc_score(y_te, te_proba)
    te_f1 = f1_score(y_te, te_pred, pos_label=1)
    
    logger.info(f"  ROC-AUC: {te_auc:.4f}")
    logger.info(f"  F1-Score: {te_f1:.4f}")
    logger.info("")
    
    # 2. Silhouette Score
    logger.info("2ï¸âƒ£  èšç±»è´¨é‡è¯„ä¼° (Silhouette Score)")
    if len(np.unique(y_train)) > 1:
        sil_score = silhouette_score(features, y_train)
        logger.info(f"  Silhouette Score: {sil_score:.4f}")
        if sil_score > 0.5:
            logger.info("  âœ… ä¼˜ç§€ - ç‰¹å¾èšç±»è´¨é‡å¾ˆå¥½")
        elif sil_score > 0.3:
            logger.info("  âœ… è‰¯å¥½ - ç‰¹å¾èšç±»è´¨é‡è¾ƒå¥½")
        else:
            logger.info("  âš ï¸  ä¸€èˆ¬ - ç‰¹å¾èšç±»è´¨é‡æœ‰å¾…æå‡")
    logger.info("")
    
    # 3. PCAæ–¹å·®è§£é‡Š
    logger.info("3ï¸âƒ£  ä¸»æˆåˆ†åˆ†æ (PCA)")
    pca = PCA(n_components=min(50, features.shape[1]))
    pca.fit(features)
    
    cumsum_var = np.cumsum(pca.explained_variance_ratio_)
    n_95 = np.argmax(cumsum_var >= 0.95) + 1
    n_99 = np.argmax(cumsum_var >= 0.99) + 1
    
    logger.info(f"  å‰10ä¸ªä¸»æˆåˆ†è§£é‡Šæ–¹å·®: {cumsum_var[9]:.4f}")
    logger.info(f"  è¾¾åˆ°95%æ–¹å·®éœ€è¦: {n_95} ä¸ªä¸»æˆåˆ†")
    logger.info(f"  è¾¾åˆ°99%æ–¹å·®éœ€è¦: {n_99} ä¸ªä¸»æˆåˆ†")
    logger.info("")
    
    # 4. ç‰¹å¾ç»Ÿè®¡
    logger.info("4ï¸âƒ£  ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯")
    logger.info(f"  ç‰¹å¾ç»´åº¦: {features.shape[1]}")
    logger.info(f"  ç‰¹å¾å‡å€¼: {features.mean():.4f}")
    logger.info(f"  ç‰¹å¾æ ‡å‡†å·®: {features.std():.4f}")
    logger.info(f"  ç‰¹å¾æœ€å°å€¼: {features.min():.4f}")
    logger.info(f"  ç‰¹å¾æœ€å¤§å€¼: {features.max():.4f}")
    logger.info("")
    
    # ========================
    # ç”Ÿæˆå¯è§†åŒ–
    # ========================
    logger.info("="*70)
    logger.info("ğŸ“ˆ ç”Ÿæˆç‰¹å¾åˆ†å¸ƒå¯è§†åŒ–")
    logger.info("="*70)
    logger.info("")
    
    # t-SNE
    logger.info("ç”Ÿæˆ t-SNE å¯è§†åŒ–...")
    tsne_path = os.path.join(analysis_dir, "figures", "feature_distribution_tsne.png")
    plot_feature_space(features, y_train, tsne_path, 
                      title="Feature Distribution (t-SNE)", method='tsne')
    logger.info(f"  âœ“ t-SNEå›¾: {tsne_path}")
    
    # PCA
    logger.info("ç”Ÿæˆ PCA å¯è§†åŒ–...")
    pca_path = os.path.join(analysis_dir, "figures", "feature_distribution_pca.png")
    plot_feature_space(features, y_train, pca_path,
                      title="Feature Distribution (PCA)", method='pca')
    logger.info(f"  âœ“ PCAå›¾: {pca_path}")
    logger.info("")
    
    # ========================
    # ç”Ÿæˆåˆ†ææŠ¥å‘Š
    # ========================
    logger.info("="*70)
    logger.info("ğŸ“ ç”Ÿæˆåˆ†ææŠ¥å‘Š")
    logger.info("="*70)
    logger.info("")
    
    report_path = os.path.join(analysis_dir, "reports", "feature_analysis_report.md")
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# ç‰¹å¾æå–åˆ†ææŠ¥å‘Š\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"**éª¨å¹²ç½‘ç»œ**: {os.path.basename(backbone_path)}\n\n")
        f.write("---\n\n")
        
        f.write("## 1. æ•°æ®é›†ä¿¡æ¯\n\n")
        f.write(f"- **æ ·æœ¬æ•°**: {len(X_train)}\n")
        f.write(f"- **æ­£å¸¸æ ·æœ¬**: {(y_train==0).sum()} ({(y_train==0).sum()/len(y_train)*100:.2f}%)\n")
        f.write(f"- **æ¶æ„æ ·æœ¬**: {(y_train==1).sum()} ({(y_train==1).sum()/len(y_train)*100:.2f}%)\n")
        f.write(f"- **åºåˆ—é•¿åº¦**: {X_train.shape[1]}\n")
        f.write(f"- **ç‰¹å¾ç»´åº¦**: {X_train.shape[2]}\n\n")
        
        f.write("---\n\n")
        f.write("## 2. æå–ç‰¹å¾ä¿¡æ¯\n\n")
        f.write(f"- **ç‰¹å¾ç»´åº¦**: {features.shape[1]}\n")
        f.write(f"- **ç‰¹å¾å‡å€¼**: {features.mean():.4f}\n")
        f.write(f"- **ç‰¹å¾æ ‡å‡†å·®**: {features.std():.4f}\n")
        f.write(f"- **ç‰¹å¾èŒƒå›´**: [{features.min():.4f}, {features.max():.4f}]\n\n")
        
        f.write("---\n\n")
        f.write("## 3. ç‰¹å¾å¯åˆ†æ€§è¯„ä¼°\n\n")
        f.write("### Logistic Regression æ€§èƒ½\n\n")
        f.write(f"- **ROC-AUC**: {te_auc:.4f}\n")
        f.write(f"- **F1-Score**: {te_f1:.4f}\n\n")
        
        if te_auc >= 0.9 and te_f1 >= 0.8:
            f.write("âœ… **ä¼˜ç§€** - ç‰¹å¾å…·æœ‰å¾ˆå¼ºçš„åˆ¤åˆ«èƒ½åŠ›\n\n")
        elif te_auc >= 0.8 and te_f1 >= 0.7:
            f.write("âœ… **è‰¯å¥½** - ç‰¹å¾å…·æœ‰è¾ƒå¥½çš„åˆ¤åˆ«èƒ½åŠ›\n\n")
        else:
            f.write("âš ï¸ **ä¸€èˆ¬** - ç‰¹å¾åˆ¤åˆ«èƒ½åŠ›æœ‰å¾…æå‡\n\n")
        
        f.write("### Silhouette Score\n\n")
        if len(np.unique(y_train)) > 1:
            f.write(f"- **Silhouette Score**: {sil_score:.4f}\n\n")
            if sil_score > 0.5:
                f.write("âœ… **ä¼˜ç§€** - ç±»å†…ç´§å¯†ï¼Œç±»é—´åˆ†ç¦»è‰¯å¥½\n\n")
            elif sil_score > 0.3:
                f.write("âœ… **è‰¯å¥½** - ç±»åˆ«åˆ†ç¦»è¾ƒä¸ºæ˜æ˜¾\n\n")
            else:
                f.write("âš ï¸ **ä¸€èˆ¬** - ç±»åˆ«åˆ†ç¦»ä¸å¤Ÿæ˜æ˜¾\n\n")
        
        f.write("---\n\n")
        f.write("## 4. ä¸»æˆåˆ†åˆ†æ (PCA)\n\n")
        f.write(f"- **å‰10ä¸ªä¸»æˆåˆ†è§£é‡Šæ–¹å·®**: {cumsum_var[9]*100:.2f}%\n")
        f.write(f"- **è¾¾åˆ°95%æ–¹å·®éœ€è¦**: {n_95} ä¸ªä¸»æˆåˆ†\n")
        f.write(f"- **è¾¾åˆ°99%æ–¹å·®éœ€è¦**: {n_99} ä¸ªä¸»æˆåˆ†\n\n")
        
        if n_95 < features.shape[1] * 0.2:
            f.write("âœ… **ä¿¡æ¯é›†ä¸­åº¦é«˜** - å°‘é‡ä¸»æˆåˆ†å³å¯è¡¨ç¤ºå¤§éƒ¨åˆ†ä¿¡æ¯\n\n")
        else:
            f.write("âš ï¸ **ä¿¡æ¯è¾ƒåˆ†æ•£** - éœ€è¦è¾ƒå¤šä¸»æˆåˆ†æ‰èƒ½ä¿ç•™è¶³å¤Ÿä¿¡æ¯\n\n")
        
        f.write("---\n\n")
        f.write("## 5. å¯è§†åŒ–ç»“æœ\n\n")
        f.write(f"- **t-SNEå›¾**: `{tsne_path}`\n")
        f.write(f"- **PCAå›¾**: `{pca_path}`\n\n")
        
        f.write("---\n\n")
        f.write("## 6. å»ºè®®\n\n")
        
        if te_auc >= 0.9 and sil_score > 0.5:
            f.write("### âœ… ç‰¹å¾è´¨é‡ä¼˜ç§€\n\n")
            f.write("- éª¨å¹²ç½‘ç»œå­¦åˆ°äº†é«˜è´¨é‡çš„ç‰¹å¾è¡¨ç¤º\n")
            f.write("- å¯ä»¥ç›´æ¥ç”¨äºä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡\n")
            f.write("- å»ºè®®ä¿å­˜æ­¤éª¨å¹²ç½‘ç»œç”¨äºåç»­å®éªŒ\n\n")
        elif te_auc >= 0.8:
            f.write("### âœ… ç‰¹å¾è´¨é‡è‰¯å¥½\n\n")
            f.write("- éª¨å¹²ç½‘ç»œå­¦åˆ°äº†è¾ƒå¥½çš„ç‰¹å¾è¡¨ç¤º\n")
            f.write("- å¯ä»¥ç”¨äºåˆ†ç±»ä»»åŠ¡ï¼Œä½†ä»æœ‰æå‡ç©ºé—´\n")
            f.write("- å»ºè®®å°è¯•ï¼š\n")
            f.write("  - å¢åŠ é¢„è®­ç»ƒè½®æ•°\n")
            f.write("  - è°ƒæ•´æ©ç ç‡\n")
            f.write("  - å°è¯•å®ä¾‹å¯¹æ¯”å­¦ä¹  (InfoNCE)\n\n")
        else:
            f.write("### âš ï¸ ç‰¹å¾è´¨é‡éœ€è¦æ”¹è¿›\n\n")
            f.write("- éª¨å¹²ç½‘ç»œå­¦åˆ°çš„ç‰¹å¾åˆ¤åˆ«èƒ½åŠ›ä¸è¶³\n")
            f.write("- å»ºè®®ï¼š\n")
            f.write("  - æ£€æŸ¥æ•°æ®è´¨é‡\n")
            f.write("  - å¢åŠ é¢„è®­ç»ƒè½®æ•°\n")
            f.write("  - è°ƒæ•´ç½‘ç»œç»“æ„\n")
            f.write("  - å°è¯•ä¸åŒçš„é¢„è®­ç»ƒæ–¹æ³•\n\n")
        
        f.write("---\n\n")
        f.write("*æŠ¥å‘Šç”±MEDAL-Liteè‡ªåŠ¨ç”Ÿæˆ*\n")
    
    logger.info(f"âœ“ åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    logger.info("")
    
    logger.info("="*70)
    logger.info("ğŸ‰ ç‰¹å¾åˆ†æå®Œæˆ!")
    logger.info("="*70)
    logger.info("")
    logger.info("ğŸ“ è¾“å‡ºæ–‡ä»¶:")
    logger.info(f"  - ç‰¹å¾æ–‡ä»¶: {features_path}")
    logger.info(f"  - t-SNEå›¾: {tsne_path}")
    logger.info(f"  - PCAå›¾: {pca_path}")
    logger.info(f"  - åˆ†ææŠ¥å‘Š: {report_path}")
    logger.info("")

if __name__ == "__main__":
    main()
