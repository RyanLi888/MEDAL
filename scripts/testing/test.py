"""
MEDAL-Lite Testing Script
Evaluate trained model on test dataset
"""
import sys
import os
from pathlib import Path

# Ensure project root is on sys.path when running as a script
project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

import torch
from torch.utils.data import TensorDataset, DataLoader
import numpy as np
import random
import hashlib
import argparse
from datetime import datetime
import json
import csv

from sklearn.metrics import precision_score, recall_score, f1_score

from MoudleCode.utils.config import config
from MoudleCode.utils.helpers import (
    set_seed, setup_logger, calculate_metrics, print_metrics, find_optimal_threshold
)
from MoudleCode.utils.visualization import (
    plot_feature_space, plot_confusion_matrix, plot_roc_curve, plot_probability_distribution
)
from MoudleCode.preprocessing.pcap_parser import load_dataset
from MoudleCode.feature_extraction.backbone import MicroBiMambaBackbone, build_backbone
from MoudleCode.classification.dual_stream import MEDAL_Classifier
from MoudleCode.utils.checkpoint import load_state_dict_shape_safe

# å¯¼å…¥é¢„å¤„ç†æ¨¡å—
try:
    from scripts.utils.preprocess import check_preprocessed_exists, load_preprocessed, normalize_burstsize_inplace
    PREPROCESS_AVAILABLE = True
except ImportError:
    PREPROCESS_AVAILABLE = False


def _rng_fingerprint_short() -> str:
    h = hashlib.sha256()
    try:
        h.update(repr(random.getstate()).encode('utf-8'))
    except Exception:
        h.update(b'py_random_error')
    try:
        ns = np.random.get_state()
        h.update(str(ns[0]).encode('utf-8'))
        h.update(np.asarray(ns[1], dtype=np.uint32).tobytes())
        h.update(str(ns[2]).encode('utf-8'))
        h.update(str(ns[3]).encode('utf-8'))
        h.update(str(ns[4]).encode('utf-8'))
    except Exception:
        h.update(b'numpy_random_error')
    try:
        h.update(torch.get_rng_state().detach().cpu().numpy().tobytes())
    except Exception:
        h.update(b'torch_cpu_rng_error')
    try:
        if torch.cuda.is_available():
            for s in torch.cuda.get_rng_state_all():
                h.update(s.detach().cpu().numpy().tobytes())
        else:
            h.update(b'no_cuda')
    except Exception:
        h.update(b'torch_cuda_rng_error')
    return h.hexdigest()[:16]


def _seed_snapshot() -> str:
    torch_seed = None
    try:
        torch_seed = int(torch.initial_seed())
    except Exception:
        torch_seed = None
    return (
        f"config.SEED={int(getattr(config, 'SEED', -1))} | "
        f"torch.initial_seed={torch_seed}"
    )


def _load_classifier_checkpoint(classifier, state_dict, logger=None):
    """Load classifier checkpoint saved either as full classifier.state_dict or as classifier.dual_mlp.state_dict."""
    if not isinstance(state_dict, dict):
        raise TypeError(f"Expected state_dict to be a dict, got {type(state_dict)}")

    try:
        classifier.load_state_dict(state_dict, strict=True)
        return
    except Exception as e:
        if logger is not None:
            logger.warning(f"âš  æ•´æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†å°è¯•ä»…åŠ è½½åˆ†ç±»å¤´(dual_mlp): {e}")

    keys = list(state_dict.keys())
    if keys and all(isinstance(k, str) and k.startswith('dual_mlp.') for k in keys):
        stripped = {k[len('dual_mlp.'):]: v for k, v in state_dict.items()}
        classifier.dual_mlp.load_state_dict(stripped, strict=True)
        return

    classifier.dual_mlp.load_state_dict(state_dict, strict=True)

def test_model(classifier, X_test, y_test, config, logger, save_prefix="test"):
    """
    Test the model on test dataset
    
    Args:
        classifier: Trained MEDAL classifier
        X_test: (N, L, D) test sequences
        y_test: (N,) test labels
        config: configuration object
        logger: logger
        save_prefix: prefix for saved files
        
    Returns:
        metrics: dictionary of evaluation metrics
    """
    logger.info("="*70)
    logger.info("ğŸ” æ¨¡å‹æµ‹è¯• Model Testing")
    logger.info("="*70)
    logger.info(f"æµ‹è¯•æ ·æœ¬æ•°: {len(X_test)}")
    test_batch_size = int(getattr(config, 'TEST_BATCH_SIZE', 256))
    logger.info(f"æ‰¹æ¬¡å¤§å°: {test_batch_size}")
    
    # è®°å½•é…ç½®ä¸­çš„å‚è€ƒé˜ˆå€¼ï¼ˆç”¨äºå¯¹æ¯”ä¸å¯è§†åŒ–ï¼‰
    config_threshold = getattr(config, 'MALICIOUS_THRESHOLD', 0.5)
    logger.info(f"âœ“ é…ç½®å‚è€ƒé˜ˆå€¼: {config_threshold:.2f} (ä»…ç”¨äºå‚è€ƒä¸å¯è§†åŒ–)")
    logger.info("")
    
    classifier.eval()
    classifier.to(config.DEVICE)
    
    # Create DataLoader
    logger.info(f"ğŸ”§ RNGæŒ‡çº¹(æµ‹è¯•-Dataloaderåˆ›å»ºå‰): {_rng_fingerprint_short()} ({_seed_snapshot()})")
    dataset = TensorDataset(torch.FloatTensor(X_test), torch.LongTensor(y_test))
    test_loader = DataLoader(dataset, batch_size=test_batch_size, shuffle=False)
    logger.info(f"ğŸ”§ RNGæŒ‡çº¹(æµ‹è¯•-Dataloaderåˆ›å»ºå): {_rng_fingerprint_short()} ({_seed_snapshot()})")
    total_batches = len(test_loader)
    
    # Collect probabilities / labels / featuresï¼ˆå…ˆä¸ä½¿ç”¨é˜ˆå€¼ï¼‰
    all_probs = []
    all_labels = []
    all_features = []
    
    logger.info(f"ğŸ”§ RNGæŒ‡çº¹(æµ‹è¯•-æ¨ç†å¼€å§‹å‰): {_rng_fingerprint_short()} ({_seed_snapshot()})")
    logger.info("å¼€å§‹æ¨ç†...")
    logger.info("-"*70)
    
    with torch.no_grad():
        for batch_idx, (X_batch, y_batch) in enumerate(test_loader):
            X_batch = X_batch.to(config.DEVICE)
            y_batch = y_batch.to(config.DEVICE)
            
            # ç›´æ¥å‰å‘è·å¾— logits å’Œç‰¹å¾ï¼Œä¸åœ¨è¿™é‡Œåšé˜ˆå€¼åˆ¤å†³
            logits, z = classifier(X_batch, return_features=True, return_separate=False)
            probs = torch.softmax(logits, dim=1)
            
            all_probs.append(probs.cpu().numpy())
            all_labels.append(y_batch.cpu().numpy())
            all_features.append(z.cpu().numpy())
            
            # æ¯10ä¸ªæ‰¹æ¬¡æˆ–æœ€åä¸€ä¸ªæ‰¹æ¬¡è¾“å‡ºè¿›åº¦
            if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == total_batches:
                progress = (batch_idx + 1) / total_batches * 100
                processed = min((batch_idx + 1) * test_batch_size, len(X_test))
                logger.info(f"  æ¨ç†è¿›åº¦: {batch_idx+1}/{total_batches} batches ({progress:.1f}%) | å·²å¤„ç† {processed}/{len(X_test)} ä¸ªæ ·æœ¬")
    
    logger.info(f"ğŸ”§ RNGæŒ‡çº¹(æµ‹è¯•-æ¨ç†ç»“æŸå): {_rng_fingerprint_short()} ({_seed_snapshot()})")
    # Concatenate results
    y_prob = np.concatenate(all_probs)
    y_true = np.concatenate(all_labels)
    features = np.concatenate(all_features)
    
    logger.info("-"*70)
    logger.info("âœ“ æ¨ç†å®Œæˆ")
    logger.info("")
    
    # ğŸš€ åœ¨æµ‹è¯•é›†ä¸Šæœç´¢æœ€ä¼˜é˜ˆå€¼ï¼ˆåå¤„ç†æ­¥éª¤ï¼Œæ¨¡å‹å·²å›ºå®šï¼Œä¸æ˜¯æ•°æ®æ³„éœ²ï¼‰
    logger.info("ğŸ“Š åŸºäºæµ‹è¯•é›† Binary F1-Score(pos=1) æœç´¢æœ€ä¼˜é˜ˆå€¼...")
    logger.info("   è¯´æ˜: è¿™æ˜¯åå¤„ç†æ­¥éª¤ï¼Œæ¨¡å‹å‚æ•°å·²å›ºå®šï¼Œç”¨äºä¼˜åŒ–å†³ç­–é˜ˆå€¼")
    optimal_threshold, optimal_metric, _ = find_optimal_threshold(
        y_true, y_prob, metric='f1_binary', positive_class=1
    )
    logger.info(f"âœ… æµ‹è¯•é›†æœ€ä¼˜é˜ˆå€¼: {optimal_threshold:.4f} (Binary F1 pos=1 = {optimal_metric:.4f})")
    logger.info(f"   é…ç½®é»˜è®¤é˜ˆå€¼: {config_threshold:.4f}")
    
    # é˜ˆå€¼å¯¹æ¯”ï¼ˆè§£é‡Šä¸ºä»€ä¹ˆå›¾é‡Œå¯èƒ½æ˜¯ 0.8+ è€Œä¸æ˜¯ 0.6ï¼‰
    candidate_thresholds = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, float(config_threshold)]
    # æ³¨æ„ï¼šä¸å¯¹optimal_thresholdè¿›è¡Œå››èˆäº”å…¥ï¼Œä¿æŒåŸå§‹ç²¾åº¦
    candidate_thresholds_display = sorted(set([round(t, 4) for t in candidate_thresholds]))
    # å°†optimal_thresholdæ’å…¥åˆ°æ­£ç¡®çš„ä½ç½®ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
    optimal_threshold_rounded = round(float(optimal_threshold), 4)
    if optimal_threshold_rounded not in candidate_thresholds_display:
        candidate_thresholds_display.append(optimal_threshold_rounded)
        candidate_thresholds_display.sort()
    
    logger.info("ğŸ“ é˜ˆå€¼å¯¹æ¯” (Malicious=Positive):")
    logger.info(f"  {'threshold':>10s} | {'precision':>9s} | {'recall':>7s} | {'f1':>7s}")
    logger.info("  " + "-"*44)
    for th_display in candidate_thresholds_display:
        # å¯¹äºæœ€ä¼˜é˜ˆå€¼ï¼Œä½¿ç”¨åŸå§‹é«˜ç²¾åº¦å€¼ï¼›å…¶ä»–ä½¿ç”¨æ˜¾ç¤ºå€¼
        if abs(th_display - optimal_threshold_rounded) < 0.00001:
            th_actual = float(optimal_threshold)  # ä½¿ç”¨åŸå§‹é«˜ç²¾åº¦å€¼
        else:
            th_actual = th_display
        
        y_pred_th = (y_prob[:, 1] >= th_actual).astype(int)
        p = precision_score(y_true, y_pred_th, pos_label=1, zero_division=0)
        r = recall_score(y_true, y_pred_th, pos_label=1, zero_division=0)
        f1 = f1_score(y_true, y_pred_th, pos_label=1, zero_division=0)
        marker = " â† æœ€ä¼˜" if abs(th_display - optimal_threshold_rounded) < 0.00001 else ""
        logger.info(f"  {th_display:10.4f} | {p:9.4f} | {r:7.4f} | {f1:7.4f}{marker}")
    logger.info("")
    
    # ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼ç”Ÿæˆé¢„æµ‹æ ‡ç­¾ï¼ˆè¿™æ˜¯æœ€ç»ˆä½¿ç”¨çš„é˜ˆå€¼ï¼‰
    y_pred = (y_prob[:, 1] >= optimal_threshold).astype(int)
    logger.info(f"âœ… æœ€ç»ˆè¯„ä¼°ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼: {optimal_threshold:.4f}")
    logger.info(f"   (ä¸‹æ–¹æ€§èƒ½æŒ‡æ ‡å‡åŸºäºæ­¤é˜ˆå€¼è®¡ç®—)")
    logger.info("")
    
    # Calculate metrics at optimal threshold
    logger.info("ğŸ“Š è®¡ç®—æ€§èƒ½æŒ‡æ ‡ (åŸºäºè‡ªåŠ¨é˜ˆå€¼)...")
    metrics = calculate_metrics(y_true, y_pred, y_prob)
    
    # Print metricsï¼ˆåŒ…å«è®ºæ–‡å£å¾„ï¼šæ¶æ„ç±»ä¸ºæ­£ç±»çš„å•ç±»F1ï¼‰
    print_metrics(metrics, logger)
    
    # Per-class metrics
    logger.info("\nPer-Class Analysis:")
    for class_idx, class_name in enumerate(['Benign', 'Malicious']):
        class_mask = y_true == class_idx
        class_acc = (y_pred[class_mask] == y_true[class_mask]).mean()
        logger.info(f"  {class_name:10s}: {class_mask.sum():5d} samples, Accuracy: {class_acc*100:.2f}%")
    
    # Visualizations
    logger.info("\nGenerating visualizations...")
    
    # Feature space
    feature_space_path = os.path.join(config.RESULT_DIR, "figures", f"{save_prefix}_feature_space.png")
    plot_feature_space(features, y_true, feature_space_path, 
                      title=f"Test Set Feature Space", method='tsne')
    
    # Confusion matrix
    confusion_matrix_path = os.path.join(config.RESULT_DIR, "figures", f"{save_prefix}_confusion_matrix.png")
    plot_confusion_matrix(metrics['confusion_matrix'], ['Benign', 'Malicious'], 
                         confusion_matrix_path, title=f"Test Set Confusion Matrix")
    
    # ROC curve
    roc_curve_path = os.path.join(config.RESULT_DIR, "figures", f"{save_prefix}_roc_curve.png")
    plot_roc_curve(y_true, y_prob, roc_curve_path, title=f"Test Set ROC Curve")
    
    # Probability distribution (ä½¿ç”¨è‡ªåŠ¨æœç´¢å¾—åˆ°çš„æœ€ä¼˜é˜ˆå€¼)
    prob_dist_path = os.path.join(config.RESULT_DIR, "figures", f"{save_prefix}_probability_distribution.png")
    plot_probability_distribution(y_true, y_prob, prob_dist_path, 
                                  title=f"Test Set Probability Distribution", threshold=optimal_threshold)
    
    logger.info("")
    logger.info("ğŸ“ è¾“å‡ºæ–‡ä»¶è·¯å¾„:")
    logger.info(f"  âœ“ ç‰¹å¾ç©ºé—´å¯è§†åŒ–: {feature_space_path}")
    logger.info(f"  âœ“ æ··æ·†çŸ©é˜µ: {confusion_matrix_path}")
    logger.info(f"  âœ“ ROCæ›²çº¿: {roc_curve_path}")
    logger.info(f"  âœ“ æ¦‚ç‡åˆ†å¸ƒå›¾: {prob_dist_path}")
    
    # Save predictions to result directory
    results_file = os.path.join(config.RESULT_DIR, "models", f"{save_prefix}_predictions.npz")
    np.savez(results_file,
             y_true=y_true,
             y_pred=y_pred,
             y_prob=y_prob,
             features=features)
    # Save metrics to text file
    metrics_file = os.path.join(config.RESULT_DIR, "models", f"{save_prefix}_metrics.txt")
    with open(metrics_file, 'w') as f:
        f.write("="*70 + "\n")
        f.write(f"MEDAL-Lite Test Results - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("="*70 + "\n\n")
        f.write(f"Test Samples: {len(y_true)}\n")
        f.write(f"  Benign:    {(y_true==0).sum()}\n")
        f.write(f"  Malicious: {(y_true==1).sum()}\n\n")
        f.write("Performance Metrics (Malicious=Positive):\n")
        f.write(f"  Accuracy:          {metrics['accuracy']:.4f}\n")
        f.write(f"  Precision (pos=1): {metrics['precision_pos']:.4f}\n")
        f.write(f"  Recall    (pos=1): {metrics['recall_pos']:.4f}\n")
        f.write(f"  F1 (pos=1):        {metrics['f1_pos']:.4f}\n")
        f.write("  --- reference ---\n")
        f.write(f"  F1-Macro:          {metrics['f1_macro']:.4f}\n")
        f.write(f"  F1-Weighted:       {metrics['f1_weighted']:.4f}\n")
        if 'auc' in metrics:
            f.write(f"  AUC:               {metrics['auc']:.4f}\n")
        f.write("\nConfusion Matrix:\n")
        f.write(str(metrics['confusion_matrix']) + "\n")
    
    logger.info(f"  âœ“ é¢„æµ‹ç»“æœ: {results_file}")
    logger.info(f"  âœ“ æ€§èƒ½æŒ‡æ ‡: {metrics_file}")
    
    # æ·»åŠ æ ‡å‡†åŒ–çš„é”®åä»¥ä¾¿å¯¹æ¯”
    metrics['precision'] = metrics['precision_pos']
    metrics['recall'] = metrics['recall_pos']
    metrics['f1'] = metrics['f1_pos']
    
    return metrics


def _infer_family_from_filename(filename: str) -> str:
    name = str(filename).lower()
    if 'dnscat' in name or 'dnscat2' in name:
        return 'dnscat2'
    if 'iodine' in name:
        return 'iodine'
    if 'dohbrw' in name or 'doh' in name:
        return 'doh'
    return 'unknown'


def _export_family_breakdown(save_prefix: str, y_true: np.ndarray, y_pred: np.ndarray, y_prob: np.ndarray, test_files, config, logger):
    if test_files is None:
        return

    if len(test_files) != len(y_true):
        logger.warning(f"âš  test_files é•¿åº¦({len(test_files)})ä¸æ ·æœ¬æ•°({len(y_true)})ä¸ä¸€è‡´ï¼Œè·³è¿‡åˆ†ç»„è¯„ä¼°")
        return

    families = np.array([_infer_family_from_filename(f) for f in test_files], dtype=object)
    unique_families = sorted(set(families.tolist()))

    rows = []
    for fam in unique_families:
        idx = families == fam
        n = int(idx.sum())
        if n <= 0:
            continue
        y_t = y_true[idx]
        y_p = y_pred[idx]
        y_pb = y_prob[idx]

        p = precision_score(y_t, y_p, pos_label=1, zero_division=0)
        r = recall_score(y_t, y_p, pos_label=1, zero_division=0)
        f1 = f1_score(y_t, y_p, pos_label=1, zero_division=0)
        acc = float((y_t == y_p).mean()) if n > 0 else 0.0

        rows.append({
            'family': fam,
            'n_samples': n,
            'n_benign': int((y_t == 0).sum()),
            'n_malicious': int((y_t == 1).sum()),
            'accuracy': float(acc),
            'precision_pos': float(p),
            'recall_pos': float(r),
            'f1_pos': float(f1),
        })

    if not rows:
        return

    models_dir = os.path.join(config.RESULT_DIR, 'models')
    os.makedirs(models_dir, exist_ok=True)

    csv_path = os.path.join(models_dir, f'{save_prefix}_family_metrics.csv')
    with open(csv_path, 'w', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=list(rows[0].keys()))
        writer.writeheader()
        writer.writerows(rows)

    json_path = os.path.join(models_dir, f'{save_prefix}_family_report.json')
    payload = {
        'save_prefix': save_prefix,
        'generated_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'family_metrics': rows,
    }
    with open(json_path, 'w') as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)

    logger.info("")
    logger.info("ğŸ“ åˆ†ç»„è¯„ä¼°è¾“å‡º:")
    logger.info(f"  âœ“ CSV:  {csv_path}")
    logger.info(f"  âœ“ JSON: {json_path}")


def main(args):
    """Main testing function"""
    
    # Setup
    rng_fp_before_seed = _rng_fingerprint_short()
    set_seed(config.SEED)
    rng_fp_after_seed = _rng_fingerprint_short()
    config.create_dirs()
    logger = setup_logger(os.path.join(config.OUTPUT_ROOT, "logs"), name='test')

    logger.info(f"ğŸ”§ RNGæŒ‡çº¹(seedå‰): {rng_fp_before_seed}")
    logger.info(f"ğŸ”§ RNGæŒ‡çº¹(seedå): {rng_fp_after_seed} ({_seed_snapshot()})")
    
    logger.info("="*70)
    logger.info("ğŸ§ª MEDAL-Lite Testing Pipeline")
    logger.info("="*70)
    
    # GPUä¿¡æ¯
    if torch.cuda.is_available():
        logger.info(f"âœ“ GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
        logger.info(f"  ä½¿ç”¨è®¾å¤‡: {config.DEVICE}")
    else:
        logger.info(f"âš  ä½¿ç”¨CPUè¿›è¡Œæ¨ç†")
        logger.info(f"  ä½¿ç”¨è®¾å¤‡: {config.DEVICE}")
    
    logger.info(f"æ—¶é—´æˆ³: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("")
    
    # ========================
    # Load Test Dataset
    # ========================
    logger.info("="*70)
    logger.info("ğŸ“¦ åŠ è½½æµ‹è¯•æ•°æ®é›† Loading Test Dataset")
    logger.info("="*70)
    logger.info(f"æµ‹è¯•é›†é…ç½®:")
    logger.info(f"  æ­£å¸¸æµé‡è·¯å¾„: {config.BENIGN_TEST}")
    logger.info(f"  æ¶æ„æµé‡è·¯å¾„: {config.MALICIOUS_TEST}")
    logger.info(f"  è¯´æ˜: å°†è¯»å–ä¸Šè¿°è·¯å¾„ä¸‹æ‰€æœ‰pcapæ–‡ä»¶ï¼Œæµæ•°åœ¨å¤„ç†æ—¶ç»Ÿè®¡")
    logger.info("")
    
    logger.info(f"ğŸ”§ RNGæŒ‡çº¹(åŠ è½½æµ‹è¯•æ•°æ®å‰): {_rng_fingerprint_short()} ({_seed_snapshot()})")
    # ä¼˜å…ˆä½¿ç”¨é¢„å¤„ç†å¥½çš„æ•°æ®
    if PREPROCESS_AVAILABLE and check_preprocessed_exists('test'):
        logger.info("âœ“ å‘ç°é¢„å¤„ç†æ–‡ä»¶ï¼Œç›´æ¥åŠ è½½...")
        X_test, y_test, test_files = load_preprocessed('test')
        X_test = normalize_burstsize_inplace(X_test)
        logger.info(f"  ä»é¢„å¤„ç†æ–‡ä»¶åŠ è½½: {X_test.shape[0]} ä¸ªæ ·æœ¬")
    else:
        # ä»PCAPæ–‡ä»¶åŠ è½½
        logger.info("å¼€å§‹åŠ è½½æµ‹è¯•æ•°æ®é›†ï¼ˆä»PCAPæ–‡ä»¶ï¼‰...")
        logger.info("ğŸ’¡ æç¤º: è¿è¡Œ 'python preprocess.py --test_only' å¯é¢„å¤„ç†æµ‹è¯•é›†ï¼ŒåŠ é€Ÿåç»­æµ‹è¯•")
        X_test, y_test, test_files = load_dataset(
            benign_dir=config.BENIGN_TEST,
            malicious_dir=config.MALICIOUS_TEST,
            sequence_length=config.SEQUENCE_LENGTH
        )
        X_test = normalize_burstsize_inplace(X_test)

    logger.info(f"ğŸ”§ RNGæŒ‡çº¹(åŠ è½½æµ‹è¯•æ•°æ®å): {_rng_fingerprint_short()} ({_seed_snapshot()})")
    
    if X_test is None:
        logger.error("âŒ æµ‹è¯•æ•°æ®é›†åŠ è½½å¤±è´¥!")
        return
    
    logger.info("âœ“ æµ‹è¯•æ•°æ®é›†åŠ è½½å®Œæˆ")
    logger.info(f"  æ•°æ®å½¢çŠ¶: {X_test.shape} (æ ·æœ¬æ•°Ã—åºåˆ—é•¿åº¦Ã—ç‰¹å¾ç»´åº¦)")
    logger.info(f"  æ­£å¸¸æ ·æœ¬: {(y_test==0).sum()} ä¸ª")
    logger.info(f"  æ¶æ„æ ·æœ¬: {(y_test==1).sum()} ä¸ª")
    logger.info("")
    
    # ========================
    # Load Model
    # ========================
    logger.info("="*70)
    logger.info("ğŸ“¥ åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ Loading Trained Model")
    logger.info("="*70)
    logger.info("ğŸ“¥ è¾“å…¥æ•°æ®è·¯å¾„:")
    logger.info(f"  âœ“ æµ‹è¯•æ•°æ®: {config.BENIGN_TEST} (æ­£å¸¸), {config.MALICIOUS_TEST} (æ¶æ„)")
    logger.info("")
    
    # Try to load model metadata to get the backbone path used during training
    metadata_path = os.path.join(config.CLASSIFICATION_DIR, "models", "model_metadata.json")
    backbone_path_from_metadata = None
    
    if os.path.exists(metadata_path):
        try:
            import json
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)
            backbone_path_from_metadata = metadata.get('backbone_path')
            input_is_features_from_metadata = metadata.get('input_is_features', False)
            feature_dim_from_metadata = metadata.get('feature_dim', None)
            
            if backbone_path_from_metadata:
                logger.info(f"âœ“ ä»æ¨¡å‹å…ƒæ•°æ®ä¸­è¯»å–åˆ°è®­ç»ƒæ—¶ä½¿ç”¨çš„éª¨å¹²ç½‘ç»œ:")
                logger.info(f"  {backbone_path_from_metadata}")
                if input_is_features_from_metadata:
                    logger.info(f"âœ“ è®­ç»ƒæ—¶è¾“å…¥ç±»å‹: ç‰¹å¾å‘é‡ (ç»´åº¦={feature_dim_from_metadata})")
                    logger.info(f"  æµ‹è¯•æ—¶å°†è‡ªåŠ¨ä»åºåˆ—æå–ç‰¹å¾")
                logger.info("")
        except Exception as e:
            logger.warning(f"âš  æ— æ³•è¯»å–æ¨¡å‹å…ƒæ•°æ®: {e}")
    
    # Load backbone from feature_extraction directory
    logger.info(f"ğŸ”§ RNGæŒ‡çº¹(æ„å»ºbackboneå‰): {_rng_fingerprint_short()} ({_seed_snapshot()})")
    backbone = build_backbone(config, logger=logger)
    logger.info(f"ğŸ”§ RNGæŒ‡çº¹(æ„å»ºbackboneå): {_rng_fingerprint_short()} ({_seed_snapshot()})")
    
    # ç¡®å®šéª¨å¹²ç½‘ç»œè·¯å¾„
    # ä¼˜å…ˆçº§ï¼š1. å‘½ä»¤è¡Œå‚æ•° 2. å…ƒæ•°æ® 3. é»˜è®¤è·¯å¾„
    backbone_path = None
    
    # 1. æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if hasattr(args, 'backbone_path') and args.backbone_path:
        backbone_path = args.backbone_path
        logger.info(f"âœ“ ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„éª¨å¹²ç½‘ç»œ:")
        logger.info(f"  {backbone_path}")
        logger.info("")
    # 2. å°è¯•ä»å…ƒæ•°æ®è¯»å–
    elif backbone_path_from_metadata and os.path.exists(backbone_path_from_metadata):
        backbone_path = backbone_path_from_metadata
        logger.info("âœ“ ä½¿ç”¨è®­ç»ƒæ—¶çš„éª¨å¹²ç½‘ç»œï¼ˆä»å…ƒæ•°æ®ï¼‰")
    # 3. ä½¿ç”¨é»˜è®¤è·¯å¾„
    else:
        backbone_path = os.path.join(config.FEATURE_EXTRACTION_DIR, "models", "backbone_pretrained.pth")
        if backbone_path_from_metadata:
            logger.warning(f"âš  å…ƒæ•°æ®ä¸­çš„éª¨å¹²ç½‘ç»œä¸å­˜åœ¨: {backbone_path_from_metadata}")
            logger.warning(f"  å›é€€åˆ°é»˜è®¤è·¯å¾„: {backbone_path}")
        else:
            logger.info(f"ä½¿ç”¨é»˜è®¤éª¨å¹²ç½‘ç»œè·¯å¾„: {backbone_path}")
    
    if not os.path.exists(backbone_path):
        logger.error(f"âŒ éª¨å¹²ç½‘ç»œæ£€æŸ¥ç‚¹æœªæ‰¾åˆ°: {backbone_path}")
        logger.error("è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬!")
        return
    
    logger.info("æ­£åœ¨åŠ è½½éª¨å¹²ç½‘ç»œ...")
    logger.info(f"  ğŸ“¥ è¾“å…¥æ¨¡å‹: {backbone_path}")
    logger.info(f"ğŸ”§ RNGæŒ‡çº¹(åŠ è½½backboneæƒé‡å‰): {_rng_fingerprint_short()} ({_seed_snapshot()})")
    try:
        backbone_state = torch.load(backbone_path, map_location=config.DEVICE, weights_only=True)
    except TypeError:
        backbone_state = torch.load(backbone_path, map_location=config.DEVICE)

    load_state_dict_shape_safe(backbone, backbone_state, logger, prefix="backbone")
    logger.info(f"ğŸ”§ RNGæŒ‡çº¹(åŠ è½½backboneæƒé‡å): {_rng_fingerprint_short()} ({_seed_snapshot()})")
    backbone.freeze()
    logger.info(f"âœ“ éª¨å¹²ç½‘ç»œåŠ è½½å®Œæˆ")
    
    # ========================
    # Load classifiers (both best and final)
    # ========================
    best_path = os.path.join(config.CLASSIFICATION_DIR, "models", "classifier_best_f1.pth")
    final_path = os.path.join(config.CLASSIFICATION_DIR, "models", "classifier_final.pth")
    
    # æ£€æŸ¥å“ªäº›æ¨¡å‹å­˜åœ¨
    has_best = os.path.exists(best_path)
    has_final = os.path.exists(final_path)
    
    if not has_best and not has_final:
        logger.error(f"âŒ æœªæ‰¾åˆ°ä»»ä½•åˆ†ç±»å™¨æ£€æŸ¥ç‚¹!")
        logger.error(f"  Bestæ¨¡å‹: {best_path}")
        logger.error(f"  Finalæ¨¡å‹: {final_path}")
        logger.error("è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬!")
        return
    
    # å¦‚æœå‘½ä»¤è¡ŒæŒ‡å®šäº†åˆ†ç±»å™¨è·¯å¾„ï¼Œåªæµ‹è¯•æŒ‡å®šçš„æ¨¡å‹
    if hasattr(args, 'classifier_path') and args.classifier_path:
        logger.info("æ­£åœ¨åŠ è½½æŒ‡å®šçš„åˆ†ç±»å™¨...")
        logger.info(f"  ğŸ“¥ è¾“å…¥æ¨¡å‹: {args.classifier_path}")
        
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(æ„å»ºclassifierå‰): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        classifier = MEDAL_Classifier(backbone, config)
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(æ„å»ºclassifierå): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        try:
            classifier_state = torch.load(args.classifier_path, map_location=config.DEVICE, weights_only=True)
        except TypeError:
            classifier_state = torch.load(args.classifier_path, map_location=config.DEVICE)
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(åŠ è½½classifieræƒé‡å‰): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        _load_classifier_checkpoint(classifier, classifier_state, logger=logger)
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(åŠ è½½classifieræƒé‡å): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        logger.info(f"âœ“ åˆ†ç±»å™¨åŠ è½½å®Œæˆ")
        
        # Count parameters
        n_params = sum(p.numel() for p in classifier.parameters())
        n_trainable = sum(p.numel() for p in classifier.parameters() if p.requires_grad)
        logger.info("")
        logger.info("ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
        logger.info(f"  æ€»å‚æ•°é‡: {n_params:,}")
        logger.info(f"  å¯è®­ç»ƒå‚æ•°: {n_trainable:,} (éª¨å¹²ç½‘ç»œå·²å†»ç»“)")
        logger.info("")
        
        # Test single model
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(è¿›å…¥test_modelå‰): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        metrics = test_model(classifier, X_test, y_test, config, logger)
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(test_modelè¿”å›å): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        try:
            results_path = os.path.join(config.RESULT_DIR, 'models', 'test_predictions.npz')
            npz = np.load(results_path, allow_pickle=True)
            y_true = npz['y_true']
            y_pred = npz['y_pred']
            y_prob = npz['y_prob']
            _export_family_breakdown('test', y_true, y_pred, y_prob, test_files, config, logger)
        except Exception as e:
            logger.warning(f"âš  åˆ†ç»„è¯„ä¼°å¯¼å‡ºå¤±è´¥: {e}")
        
        logger.info("")
        logger.info("="*70)
        logger.info("ğŸ‰ æµ‹è¯•å®Œæˆ! Testing Complete!")
        logger.info("="*70)
        logger.info("")
        logger.info("ğŸ“ è¾“å‡ºæ–‡ä»¶è·¯å¾„:")
        logger.info(f"  âœ“ é¢„æµ‹ç»“æœ: {os.path.join(config.RESULT_DIR, 'models', 'test_predictions.npz')}")
        logger.info(f"  âœ“ æ€§èƒ½æŒ‡æ ‡: {os.path.join(config.RESULT_DIR, 'models', 'test_metrics.txt')}")
        logger.info(f"  âœ“ å¯è§†åŒ–å›¾è¡¨: {os.path.join(config.RESULT_DIR, 'figures')}")
        logger.info("")
        logger.info("="*70)
        
        return metrics
    
    # å¦åˆ™ï¼Œæµ‹è¯•æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹å¹¶å¯¹æ¯”
    logger.info("")
    logger.info("="*70)
    logger.info("ğŸ”¬ å¯¹æ¯”æµ‹è¯•ï¼šBest F1 vs Final æ¨¡å‹")
    logger.info("="*70)
    logger.info(f"  Best F1æ¨¡å‹: {'âœ“ å­˜åœ¨' if has_best else 'âœ— ä¸å­˜åœ¨'}")
    logger.info(f"  Finalæ¨¡å‹:   {'âœ“ å­˜åœ¨' if has_final else 'âœ— ä¸å­˜åœ¨'}")
    logger.info("")
    
    models_to_test = []
    if has_best:
        models_to_test.append(("Best F1", best_path))
    if has_final:
        models_to_test.append(("Final", final_path))
    
    all_metrics = {}
    
    for model_name, model_path in models_to_test:
        logger.info("="*70)
        logger.info(f"ğŸ“Š æµ‹è¯•æ¨¡å‹: {model_name}")
        logger.info("="*70)
        logger.info(f"æ­£åœ¨åŠ è½½åˆ†ç±»å™¨...")
        logger.info(f"  ğŸ“¥ è¾“å…¥æ¨¡å‹: {model_path}")
        
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(æ„å»ºclassifierå‰): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        classifier = MEDAL_Classifier(backbone, config)
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(æ„å»ºclassifierå): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        try:
            classifier_state = torch.load(model_path, map_location=config.DEVICE, weights_only=True)
        except TypeError:
            classifier_state = torch.load(model_path, map_location=config.DEVICE)
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(åŠ è½½classifieræƒé‡å‰): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        _load_classifier_checkpoint(classifier, classifier_state, logger=logger)
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(åŠ è½½classifieræƒé‡å): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        logger.info(f"âœ“ åˆ†ç±»å™¨åŠ è½½å®Œæˆ")
        
        if model_name == "Best F1":
            # Count parameters only once
            n_params = sum(p.numel() for p in classifier.parameters())
            n_trainable = sum(p.numel() for p in classifier.parameters() if p.requires_grad)
            logger.info("")
            logger.info("ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
            logger.info(f"  æ€»å‚æ•°é‡: {n_params:,}")
            logger.info(f"  å¯è®­ç»ƒå‚æ•°: {n_trainable:,} (éª¨å¹²ç½‘ç»œå·²å†»ç»“)")
            logger.info("")
        
        # Test model with specific save prefix
        save_prefix = "test_best" if model_name == "Best F1" else "test_final"
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(è¿›å…¥test_modelå‰): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        metrics = test_model(classifier, X_test, y_test, config, logger, save_prefix=save_prefix)
        logger.info(f"ğŸ”§ RNGæŒ‡çº¹(test_modelè¿”å›å): {_rng_fingerprint_short()} ({_seed_snapshot()})")
        try:
            results_path = os.path.join(config.RESULT_DIR, 'models', f'{save_prefix}_predictions.npz')
            npz = np.load(results_path, allow_pickle=True)
            y_true = npz['y_true']
            y_pred = npz['y_pred']
            y_prob = npz['y_prob']
            _export_family_breakdown(save_prefix, y_true, y_pred, y_prob, test_files, config, logger)
        except Exception as e:
            logger.warning(f"âš  åˆ†ç»„è¯„ä¼°å¯¼å‡ºå¤±è´¥: {e}")
        all_metrics[model_name] = metrics
        
        logger.info("")
    
    # ========================
    # Compare Results
    # ========================
    if len(all_metrics) > 1:
        logger.info("="*70)
        logger.info("ğŸ“Š æ¨¡å‹å¯¹æ¯”ç»“æœ")
        logger.info("="*70)
        logger.info("")
        
        # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
        logger.info(f"{'æŒ‡æ ‡':<20} | {'Best F1':<12} | {'Final':<12} | {'å·®å¼‚':<12}")
        logger.info("-"*70)
        
        metrics_to_compare = ['accuracy', 'precision', 'recall', 'f1', 'f1_macro', 'auc']
        metric_names = {
            'accuracy': 'Accuracy',
            'precision': 'Precision (pos=1)',
            'recall': 'Recall (pos=1)',
            'f1': 'F1 (pos=1)',
            'f1_macro': 'F1-Macro',
            'auc': 'AUC'
        }
        
        for metric_key in metrics_to_compare:
            if metric_key in all_metrics.get("Best F1", {}) and metric_key in all_metrics.get("Final", {}):
                best_val = all_metrics["Best F1"][metric_key]
                final_val = all_metrics["Final"][metric_key]
                diff = final_val - best_val
                diff_str = f"{diff:+.4f}" if abs(diff) > 0.0001 else "0.0000"
                
                # æ ‡è®°å“ªä¸ªæ›´å¥½
                if abs(diff) > 0.001:
                    if diff > 0:
                        marker = " â† Finalæ›´å¥½"
                    else:
                        marker = " â† Bestæ›´å¥½"
                else:
                    marker = " (ç›¸è¿‘)"
                
                logger.info(f"{metric_names[metric_key]:<20} | {best_val:<12.4f} | {final_val:<12.4f} | {diff_str:<12}{marker}")
        
        logger.info("")
        logger.info("ğŸ’¡ è¯´æ˜:")
        logger.info("  - Best F1: è®­ç»ƒè¿‡ç¨‹ä¸­éªŒè¯é›†F1æœ€é«˜çš„æ¨¡å‹")
        logger.info("  - Final: è®­ç»ƒç»“æŸæ—¶çš„æœ€ç»ˆæ¨¡å‹")
        logger.info("  - å·®å¼‚: Final - Best F1 (æ­£å€¼è¡¨ç¤ºFinalæ›´å¥½)")
        logger.info("")
    
    logger.info("="*70)
    logger.info("ğŸ‰ æµ‹è¯•å®Œæˆ! Testing Complete!")
    logger.info("="*70)
    logger.info("")
    logger.info("ğŸ“ è¾“å‡ºæ–‡ä»¶è·¯å¾„:")
    if has_best:
        logger.info(f"  âœ“ Best F1é¢„æµ‹ç»“æœ: {os.path.join(config.RESULT_DIR, 'models', 'test_best_predictions.npz')}")
        logger.info(f"  âœ“ Best F1æ€§èƒ½æŒ‡æ ‡: {os.path.join(config.RESULT_DIR, 'models', 'test_best_metrics.txt')}")
        logger.info(f"  âœ“ Best F1å¯è§†åŒ–: {os.path.join(config.RESULT_DIR, 'figures', 'test_best_*.png')}")
    if has_final:
        logger.info(f"  âœ“ Finalé¢„æµ‹ç»“æœ: {os.path.join(config.RESULT_DIR, 'models', 'test_final_predictions.npz')}")
        logger.info(f"  âœ“ Finalæ€§èƒ½æŒ‡æ ‡: {os.path.join(config.RESULT_DIR, 'models', 'test_final_metrics.txt')}")
        logger.info(f"  âœ“ Finalå¯è§†åŒ–: {os.path.join(config.RESULT_DIR, 'figures', 'test_final_*.png')}")
    logger.info("")
    logger.info("="*70)
    
    return metrics


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Test MEDAL-Lite model")
    parser.add_argument('--backbone_path', type=str, default='', help='Path to backbone checkpoint (optional)')
    parser.add_argument('--classifier_path', type=str, default='', help='Path to classifier checkpoint (optional)')
    
    args = parser.parse_args()
    
    main(args)

