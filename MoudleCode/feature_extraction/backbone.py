"""
Micro-Bi-Mamba Backbone for Feature Extraction
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
from .mamba_block import MambaLayer


class TrafficHybridEmbedding(nn.Module):
    def __init__(self, d_model: int, direction_index: int = 2, input_dim: int = 6):
        super().__init__()
        self.direction_index = int(direction_index)
        self.input_dim = int(input_dim)

        cont_indices = [i for i in range(self.input_dim) if i != self.direction_index]
        self.register_buffer('_cont_indices', torch.tensor(cont_indices, dtype=torch.long), persistent=False)

        self.cont_proj = nn.Sequential(
            nn.Linear(len(cont_indices), d_model),
            nn.LayerNorm(d_model),
            nn.GELU(),
        )

        self.dir_emb = nn.Embedding(num_embeddings=2, embedding_dim=d_model)
        nn.init.normal_(self.dir_emb.weight, std=0.02)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x_cont = x.index_select(dim=-1, index=self._cont_indices)

        x_dir_raw = x[:, :, self.direction_index]
        dir_indices = (x_dir_raw > 0).to(dtype=torch.long)

        embed_cont = self.cont_proj(x_cont)
        embed_dir = self.dir_emb(dir_indices)

        return embed_cont + embed_dir


class SinusoidalPositionalEncoding(nn.Module):
    """Sinusoidal positional encoding"""
    
    def __init__(self, d_model, max_len=2048):
        super().__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        """
        Args:
            x: (B, L, d_model)
        
        Returns:
            x + pe: (B, L, d_model)
        """
        return x + self.pe[:, :x.size(1), :]


class MicroBiMambaBackbone(nn.Module):
    """
    Micro-Bi-Mamba Backbone
    
    Bidirectional Mamba architecture for traffic feature extraction
    with frozen backbone after pre-training
    """
    
    def __init__(self, config):
        super().__init__()
        
        self.config = config
        self.output_dim = getattr(config, 'OUTPUT_DIM', getattr(config, 'FEATURE_DIM', config.MODEL_DIM))
        
        # Embedding layer
        input_dim = int(getattr(config, 'INPUT_FEATURE_DIM', 0))
        direction_index = getattr(config, 'DIRECTION_INDEX', None)
        if direction_index is not None:
            try:
                direction_index = int(direction_index)
            except Exception:
                direction_index = None

        if input_dim > 0 and direction_index is not None and 0 <= int(direction_index) < input_dim:
            self.embedding = TrafficHybridEmbedding(
                d_model=config.MODEL_DIM,
                direction_index=int(direction_index),
                input_dim=input_dim,
            )
        else:
            self.embedding = nn.Linear(config.INPUT_FEATURE_DIM, config.MODEL_DIM)
        # ‰ΩøÁî® EFFECTIVE_SEQUENCE_LENGTH ‰ª•ÊîØÊåÅÂÖ®Â±ÄÁªüËÆ°‰ª§ÁâåÔºàÁ¨¨1025Ë°åÔºâ
        max_seq_len = getattr(config, 'EFFECTIVE_SEQUENCE_LENGTH', config.SEQUENCE_LENGTH)
        self.pos_encoding = SinusoidalPositionalEncoding(config.MODEL_DIM, max_len=max_seq_len)
        self.embed_dropout = nn.Dropout(config.EMBEDDING_DROPOUT)
        
        # Forward Mamba layers
        self.forward_layers = nn.ModuleList([
            MambaLayer(
                d_model=config.MODEL_DIM,
                d_state=config.MAMBA_STATE_DIM,
                d_conv=config.MAMBA_CONV_KERNEL,
                expand=config.MAMBA_EXPANSION_FACTOR,
                dropout=config.MAMBA_DROPOUT
            )
            for _ in range(config.MAMBA_LAYERS)
        ])
        
        # Backward Mamba layers
        self.backward_layers = nn.ModuleList([
            MambaLayer(
                d_model=config.MODEL_DIM,
                d_state=config.MAMBA_STATE_DIM,
                d_conv=config.MAMBA_CONV_KERNEL,
                expand=config.MAMBA_EXPANSION_FACTOR,
                dropout=config.MAMBA_DROPOUT
            )
            for _ in range(config.MAMBA_LAYERS)
        ])
        
        # Projection layer to combine bidirectional features
        # Keep internal MODEL_DIM for sequence modeling, but output compressed features for downstream.
        self.projection = nn.Linear(config.MODEL_DIM * 2, self.output_dim)
        
        # Decoder for SimMTM (pre-training only)
        use_mlp_decoder = bool(getattr(config, 'SIMMTM_DECODER_USE_MLP', False))
        decoder_hidden_dim = int(getattr(config, 'SIMMTM_DECODER_HIDDEN_DIM', 64))
        if use_mlp_decoder:
            self.decoder = nn.Sequential(
                nn.Linear(self.output_dim, decoder_hidden_dim),
                nn.GELU(),
                nn.Linear(decoder_hidden_dim, config.INPUT_FEATURE_DIM),
            )
        else:
            self.decoder = nn.Linear(self.output_dim, config.INPUT_FEATURE_DIM)
        
        self.frozen = False
    
    def forward(self, x, return_sequence=False):
        """
        Forward pass
        
        Args:
            x: (B, L, D_in) - input features
            return_sequence: If True, return sequence features; else return pooled features
            
        Returns:
            z: (B, output_dim) if return_sequence=False, else (B, L, output_dim)
        """
        valid_mask = None
        valid_mask_index = getattr(self.config, 'VALID_MASK_INDEX', None)
        if valid_mask_index is not None:
            try:
                if int(valid_mask_index) >= 0 and int(valid_mask_index) < x.shape[-1]:
                    valid_mask = x[:, :, int(valid_mask_index)]
            except Exception:
                valid_mask = None

        # Embedding
        x = self.embedding(x)  # (B, L, d_model)
        x = self.pos_encoding(x)
        x = self.embed_dropout(x)
        
        # Forward pass
        x_fwd = x
        for layer in self.forward_layers:
            x_fwd = layer(x_fwd)
        
        # Backward pass (reverse sequence)
        x_bwd = torch.flip(x, dims=[1])
        for layer in self.backward_layers:
            x_bwd = layer(x_bwd)
        x_bwd = torch.flip(x_bwd, dims=[1])  # Reverse back
        
        # Combine bidirectional features
        if return_sequence:
            # Concatenate and project for sequence output
            x_combined = torch.cat([x_fwd, x_bwd], dim=-1)  # (B, L, 2*d_model)
            z = self.projection(x_combined)  # (B, L, d_model)
        else:
            # Average pooling and combine for single vector output
            if valid_mask is not None:
                m = (valid_mask > 0.5).to(dtype=x_fwd.dtype, device=x_fwd.device).unsqueeze(-1)  # (B, L, 1)
                denom = m.sum(dim=1).clamp_min(1e-6)
                z_fwd = (x_fwd * m).sum(dim=1) / denom
                z_bwd = (x_bwd * m).sum(dim=1) / denom
            else:
                z_fwd = torch.mean(x_fwd, dim=1)  # (B, d_model)
                z_bwd = torch.mean(x_bwd, dim=1)  # (B, d_model)
            z_cat = torch.cat([z_fwd, z_bwd], dim=-1)  # (B, 2*d_model)
            z = self.projection(z_cat)  # (B, d_model)
        
        return z
    
    def freeze(self):
        """Freeze all parameters"""
        for param in self.parameters():
            param.requires_grad = False
        self.frozen = True
    
    def unfreeze(self):
        """Unfreeze all parameters"""
        for param in self.parameters():
            param.requires_grad = True
        self.frozen = False


class _DualStreamHybridEmbedding(nn.Module):
    """ÊóßÁâàÔºö‰ªÖÊîØÊåÅ Direction + BurstSize"""
    def __init__(self, d_model: int):
        super().__init__()
        self.cont_proj = nn.Sequential(
            nn.Linear(1, d_model),
            nn.LayerNorm(d_model),
            nn.GELU(),
        )
        self.dir_emb = nn.Embedding(num_embeddings=2, embedding_dim=d_model)
        nn.init.normal_(self.dir_emb.weight, std=0.02)

    def forward(self, x_dir: torch.Tensor, x_cont: torch.Tensor) -> torch.Tensor:
        dir_indices = (x_dir > 0).to(dtype=torch.long)
        embed_cont = self.cont_proj(x_cont)
        embed_dir = self.dir_emb(dir_indices)
        return embed_cont + embed_dir


class _DualStreamStructureEmbedding(nn.Module):
    """
    MEDAL-Lite4 ÁªìÊûÑÊµÅ Embedding
    
    ËæìÂÖ•ÔºöDirection (Á¶ªÊï£) + BurstSize (ËøûÁª≠)
    ËæìÂá∫ÔºöËûçÂêàÂêéÁöÑÁªìÊûÑÁâπÂæÅÂêëÈáè
    
    ËÆæËÆ°Ôºö
    - Direction: Embedding Â±ÇÔºà+1/-1 Êò†Â∞ÑÂà∞ÂêëÈáèÔºâ
    - BurstSize: Á∫øÊÄßÊäïÂΩ±Ôºà1Áª¥ ‚Üí d_modelÔºâ
    - ËûçÂêàÔºöÂä†Ê≥ïÔºà‰øùÊåÅÁâπÂæÅÁã¨Á´ãÊÄßÔºâ
    """
    def __init__(self, d_model: int, cont_dim: int = 1):
        super().__init__()
        self.cont_proj = nn.Sequential(
            nn.Linear(cont_dim, d_model),
            nn.LayerNorm(d_model),
            nn.GELU(),
        )
        self.dir_emb = nn.Embedding(num_embeddings=2, embedding_dim=d_model)
        nn.init.normal_(self.dir_emb.weight, std=0.02)

    def forward(self, x_dir: torch.Tensor, x_cont: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x_dir: (B, L) - Direction ÁâπÂæÅ (+1/-1)
            x_cont: (B, L, cont_dim) - ËøûÁª≠ÁâπÂæÅ [BurstSize]
        
        Returns:
            embed: (B, L, d_model) - ËûçÂêàÂêéÁöÑÁªìÊûÑÁâπÂæÅ
        """
        dir_indices = (x_dir > 0).to(dtype=torch.long)
        embed_cont = self.cont_proj(x_cont)
        embed_dir = self.dir_emb(dir_indices)
        return embed_cont + embed_dir


class DualStreamBiMambaBackbone(nn.Module):
    """
    MEDAL-Lite4 ÂèåÊµÅ Bi-Mamba È™®Âπ≤ÁΩëÁªú
    
    ÂèåÊµÅÁâ©ÁêÜÂàáÂàÜÔºö
    - Stream 1 (ÂÜÖÂÆπÊµÅ): Length - Â≠¶‰π†"Â§ßÂåÖ‰∏éÂ∞èÂåÖÁöÑÊéíÂàóÊóãÂæã"
    - Stream 2 (ÁªìÊûÑÊµÅ): Direction + BurstSize - Â≠¶‰π†"‰∫§‰∫íÊ®°ÂºèÁöÑÈÖçÂêà"
    
    Èó®ÊéßËûçÂêàÔºöÂä®ÊÄÅË∞ÉÊï¥‰∏§Ë∑ØÁâπÂæÅÊùÉÈáç
    InfoNCE ‰∏ÄËá¥ÊÄßÔºöÂº∫Ëø´ÂèåÊµÅÁâπÂæÅÂØπÈΩêÔºàÈÄöËøá instance_contrastive Ê®°ÂùóÔºâ
    """
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.output_dim = getattr(config, 'OUTPUT_DIM', getattr(config, 'FEATURE_DIM', config.MODEL_DIM))

        model_dim = int(getattr(config, 'MODEL_DIM', 32))
        d_len = max(1, model_dim // 2)
        d_struct = max(1, model_dim - d_len)

        self._d_len = int(d_len)
        self._d_struct = int(d_struct)

        self.fusion_type = str(getattr(config, 'MAMBA_FUSION_TYPE', 'gate')).strip().lower()

        # Lite4 ÁâπÂæÅÁ¥¢Âºï
        self.length_index = getattr(config, 'LENGTH_INDEX', 0)
        self.direction_index = getattr(config, 'DIRECTION_INDEX', None)
        self.burst_index = getattr(config, 'BURST_SIZE_INDEX', None)
        self.valid_mask_index = getattr(config, 'VALID_MASK_INDEX', None)

        # Stream 1: ÂÜÖÂÆπÊµÅ (Length only)
        self.len_embed = nn.Sequential(
            nn.Linear(1, self._d_len),
            nn.LayerNorm(self._d_len),
            nn.GELU(),
        )
        # ‰ΩøÁî® EFFECTIVE_SEQUENCE_LENGTH ‰ª•ÊîØÊåÅÂÖ®Â±ÄÁªüËÆ°‰ª§ÁâåÔºàÁ¨¨1025Ë°åÔºâ
        max_seq_len = getattr(config, 'EFFECTIVE_SEQUENCE_LENGTH', config.SEQUENCE_LENGTH)
        self.len_pos = SinusoidalPositionalEncoding(self._d_len, max_len=max_seq_len)
        self.len_dropout = nn.Dropout(config.EMBEDDING_DROPOUT)

        # Stream 2: ÁªìÊûÑÊµÅ (Direction + BurstSize)
        self.struct_embed = _DualStreamStructureEmbedding(self._d_struct, cont_dim=1)
        self.struct_pos = SinusoidalPositionalEncoding(self._d_struct, max_len=max_seq_len)
        self.struct_dropout = nn.Dropout(config.EMBEDDING_DROPOUT)

        self.len_forward_layers = nn.ModuleList([
            MambaLayer(
                d_model=self._d_len,
                d_state=config.MAMBA_STATE_DIM,
                d_conv=config.MAMBA_CONV_KERNEL,
                expand=config.MAMBA_EXPANSION_FACTOR,
                dropout=config.MAMBA_DROPOUT,
            )
            for _ in range(config.MAMBA_LAYERS)
        ])
        self.len_backward_layers = nn.ModuleList([
            MambaLayer(
                d_model=self._d_len,
                d_state=config.MAMBA_STATE_DIM,
                d_conv=config.MAMBA_CONV_KERNEL,
                expand=config.MAMBA_EXPANSION_FACTOR,
                dropout=config.MAMBA_DROPOUT,
            )
            for _ in range(config.MAMBA_LAYERS)
        ])
        self.len_proj = nn.Linear(self._d_len * 2, self._d_len)

        self.struct_forward_layers = nn.ModuleList([
            MambaLayer(
                d_model=self._d_struct,
                d_state=config.MAMBA_STATE_DIM,
                d_conv=config.MAMBA_CONV_KERNEL,
                expand=config.MAMBA_EXPANSION_FACTOR,
                dropout=config.MAMBA_DROPOUT,
            )
            for _ in range(config.MAMBA_LAYERS)
        ])
        self.struct_backward_layers = nn.ModuleList([
            MambaLayer(
                d_model=self._d_struct,
                d_state=config.MAMBA_STATE_DIM,
                d_conv=config.MAMBA_CONV_KERNEL,
                expand=config.MAMBA_EXPANSION_FACTOR,
                dropout=config.MAMBA_DROPOUT,
            )
            for _ in range(config.MAMBA_LAYERS)
        ])
        self.struct_proj = nn.Linear(self._d_struct * 2, self._d_struct)

        self.fusion_gate = None
        self.fusion_proj = None
        self.len_to_fused = None
        self.struct_to_fused = None

        if self.fusion_type in ('gate', 'gated', 'gated_fusion'):
            self.fusion_gate = nn.Sequential(
                nn.Linear(model_dim, model_dim),
                nn.Sigmoid(),
            )
            self.fusion_proj = nn.Linear(model_dim, model_dim)
        elif self.fusion_type in ('concat_project', 'concat', 'project'):
            self.fusion_proj = nn.Linear(model_dim, model_dim)
        elif self.fusion_type in ('average', 'avg', 'mean'):
            self.len_to_fused = nn.Linear(self._d_len, model_dim)
            self.struct_to_fused = nn.Linear(self._d_struct, model_dim)
        else:
            self.fusion_type = 'gate'
            self.fusion_gate = nn.Sequential(
                nn.Linear(model_dim, model_dim),
                nn.Sigmoid(),
            )
            self.fusion_proj = nn.Linear(model_dim, model_dim)
        self.projection = nn.Linear(model_dim, self.output_dim)

        use_mlp_decoder = bool(getattr(config, 'SIMMTM_DECODER_USE_MLP', False))
        decoder_hidden_dim = int(getattr(config, 'SIMMTM_DECODER_HIDDEN_DIM', 64))
        if use_mlp_decoder:
            self.decoder = nn.Sequential(
                nn.Linear(self.output_dim, decoder_hidden_dim),
                nn.GELU(),
                nn.Linear(decoder_hidden_dim, config.INPUT_FEATURE_DIM),
            )
        else:
            self.decoder = nn.Linear(self.output_dim, config.INPUT_FEATURE_DIM)

        self.frozen = False

    def forward(self, x, return_sequence=False, return_dual_features=False, return_pre_fusion=False):
        """
        ÂâçÂêë‰º†Êí≠
        
        Args:
            x: (B, L, D) - ËæìÂÖ•ÁâπÂæÅ
            return_sequence: ÊòØÂê¶ËøîÂõûÂ∫èÂàóÁâπÂæÅ
            return_dual_features: ÊòØÂê¶ËøîÂõûÂèåÊµÅÁâπÂæÅÔºàÁî®‰∫é InfoNCE ‰∏ÄËá¥ÊÄßÔºåÊ±†ÂåñÂêéÔºâ
            return_pre_fusion: ÊòØÂê¶ËøîÂõûËûçÂêàÂâçÁöÑÂ∫èÂàóÁâπÂæÅÔºàÁî®‰∫é InfoNCE ‰∏ÄËá¥ÊÄßÊçüÂ§±ËÆ°ÁÆóÔºâ
        
        Returns:
            Â¶ÇÊûú return_pre_fusion=True:
                h_len: (B, L, d_len) - ÂÜÖÂÆπÊµÅÂ∫èÂàóÁâπÂæÅÔºàËûçÂêàÂâçÔºâ
                h_struct: (B, L, d_struct) - ÁªìÊûÑÊµÅÂ∫èÂàóÁâπÂæÅÔºàËûçÂêàÂâçÔºâ
            Â¶ÇÊûú return_dual_features=True:
                z_content: (B, d_len) - ÂÜÖÂÆπÊµÅÁâπÂæÅÔºàÊ±†ÂåñÂêéÔºâ
                z_structure: (B, d_struct) - ÁªìÊûÑÊµÅÁâπÂæÅÔºàÊ±†ÂåñÂêéÔºâ
                z_fused: (B, output_dim) - ËûçÂêàÂêéÁâπÂæÅ
            Âê¶Âàô:
                z: (B, output_dim) Êàñ (B, L, output_dim)
        """
        valid_mask = None
        if self.valid_mask_index is not None:
            try:
                if int(self.valid_mask_index) >= 0 and int(self.valid_mask_index) < x.shape[-1]:
                    valid_mask = x[:, :, int(self.valid_mask_index)]
            except Exception:
                valid_mask = None

        gate = None
        if valid_mask is not None:
            gate = (valid_mask > 0.5).to(dtype=x.dtype, device=x.device).unsqueeze(-1)

        # ===== Stream 1: ÂÜÖÂÆπÊµÅ (Length) =====
        length_index = int(self.length_index) if self.length_index is not None else 0
        x_len = x[:, :, length_index:length_index + 1]
        x_len = self.len_embed(x_len)
        x_len = self.len_pos(x_len)
        x_len = self.len_dropout(x_len)
        if gate is not None:
            x_len = x_len * gate

        # ===== Stream 2: ÁªìÊûÑÊµÅ (Direction + BurstSize) =====
        x_dir = None
        x_burst = None
        
        # Extract Direction
        if self.direction_index is not None:
            try:
                di = int(self.direction_index)
                if 0 <= di < x.shape[-1]:
                    x_dir = x[:, :, di]
            except Exception:
                x_dir = None
        
        # Extract BurstSize
        if self.burst_index is not None:
            try:
                bi = int(self.burst_index)
                if 0 <= bi < x.shape[-1]:
                    x_burst = x[:, :, bi:bi + 1]
            except Exception:
                x_burst = None
        
        # Fallback to zeros if features are missing
        if x_dir is None:
            x_dir = torch.zeros(x.shape[0], x.shape[1], device=x.device, dtype=x.dtype)
        if x_burst is None:
            x_burst = torch.zeros(x.shape[0], x.shape[1], 1, device=x.device, dtype=x.dtype)

        x_struct_cont = x_burst  # (B, L, 1)
        
        x_struct = self.struct_embed(x_dir, x_struct_cont)
        x_struct = self.struct_pos(x_struct)
        x_struct = self.struct_dropout(x_struct)
        if gate is not None:
            x_struct = x_struct * gate

        # ===== Bi-Mamba ÁºñÁ†Å =====
        # Content stream
        x_len_fwd = x_len
        for layer in self.len_forward_layers:
            x_len_fwd = layer(x_len_fwd)
        x_len_bwd = torch.flip(x_len, dims=[1])
        for layer in self.len_backward_layers:
            x_len_bwd = layer(x_len_bwd)
        x_len_bwd = torch.flip(x_len_bwd, dims=[1])
        h_len = self.len_proj(torch.cat([x_len_fwd, x_len_bwd], dim=-1))

        # Structure stream
        x_struct_fwd = x_struct
        for layer in self.struct_forward_layers:
            x_struct_fwd = layer(x_struct_fwd)
        x_struct_bwd = torch.flip(x_struct, dims=[1])
        for layer in self.struct_backward_layers:
            x_struct_bwd = layer(x_struct_bwd)
        x_struct_bwd = torch.flip(x_struct_bwd, dims=[1])
        h_struct = self.struct_proj(torch.cat([x_struct_fwd, x_struct_bwd], dim=-1))

        # ===== ËøîÂõûËûçÂêàÂâçÁöÑÂ∫èÂàóÁâπÂæÅÔºàÁî®‰∫é InfoNCE ‰∏ÄËá¥ÊÄßÊçüÂ§±Ôºâ =====
        if return_pre_fusion:
            return h_len, h_struct

        h = torch.cat([h_len, h_struct], dim=-1)  # (B, L, model_dim)
        if self.fusion_type == 'gate':
            fusion_gate = self.fusion_gate(h)  # (B, L, model_dim)
            h_fused = fusion_gate * h + (1.0 - fusion_gate) * self.fusion_proj(h)
        elif self.fusion_type == 'concat_project':
            h_fused = self.fusion_proj(h)
        elif self.fusion_type == 'average':
            h_fused = 0.5 * (self.len_to_fused(h_len) + self.struct_to_fused(h_struct))
        else:
            fusion_gate = self.fusion_gate(h)  # (B, L, model_dim)
            h_fused = fusion_gate * h + (1.0 - fusion_gate) * self.fusion_proj(h)

        # ===== ËøîÂõûÂèåÊµÅÁâπÂæÅÔºàÁî®‰∫é InfoNCE ‰∏ÄËá¥ÊÄßÔºâ =====
        if return_dual_features:
            # Pool each stream separately
            if valid_mask is not None:
                m = (valid_mask > 0.5).to(dtype=h.dtype, device=h.device).unsqueeze(-1)
                denom = m.sum(dim=1).clamp_min(1e-6)
                z_content = (h_len * m).sum(dim=1) / denom  # (B, d_len)
                z_structure = (h_struct * m).sum(dim=1) / denom  # (B, d_struct)
                z_fused_pooled = (h_fused * m).sum(dim=1) / denom
            else:
                z_content = torch.mean(h_len, dim=1)
                z_structure = torch.mean(h_struct, dim=1)
                z_fused_pooled = torch.mean(h_fused, dim=1)
            
            z_fused = self.projection(z_fused_pooled)
            return z_content, z_structure, z_fused

        # ===== Ê†áÂáÜËøîÂõû =====
        if return_sequence:
            return self.projection(h_fused)

        if valid_mask is not None:
            m = (valid_mask > 0.5).to(dtype=h_fused.dtype, device=h_fused.device).unsqueeze(-1)
            denom = m.sum(dim=1).clamp_min(1e-6)
            pooled = (h_fused * m).sum(dim=1) / denom
        else:
            pooled = torch.mean(h_fused, dim=1)
        return self.projection(pooled)

    def freeze(self):
        for param in self.parameters():
            param.requires_grad = False
        self.frozen = True

    def unfreeze(self):
        for param in self.parameters():
            param.requires_grad = True
        self.frozen = False


def build_backbone(config, logger=None):
    if logger is not None:
        logger.info("üß† Backbone arch: DualStreamBiMambaBackbone (fixed)")
    return DualStreamBiMambaBackbone(config)


class SimMTMLoss(nn.Module):
    """
    Masked multi-head reconstruction loss (SimMTM++)
    
    - ÈöèÊú∫Êé©Á†Å + ËΩªÂô™Â£∞
    - ËøûÁª≠ÁâπÂæÅÁî® MSE/HuberÔºåÁ¶ªÊï£ÁâπÂæÅÁî® BCE
    - Âè™Âú®Ë¢´Êé©Á†Å‰∏îÊúâÊïàÁöÑ‰ΩçÁΩÆ‰∏äËÆ°ÁÆó
    """
    
    def __init__(
        self,
        config,
        mask_rate: float = 0.5,
        noise_std: float = 0.05,
    ):
        super().__init__()
        self.config = config
        self.mask_rate = mask_rate
        self.noise_std = noise_std
    
    def forward(self, backbone, x_original):
        """
        Args:
            backbone: ÂæÆÂûã Bi-Mamba ÁºñÁ†ÅÂô®
            x_original: (B, L, D) ÂéüÂßãËæìÂÖ•
        
        Returns:
            total_loss: scalar
        """
        B, L, D = x_original.shape
        device = x_original.device
        cfg = backbone.config

        seq_len = int(getattr(cfg, 'SEQUENCE_LENGTH', L))
        effective_len = int(getattr(cfg, 'EFFECTIVE_SEQUENCE_LENGTH', seq_len))
        use_global_stats = bool(getattr(cfg, 'USE_GLOBAL_STATS_TOKEN', False))
        has_global = bool(use_global_stats and L == effective_len and L > seq_len)
        pkt_len = seq_len if has_global else L

        # ===== masking & noise =====
        valid_mask_idx = getattr(cfg, 'VALID_MASK_INDEX', None)
        valid = None
        if valid_mask_idx is not None and 0 <= int(valid_mask_idx) < D:
            try:
                valid = x_original[:, :pkt_len, int(valid_mask_idx)] > 0.5
            except Exception:
                valid = None

        # True Ë°®Á§∫Ë¶ÅË¢´Êé©Á†ÅÈáçÊûÑ
        mask_pos_pkt = torch.rand(B, pkt_len, device=device) < self.mask_rate
        if valid is not None:
            mask_pos_pkt = mask_pos_pkt & valid
        mask_pos_pkt = mask_pos_pkt.unsqueeze(-1)  # (B, pkt_len, 1)

        x_masked = x_original.clone()
        x_masked[:, :pkt_len, :] = x_masked[:, :pkt_len, :] * (~mask_pos_pkt)
        if valid_mask_idx is not None and 0 <= int(valid_mask_idx) < D:
            x_masked[:, :pkt_len, int(valid_mask_idx)] = x_original[:, :pkt_len, int(valid_mask_idx)]

        if self.noise_std > 0:
            # Noise is only applied to continuous channels (e.g., Length/BurstSize) on
            # unmasked valid positions. Do NOT perturb Direction/ValidMask.
            noise_apply = (~mask_pos_pkt.squeeze(-1))
            if valid is not None:
                noise_apply = noise_apply & valid

            length_idx = getattr(cfg, 'LENGTH_INDEX', None)
            if length_idx is not None and 0 <= int(length_idx) < D:
                li = int(length_idx)
                n = torch.randn_like(x_masked[:, :pkt_len, li]) * self.noise_std
                x_masked[:, :pkt_len, li] = torch.where(noise_apply, x_masked[:, :pkt_len, li] + n, x_masked[:, :pkt_len, li])

            burst_idx = getattr(cfg, 'BURST_SIZE_INDEX', None)
            if burst_idx is not None and 0 <= int(burst_idx) < D:
                bi = int(burst_idx)
                n = torch.randn_like(x_masked[:, :pkt_len, bi]) * self.noise_std
                x_masked[:, :pkt_len, bi] = torch.where(noise_apply, x_masked[:, :pkt_len, bi] + n, x_masked[:, :pkt_len, bi])

        # ===== encode & decode =====
        z_seq = backbone(x_masked, return_sequence=True)  # (B, L, d)
        x_recon = backbone.decoder(z_seq)  # (B, L, D)

        # ===== per-feature losses =====
        mask_scalar = mask_pos_pkt.squeeze(-1).to(dtype=x_original.dtype)  # (B, pkt_len)

        def _masked_mean(loss_tensor, mask_tensor):
            denom = mask_tensor.sum().clamp_min(1e-6)
            return (loss_tensor * mask_tensor).sum() / denom

        total_components = {}
        recon_loss = 0.0

        length_idx = getattr(cfg, 'LENGTH_INDEX', None)
        if length_idx is not None and 0 <= int(length_idx) < D:
            lt = int(length_idx)
            loss_len = F.mse_loss(x_recon[:, :pkt_len, lt], x_original[:, :pkt_len, lt], reduction='none')
            loss_len = _masked_mean(loss_len, mask_scalar)
            total_components['length'] = loss_len
            recon_loss = recon_loss + float(getattr(cfg, 'PRETRAIN_LENGTH_WEIGHT', 1.0)) * loss_len

        burst_idx = getattr(cfg, 'BURST_SIZE_INDEX', None)
        if burst_idx is not None and 0 <= int(burst_idx) < D:
            bt = int(burst_idx)
            loss_burst = F.mse_loss(x_recon[:, :pkt_len, bt], x_original[:, :pkt_len, bt], reduction='none')
            loss_burst = _masked_mean(loss_burst, mask_scalar)
            total_components['burst'] = loss_burst
            recon_loss = recon_loss + float(getattr(cfg, 'PRETRAIN_BURST_WEIGHT', 1.0)) * loss_burst

        dir_idx = getattr(cfg, 'DIRECTION_INDEX', None)
        if dir_idx is not None and 0 <= int(dir_idx) < D:
            dt = int(dir_idx)
            target_dir = (x_original[:, :pkt_len, dt] > 0).to(dtype=x_original.dtype)
            loss_dir = F.binary_cross_entropy_with_logits(
                x_recon[:, :pkt_len, dt],
                target_dir,
                reduction='none'
            )
            loss_dir = _masked_mean(loss_dir, mask_scalar)
            total_components['direction'] = loss_dir
            recon_loss = recon_loss + float(getattr(cfg, 'PRETRAIN_DIRECTION_WEIGHT', 1.0)) * loss_dir

        if valid_mask_idx is not None and 0 <= int(valid_mask_idx) < D:
            vm = int(valid_mask_idx)
            target_vm = x_original[:, :pkt_len, vm]
            loss_vm = F.binary_cross_entropy_with_logits(
                x_recon[:, :pkt_len, vm],
                target_vm,
                reduction='none'
            )
            loss_vm = _masked_mean(loss_vm, mask_scalar)
            total_components['validmask'] = loss_vm
            recon_loss = recon_loss + float(getattr(cfg, 'PRETRAIN_VALIDMASK_WEIGHT', 0.5)) * loss_vm

        total_loss = float(getattr(cfg, 'PRETRAIN_RECON_WEIGHT', 1.0)) * recon_loss

        # ÁºìÂ≠òÂàÜÈ°πÔºå‰æø‰∫éÂ§ñÈÉ®Ë∞ÉËØï
        self.last_loss_dict = {
            'total': float(total_loss.item()),
            **{k: float(v.item()) for k, v in total_components.items()}
        }

        return total_loss


class SupConLoss(nn.Module):
    """Supervised Contrastive Loss"""
    
    def __init__(self, temperature=0.1):
        super().__init__()
        self.temperature = temperature
    
    def forward(self, features, labels):
        """
        Args:
            features: (B, d) - feature vectors
            labels: (B,) - labels
            
        Returns:
            loss: scalar
        """
        device = features.device
        batch_size = features.shape[0]
        
        # Normalize features
        features = F.normalize(features, dim=1)
        
        # Compute similarity matrix
        similarity_matrix = torch.matmul(features, features.T) / self.temperature
        
        # Create mask for positive pairs
        labels = labels.view(-1, 1)
        mask = torch.eq(labels, labels.T).float().to(device)
        
        # Remove diagonal (self-comparison)
        logits_mask = torch.ones_like(mask)
        logits_mask.fill_diagonal_(0)
        
        mask = mask * logits_mask
        
        # Compute log_prob
        exp_logits = torch.exp(similarity_matrix) * logits_mask
        log_prob = similarity_matrix - torch.log(exp_logits.sum(1, keepdim=True))
        
        # Compute mean of log-likelihood over positive pairs
        mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-6)
        
        # Loss
        loss = -mean_log_prob_pos.mean()
        
        return loss
