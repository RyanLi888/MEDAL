import os
import sys
from pathlib import Path

# Ensure project root is on sys.path when running as a script
project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))
import json
import shutil
import argparse
from datetime import datetime

import numpy as np
import torch

from MoudleCode.utils.config import config
from MoudleCode.utils.helpers import set_seed, setup_logger
from MoudleCode.preprocessing.pcap_parser import load_dataset
from MoudleCode.feature_extraction.backbone import MicroBiMambaBackbone, build_backbone

try:
    from scripts.utils.preprocess import check_preprocessed_exists, load_preprocessed
    PREPROCESS_AVAILABLE = True
except Exception:
    PREPROCESS_AVAILABLE = False

from scripts.training.train import stage3_finetune_classifier
from scripts.testing.test import main as test_main


def _safe_makedirs(path: str) -> None:
    os.makedirs(path, exist_ok=True)


def _load_train_dataset():
    if PREPROCESS_AVAILABLE and check_preprocessed_exists('train'):
        X_train, y_train, _ = load_preprocessed('train')
        return X_train, y_train

    X_train, y_train, _ = load_dataset(
        benign_dir=config.BENIGN_TRAIN,
        malicious_dir=config.MALICIOUS_TRAIN,
        sequence_length=config.SEQUENCE_LENGTH,
    )
    return X_train, y_train


def _default_correction_npz_path() -> str:
    return os.path.join(config.LABEL_CORRECTION_DIR, 'analysis', 'correction_results.npz')


def _summarize_array(x: np.ndarray):
    x = np.asarray(x)
    if x.size == 0:
        return {
            'min': None,
            'max': None,
            'mean': None,
            'p25': None,
            'p50': None,
            'p75': None,
        }
    return {
        'min': float(np.min(x)),
        'max': float(np.max(x)),
        'mean': float(np.mean(x)),
        'p25': float(np.percentile(x, 25)),
        'p50': float(np.percentile(x, 50)),
        'p75': float(np.percentile(x, 75)),
    }


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--correction_npz', type=str, default='', help='Path to correction_results.npz')
    parser.add_argument('--use_ground_truth', action='store_true')
    parser.add_argument('--retrain_backbone', action='store_true')
    parser.add_argument('--backbone_path', type=str, default='', help='Path to backbone model (optional)')
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--run_tag', type=str, default='')
    args = parser.parse_args()

    set_seed(args.seed)
    config.create_dirs()
    logger = setup_logger(os.path.join(config.OUTPUT_ROOT, 'logs'), name='clean_only_train_test')

    # ----------------------------
    # Pure focal-loss mode (for clean-only overfit/debug experiments)
    # Keep gradients simple: no SoftF1 / no orthogonality / no consistency.
    # ----------------------------
    config.USE_FOCAL_LOSS = True
    config.USE_SOFT_F1_LOSS = False
    config.SOFT_F1_WEIGHT = 0.0
    config.SOFT_ORTH_WEIGHT_START = 0.0
    config.SOFT_ORTH_WEIGHT_END = 0.0
    config.CONSISTENCY_WEIGHT_START = 0.0
    config.CONSISTENCY_WEIGHT_END = 0.0

    # Enable backbone finetuning in clean-only experiments (do not overwrite original backbone file).
    config.FINETUNE_BACKBONE = True
    tmp_scope = str(getattr(config, 'FINETUNE_BACKBONE_SCOPE', 'projection')).lower()
    config.FINETUNE_BACKBONE_SCOPE = tmp_scope if tmp_scope in ('projection', 'all') else 'projection'

    # Use full training data in this mode; allow early-stopping on train metric.
    config.FINETUNE_VAL_SPLIT = 0.0
    config.FINETUNE_ES_ALLOW_TRAIN_METRIC = True

    run_tag = args.run_tag.strip() or datetime.now().strftime('%Y%m%d_%H%M%S')
    run_dir = os.path.join(config.LABEL_CORRECTION_DIR, 'analysis', 'clean_only_runs', run_tag)
    _safe_makedirs(run_dir)
    _safe_makedirs(os.path.join(run_dir, 'models'))
    _safe_makedirs(os.path.join(run_dir, 'result_snapshot'))

    # Override output dirs to avoid overwriting the default training/test artifacts
    original_classification_dir = getattr(config, 'CLASSIFICATION_DIR', None)
    original_result_dir = getattr(config, 'RESULT_DIR', None)

    config.CLASSIFICATION_DIR = os.path.join(run_dir, 'classification')
    config.RESULT_DIR = os.path.join(run_dir, 'result')

    for d in [config.CLASSIFICATION_DIR, config.RESULT_DIR]:
        _safe_makedirs(d)
        _safe_makedirs(os.path.join(d, 'models'))
        _safe_makedirs(os.path.join(d, 'figures'))
        _safe_makedirs(os.path.join(d, 'logs'))

    try:

        logger.info('=' * 70)
        logger.info('ğŸš€ å¹²å‡€æ•°æ®è®­ç»ƒ+æµ‹è¯•æ¨¡å¼')
        logger.info('=' * 70)
        logger.info(f'è¿è¡Œç›®å½•: {run_dir}')
        logger.info(f'æ—¶é—´æˆ³: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}')
        logger.info('')
        
        # æ˜¾ç¤ºè®­ç»ƒæ¨¡å¼
        if args.use_ground_truth:
            logger.info('ğŸ“‹ è®­ç»ƒæ¨¡å¼: ä½¿ç”¨çœŸå®æ ‡ç­¾ï¼ˆæ— å™ªå£°ï¼‰')
            logger.info('  - æ•°æ®æ¥æº: åŸå§‹è®­ç»ƒé›†')
            logger.info('  - æ ‡ç­¾: çœŸå®æ ‡ç­¾ï¼ˆground truthï¼‰')
            logger.info('  - æ ‡ç­¾çŸ«æ­£: è·³è¿‡')
            logger.info('  - æ•°æ®å¢å¼º: è·³è¿‡')
        else:
            logger.info('ğŸ“‹ è®­ç»ƒæ¨¡å¼: ä½¿ç”¨æ ‡ç­¾çŸ«æ­£ç»“æœ')
            logger.info(f'  - çŸ«æ­£æ–‡ä»¶: {correction_npz}')
            logger.info('  - æ ‡ç­¾: çŸ«æ­£åçš„æ ‡ç­¾')
            logger.info('  - æ•°æ®å¢å¼º: è·³è¿‡')
        
        logger.info('')
        logger.info('ğŸ“ è¾“å‡ºç›®å½•ï¼ˆéš”ç¦»ï¼‰:')
        logger.info(f'  - åˆ†ç±»å™¨: {config.CLASSIFICATION_DIR}')
        logger.info(f'  - æµ‹è¯•ç»“æœ: {config.RESULT_DIR}')
        logger.info('')

        X_train, y_train_true = _load_train_dataset()
        if X_train is None:
            raise RuntimeError('Failed to load train dataset')

        if args.use_ground_truth:
            y_corrected = y_train_true.astype(int)
            action_mask = np.zeros(len(y_corrected), dtype=int)
            correction_weight = np.ones(len(y_corrected), dtype=np.float32)
            correction_npz = ''
        else:
            correction_npz = args.correction_npz.strip() or _default_correction_npz_path()
            if not os.path.exists(correction_npz):
                raise FileNotFoundError(f'correction_npz not found: {correction_npz}')
            logger.info(f'Correction npz: {correction_npz}')
            data = np.load(correction_npz, allow_pickle=True)
            y_corrected = data['y_corrected'].astype(int)
            action_mask = data['action_mask'].astype(int)
            correction_weight = data['correction_weight'].astype(np.float32)

            if len(X_train) != len(y_corrected):
                raise ValueError(f'Length mismatch: X_train={len(X_train)} vs y_corrected={len(y_corrected)}')

        keep_mask = action_mask != 2
        X_clean = X_train[keep_mask]
        y_clean = y_corrected[keep_mask]
        w_clean = correction_weight[keep_mask]

        stats = {
            'n_total': int(len(y_corrected)),
            'n_keep': int(np.sum(action_mask == 0)),
            'n_flip': int(np.sum(action_mask == 1)),
            'n_drop': int(np.sum(action_mask == 2)),
            'n_reweight': int(np.sum(action_mask == 3)),
            'n_train_used': int(len(X_clean)),
            'label_dist_corrected': {
                'benign': int(np.sum(y_clean == 0)),
                'malicious': int(np.sum(y_clean == 1)),
            },
            'weight_summary': _summarize_array(w_clean),
        }

        logger.info('ğŸ“Š æ•°æ®ç»Ÿè®¡:')
        logger.info(f"  - è®­ç»ƒæ ·æœ¬æ€»æ•°: {stats['n_train_used']}")
        logger.info(f"  - æ­£å¸¸æ ·æœ¬: {stats['label_dist_corrected']['benign']}")
        logger.info(f"  - æ¶æ„æ ·æœ¬: {stats['label_dist_corrected']['malicious']}")
        
        if not args.use_ground_truth:
            logger.info('')
            logger.info('ğŸ“ æ ‡ç­¾çŸ«æ­£ç»Ÿè®¡:')
            logger.info(f"  - ä¿æŒä¸å˜: {stats['n_keep']}")
            logger.info(f"  - ç¿»è½¬æ ‡ç­¾: {stats['n_flip']}")
            logger.info(f"  - ä¸¢å¼ƒæ ·æœ¬: {stats['n_drop']}")
            logger.info(f"  - é‡æ–°åŠ æƒ: {stats['n_reweight']}")
        
        logger.info('')
        logger.info('ğŸ”§ éª¨å¹²ç½‘ç»œ:')
        
        backbone = build_backbone(config, logger=logger)
        
        # ç¡®å®šbackboneè·¯å¾„ï¼šä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
        if args.backbone_path:
            backbone_path = args.backbone_path
            logger.info(f'  - æ¥æº: å‘½ä»¤è¡ŒæŒ‡å®š')
        else:
            backbone_path = os.path.join(config.FEATURE_EXTRACTION_DIR, 'models', 'backbone_pretrained.pth')
            logger.info(f'  - æ¥æº: é»˜è®¤è·¯å¾„')
        
        logger.info(f'  - è·¯å¾„: {backbone_path}')
        
        if os.path.exists(backbone_path) and not args.retrain_backbone:
            logger.info(f'  - çŠ¶æ€: âœ“ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹')
            try:
                state_dict = torch.load(backbone_path, map_location=config.DEVICE, weights_only=True)
            except TypeError:
                state_dict = torch.load(backbone_path, map_location=config.DEVICE)
            try:
                backbone.load_state_dict(state_dict)
            except RuntimeError as e:
                logger.warning(f"âš  éª¨å¹²ç½‘ç»œæ£€æŸ¥ç‚¹ä¸å½“å‰ç»“æ„ä¸å®Œå…¨åŒ¹é…ï¼Œå°†ä½¿ç”¨ strict=False åŠ è½½: {e}")
                missing, unexpected = backbone.load_state_dict(state_dict, strict=False)
                if missing:
                    logger.warning(f"  missing_keys: {missing}")
                if unexpected:
                    logger.warning(f"  unexpected_keys: {unexpected}")
            backbone.freeze()
        else:
            if args.retrain_backbone:
                logger.info('  - çŠ¶æ€: âš  ä½¿ç”¨éšæœºåˆå§‹åŒ–ï¼ˆ--retrain_backbone æŒ‡å®šï¼‰')
            else:
                logger.info(f'  - çŠ¶æ€: âš  æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–')
            backbone.freeze()
        
        logger.info('')

        with open(os.path.join(run_dir, 'train_data_stats.json'), 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)

        X_tr = X_clean
        y_tr = y_clean
        w_tr = w_clean

        logger.info('=' * 70)
        logger.info('ğŸ¯ Stage 3: åˆ†ç±»å™¨è®­ç»ƒ')
        logger.info('=' * 70)
        logger.info('ğŸ“¥ è¾“å…¥æ•°æ®:')
        logger.info(f'  - è®­ç»ƒæ ·æœ¬: {len(X_tr)} ä¸ª')
        logger.info(f'  - ç‰¹å¾æ¥æº: éª¨å¹²ç½‘ç»œæå–')
        if args.use_ground_truth:
            logger.info(f'  - æ ‡ç­¾æ¥æº: çœŸå®æ ‡ç­¾ï¼ˆæ— å™ªå£°ï¼‰')
        else:
            logger.info(f'  - æ ‡ç­¾æ¥æº: æ ‡ç­¾çŸ«æ­£ç»“æœ')
        logger.info(f'  - æ•°æ®å¢å¼º: æœªä½¿ç”¨')
        logger.info('')

        stage3_finetune_classifier(
            backbone,
            X_tr,
            y_tr,
            w_tr,
            config,
            logger,
            n_original=len(X_tr),
            backbone_path=backbone_path,
        )

        classifier_final = os.path.join(config.CLASSIFICATION_DIR, 'models', 'classifier_final.pth')
        classifier_best = os.path.join(config.CLASSIFICATION_DIR, 'models', 'classifier_best_f1.pth')
        history_npz = os.path.join(config.CLASSIFICATION_DIR, 'models', 'training_history.npz')

        logger.info('')
        logger.info('=' * 70)
        logger.info('ğŸ§ª æµ‹è¯•è¯„ä¼°')
        logger.info('=' * 70)
        logger.info('ğŸ“¥ ä½¿ç”¨æ¨¡å‹:')
        logger.info(f'  - éª¨å¹²ç½‘ç»œ: {backbone_path}')
        logger.info(f'  - åˆ†ç±»å™¨: {classifier_best}')
        logger.info('')
        
        # Prefer finetuned backbone if available (read from model metadata)
        meta_path = os.path.join(config.CLASSIFICATION_DIR, 'models', 'model_metadata.json')
        backbone_path_for_test = backbone_path
        if os.path.exists(meta_path):
            try:
                with open(meta_path, 'r', encoding='utf-8') as f:
                    meta = json.load(f)
                bp = meta.get('backbone_path', '')
                if isinstance(bp, str) and bp.strip():
                    backbone_path_for_test = bp
            except Exception:
                pass

        # åˆ›å»ºæµ‹è¯•å‚æ•°ï¼Œä¼ é€’éª¨å¹²ç½‘ç»œè·¯å¾„
        test_args = argparse.Namespace(backbone_path=backbone_path_for_test, classifier_path=classifier_best)
        test_main(test_args)

        result_models_dir = os.path.join(config.RESULT_DIR, 'models')
        result_figures_dir = os.path.join(config.RESULT_DIR, 'figures')

        summary = {
            'run_dir': run_dir,
            'correction_npz': correction_npz,
            'use_ground_truth': bool(args.use_ground_truth),
            'backbone_path': backbone_path,
            'classifier_final': classifier_final,
            'result_dir': config.RESULT_DIR,
            'train_data_stats': stats,
        }

        with open(os.path.join(run_dir, 'run_summary.json'), 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        logger.info('')
        logger.info('=' * 70)
        logger.info('âœ… å®Œæˆï¼')
        logger.info('=' * 70)
        logger.info('ğŸ“ è¾“å‡ºæ–‡ä»¶:')
        logger.info(f'  - è¿è¡Œç›®å½•: {run_dir}')
        logger.info(f'  - åˆ†ç±»å™¨: {classifier_best}')
        logger.info(f'  - æµ‹è¯•ç»“æœ: {result_models_dir}/')
        logger.info(f'  - å¯è§†åŒ–: {result_figures_dir}/')
        logger.info(f'  - è¿è¡Œæ‘˜è¦: {os.path.join(run_dir, "run_summary.json")}')
        logger.info('=' * 70)
    finally:
        # Restore config (best-effort) for safety if this script is imported elsewhere
        if original_classification_dir is not None:
            config.CLASSIFICATION_DIR = original_classification_dir
        if original_result_dir is not None:
            config.RESULT_DIR = original_result_dir


if __name__ == '__main__':
    main()
