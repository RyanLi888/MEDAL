#!/usr/bin/env python3
"""
GPUæ£€æµ‹è„šæœ¬
æ£€æŸ¥PyTorchæ˜¯å¦èƒ½æ­£ç¡®ä½¿ç”¨GPU
"""

import sys
import os
from pathlib import Path

# Ensure project root is on sys.path when running as a script
project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

def check_gpu():
    """æ£€æµ‹GPUå¯ç”¨æ€§"""
    print("="*60)
    print("GPU å¯ç”¨æ€§æ£€æµ‹")
    print("="*60)
    
    try:
        import torch
        
        print("\n1. PyTorch ç‰ˆæœ¬ä¿¡æ¯:")
        print(f"   PyTorch ç‰ˆæœ¬: {torch.__version__}")
        
        # æ£€æŸ¥CUDA
        print("\n2. CUDA å¯ç”¨æ€§:")
        if torch.cuda.is_available():
            print("   âœ… CUDA å¯ç”¨")
            print(f"   CUDA ç‰ˆæœ¬: {torch.version.cuda}")
            print(f"   cuDNN ç‰ˆæœ¬: {torch.backends.cudnn.version()}")
            
            # GPUä¿¡æ¯
            print("\n3. GPU è®¾å¤‡ä¿¡æ¯:")
            gpu_count = torch.cuda.device_count()
            print(f"   å¯ç”¨GPUæ•°é‡: {gpu_count}")
            
            for i in range(gpu_count):
                print(f"\n   GPU {i}:")
                print(f"     åç§°: {torch.cuda.get_device_name(i)}")
                props = torch.cuda.get_device_properties(i)
                print(f"     è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
                print(f"     æ˜¾å­˜æ€»é‡: {props.total_memory / 1024**3:.2f} GB")
                
                # å½“å‰æ˜¾å­˜ä½¿ç”¨
                if torch.cuda.is_initialized():
                    allocated = torch.cuda.memory_allocated(i) / 1024**3
                    cached = torch.cuda.memory_reserved(i) / 1024**3
                    print(f"     å·²åˆ†é…: {allocated:.2f} GB")
                    print(f"     å·²ç¼“å­˜: {cached:.2f} GB")
            
            # æµ‹è¯•GPUè®¡ç®—
            print("\n4. GPU è®¡ç®—æµ‹è¯•:")
            try:
                device = torch.device("cuda:0")
                x = torch.randn(1000, 1000, device=device)
                y = torch.randn(1000, 1000, device=device)
                z = torch.matmul(x, y)
                print("   âœ… GPU è®¡ç®—æµ‹è¯•é€šè¿‡")
                print(f"   æµ‹è¯•çŸ©é˜µ: {x.shape} @ {y.shape} = {z.shape}")
                print(f"   ç»“æœè®¾å¤‡: {z.device}")
            except Exception as e:
                print(f"   âŒ GPU è®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
                return False
            
            # æµ‹è¯•é…ç½®
            print("\n5. é¡¹ç›®é…ç½®:")
            try:
                from MoudleCode.utils.config import config
                print(f"   é…ç½®è®¾å¤‡: {config.DEVICE}")
                if str(config.DEVICE) == "cuda":
                    print("   âœ… é¡¹ç›®é…ç½®æ­£ç¡®ï¼Œå°†ä½¿ç”¨GPUåŠ é€Ÿ")
                else:
                    print(f"   âš ï¸  é¡¹ç›®é…ç½®ä¸º {config.DEVICE}")
            except Exception as e:
                print(f"   âš ï¸  æ— æ³•åŠ è½½é¡¹ç›®é…ç½®: {e}")
            
            print("\n" + "="*60)
            print("âœ… GPUå®Œå…¨å¯ç”¨ï¼Œå¯ä»¥è¿›è¡Œè®­ç»ƒ")
            print("="*60)
            
            # æ€§èƒ½å»ºè®®
            print("\nğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
            print("   1. ä½¿ç”¨ CUDA 11.x æˆ–æ›´é«˜ç‰ˆæœ¬ä»¥è·å¾—æœ€ä½³æ€§èƒ½")
            print("   2. å¯ç”¨ cuDNN è‡ªåŠ¨è°ƒä¼˜:")
            print("      torch.backends.cudnn.benchmark = True")
            print("   3. ç›‘æ§æ˜¾å­˜ä½¿ç”¨ï¼Œé¿å…OOMé”™è¯¯")
            print("   4. å»ºè®®æ‰¹æ¬¡å¤§å°: 64 (å¯æ ¹æ®æ˜¾å­˜è°ƒæ•´)")
            
            return True
            
        else:
            print("   âŒ CUDA ä¸å¯ç”¨")
            print("\nå¯èƒ½çš„åŸå› :")
            print("   1. æœªå®‰è£…CUDA")
            print("   2. æœªå®‰è£…GPUç‰ˆæœ¬çš„PyTorch")
            print("   3. GPUé©±åŠ¨é—®é¢˜")
            print("\nè§£å†³æ–¹æ¡ˆ:")
            print("   1. å®‰è£…CUDA: https://developer.nvidia.com/cuda-downloads")
            print("   2. å®‰è£…GPUç‰ˆPyTorch:")
            print("      pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118")
            print("   3. æ£€æŸ¥NVIDIAé©±åŠ¨:")
            print("      nvidia-smi")
            
            print("\nâš ï¸  å½“å‰å°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
            return False
            
    except ImportError:
        print("\nâŒ é”™è¯¯: PyTorchæœªå®‰è£…")
        print("è¯·å®‰è£…PyTorch:")
        print("   pip install torch torchvision")
        return False
    
    except Exception as e:
        print(f"\nâŒ æ£€æµ‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def show_training_estimate():
    """æ˜¾ç¤ºè®­ç»ƒæ—¶é—´ä¼°ç®—"""
    print("\n" + "="*60)
    print("è®­ç»ƒæ—¶é—´ä¼°ç®— (åŸºäºå®é™…æ•°æ®é‡ï¼Œæµæ•°åœ¨å¤„ç†æ—¶ç»Ÿè®¡)")
    print("="*60)

    try:
        import torch
    except ImportError:
        print("\nâš ï¸  æ— æ³•ä¼°ç®—è®­ç»ƒæ—¶é—´ï¼šPyTorchæœªå®‰è£…")
        return
    
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        
        # æ ¹æ®GPUç±»å‹ä¼°ç®—
        if "RTX 2080" in gpu_name or "RTX 2070" in gpu_name:
            print("\nä½¿ç”¨ GPU (RTX 20ç³»åˆ—):")
            print("  Stage 1 (é¢„è®­ç»ƒ):    2-3 å°æ—¶")
            print("  Stage 2 (çŸ«æ­£+å¢å¼º): 1-2 å°æ—¶")
            print("  Stage 3 (å¾®è°ƒ):      30 åˆ†é’Ÿ")
            print("  æ€»è®¡:               çº¦ 4-6 å°æ—¶")
        elif "RTX 30" in gpu_name or "RTX 40" in gpu_name:
            print("\nä½¿ç”¨ GPU (RTX 30/40ç³»åˆ—):")
            print("  Stage 1 (é¢„è®­ç»ƒ):    1-2 å°æ—¶")
            print("  Stage 2 (çŸ«æ­£+å¢å¼º): 0.5-1 å°æ—¶")
            print("  Stage 3 (å¾®è°ƒ):      15-20 åˆ†é’Ÿ")
            print("  æ€»è®¡:               çº¦ 2-4 å°æ—¶")
        elif "Tesla" in gpu_name or "A100" in gpu_name:
            print("\nä½¿ç”¨ GPU (æ•°æ®ä¸­å¿ƒçº§):")
            print("  Stage 1 (é¢„è®­ç»ƒ):    0.5-1 å°æ—¶")
            print("  Stage 2 (çŸ«æ­£+å¢å¼º): 0.5 å°æ—¶")
            print("  Stage 3 (å¾®è°ƒ):      10 åˆ†é’Ÿ")
            print("  æ€»è®¡:               çº¦ 1-2 å°æ—¶")
        else:
            print("\nä½¿ç”¨ GPU:")
            print("  é¢„è®¡æ€»æ—¶é—´: 3-6 å°æ—¶")
    else:
        print("\nä½¿ç”¨ CPU:")
        print("  Stage 1 (é¢„è®­ç»ƒ):    8-10 å°æ—¶")
        print("  Stage 2 (çŸ«æ­£+å¢å¼º): 3-5 å°æ—¶")
        print("  Stage 3 (å¾®è°ƒ):      1-2 å°æ—¶")
        print("  æ€»è®¡:               çº¦ 12-17 å°æ—¶")
        print("\nâš ï¸  å¼ºçƒˆå»ºè®®ä½¿ç”¨GPUä»¥åŠ é€Ÿè®­ç»ƒ")


if __name__ == "__main__":
    has_gpu = check_gpu()
    show_training_estimate()
    
    print("\n" + "="*60)
    if has_gpu:
        print("âœ… ç³»ç»Ÿå°±ç»ªï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒ")
        print("\nè¿è¡Œè®­ç»ƒ:")
        print("   cd /home/lx/python/MEDAL")
        print("   python all_train_test.py")
    else:
        print("âš ï¸  å»ºè®®é…ç½®GPUåå†è¿›è¡Œè®­ç»ƒ")
        print("   æˆ–è€…è°ƒæ•´å‚æ•°ä»¥é€‚åº”CPUè®­ç»ƒ:")
        print("   - å‡å°‘è®­ç»ƒè½®æ•°")
        print("   - å‡å°æ‰¹æ¬¡å¤§å°")
        print("   - å‡å°‘è®­ç»ƒæ ·æœ¬æ•°")
    print("="*60)
    
    sys.exit(0 if has_gpu else 1)

